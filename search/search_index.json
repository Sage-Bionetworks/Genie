{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#aacr-project-genie","title":"AACR Project GENIE","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This repository documents code used to gather, QC, standardize, and analyze data uploaded by institutes participating in AACR's Project GENIE (Genomics, Evidence, Neoplasia, Information, Exchange).</p>"},{"location":"#additional-documentation","title":"Additional Documentation","text":"<p>Visit the Project Genie Data Portal to learn more about our data</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#dependencies","title":"Dependencies","text":"<p>This package contains both R, Python and cli tools.  These are tools or packages you will need, to be able to reproduce these results:</p> <ul> <li>Python &gt;=3.8 or &lt;3.10<ul> <li><code>pip install -r requirements.txt</code></li> </ul> </li> <li>bedtools</li> <li>R 4.2.2<ul> <li><code>renv::install()</code></li> <li>Follow instructions here to install synapser</li> </ul> </li> <li>Java &gt; 8<ul> <li>For mac users, it seems to work better to run <code>brew install java</code></li> </ul> </li> <li>wget<ul> <li>For mac users, have to run <code>brew install wget</code></li> </ul> </li> </ul>"},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#pypi","title":"PyPi","text":"<p>The aacrgenie package is available from PyPI. It can be installed or upgraded with pip.</p> <pre><code>pip install aacrgenie\n</code></pre>"},{"location":"getting_started/#local","title":"Local","text":"<p>Source code and development versions are available on Github. Installing from source:</p> <pre><code>git clone https://github.com/Sage-Bionetworks/Genie.git\ncd Genie\n</code></pre> <p>Install the packages locally.</p> <pre><code>pip install -e .\npip install -r requirements.txt\npip install -r requirements-dev.txt\n</code></pre> <p>You can stay on the main branch to get the latest stable release or check out the develop branch or a tagged revision:</p> <pre><code>git checkout &lt;branch or tag&gt;\n</code></pre>"},{"location":"getting_started/#configuration","title":"Configuration","text":"<p>Configure the Synapse client to authenticate to Synapse.</p> <ol> <li>Create a Synapse Personal Access token (PAT).</li> <li> <p>Add a <code>~/.synapseConfig</code> file with the following information:</p> <pre><code>[authentication]\nauthtoken = &lt;PAT here&gt;\n</code></pre> </li> <li> <p>OR set an environmental variable</p> <pre><code>export SYNAPSE_AUTH_TOKEN=&lt;PAT here&gt;\n</code></pre> </li> <li> <p>Confirm you can log in to synapse in your terminal.</p> <pre><code>synapse login\n</code></pre> </li> </ol>"},{"location":"reference/fileformats/assay/","title":"Assay Information","text":""},{"location":"reference/fileformats/assay/#genie_registry.assay","title":"<code>genie_registry.assay</code>","text":"<p>Assay information class</p>"},{"location":"reference/fileformats/assay/#genie_registry.assay-classes","title":"Classes","text":""},{"location":"reference/fileformats/assay/#genie_registry.assay.Assayinfo","title":"<code>Assayinfo</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> <p>Assay information file type</p> Source code in <code>genie_registry/assay.py</code> <pre><code>class Assayinfo(FileTypeFormat):\n    \"\"\"Assay information file type\"\"\"\n\n    _fileType = \"assayinfo\"\n\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    # _validation_kwargs = [\"project_id\"]\n\n    def _validateFilename(self, filepath_list):\n        \"\"\"Validate assay information filename\"\"\"\n        assert os.path.basename(filepath_list[0]) == \"assay_information.yaml\"\n\n    def process_steps(self, assay_info_df, newPath, databaseSynId):\n        \"\"\"\n        Process bed input and update bed database\n\n        Args:\n            assay_info_df: Assay information dataframe\n            newPath: Path to processed assay information\n            databaseSynId: assay information database synapse id\n\n        Returns:\n            path to assay information dataframe\n        \"\"\"\n        # Must pass in a list\n        process_assay_info_df = self._process(assay_info_df)\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=databaseSynId,\n            newData=process_assay_info_df,\n            filterBy=self.center,\n            toDelete=True,\n        )\n        process_assay_info_df.to_csv(newPath, sep=\"\\t\", index=False)\n        return newPath\n\n    def _process(self, df):\n        \"\"\"\n        Process assay_information.yaml. Standardizes SEQ_ASSAY_ID,\n        default 10 for gene_padding, and fills in variant_classifications\n\n        Args:\n            df: Assay information dataframe\n\n        Returns:\n            dataframe: Processed dataframe\n        \"\"\"\n        seq_assay_ids = [\n            assay.upper().replace(\"_\", \"-\") for assay in df[\"SEQ_ASSAY_ID\"]\n        ]\n        df[\"SEQ_ASSAY_ID\"] = seq_assay_ids\n        df[\"SEQ_PIPELINE_ID\"] = [\n            assay.upper().replace(\"_\", \"-\") for assay in df[\"SEQ_PIPELINE_ID\"]\n        ]\n        if process_functions.checkColExist(df, \"gene_padding\"):\n            df[\"gene_padding\"] = df[\"gene_padding\"].fillna(10)\n            df[\"gene_padding\"] = df[\"gene_padding\"].astype(int)\n        else:\n            df[\"gene_padding\"] = 10\n\n        if not process_functions.checkColExist(df, \"variant_classifications\"):\n            df[\"variant_classifications\"] = float(\"nan\")\n\n        df[\"CENTER\"] = self.center\n        return df\n\n    def _get_dataframe(self, filepath_list):\n        \"\"\"Take in yaml file, returns dataframe\"\"\"\n        filepath = filepath_list[0]\n        try:\n            with open(filepath, \"r\") as yamlfile:\n                # https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation\n                # Must add this because yaml load deprecation\n                assay_info_dict = yaml.safe_load(yamlfile)\n        except Exception:\n            raise ValueError(\n                \"assay_information.yaml: Can't read in your file. \"\n                \"Please make sure the file is a correctly formatted yaml\"\n            )\n        # assay_info_df = pd.DataFrame(panel_info_dict)\n        # assay_info_df = assay_info_df.transpose()\n        # assay_info_df['SEQ_ASSAY_ID'] = assay_info_df.index\n        # assay_info_df.reset_index(drop=True, inplace=True)\n        assay_infodf = pd.DataFrame(assay_info_dict)\n        assay_info_transposeddf = assay_infodf.transpose()\n\n        all_panel_info = pd.DataFrame()\n        for assay in assay_info_dict:\n            assay_specific_info = assay_info_dict[assay][\"assay_specific_info\"]\n            assay_specific_infodf = pd.DataFrame(assay_specific_info)\n\n            intial_seq_id_infodf = assay_info_transposeddf.loc[[assay]]\n\n            # make sure to create a skeleton for the number of seq assay ids\n            # in the seq pipeline\n            seq_assay_id_infodf = pd.concat(\n                [intial_seq_id_infodf] * len(assay_specific_info)\n            )\n            seq_assay_id_infodf.reset_index(drop=True, inplace=True)\n            assay_finaldf = pd.concat(\n                [assay_specific_infodf, seq_assay_id_infodf], axis=1\n            )\n            del assay_finaldf[\"assay_specific_info\"]\n            # Transform values containing lists to string concatenated values\n            columns_containing_lists = [\n                \"variant_classifications\",\n                \"alteration_types\",\n                \"preservation_technique\",\n                \"coverage\",\n            ]\n\n            for col in columns_containing_lists:\n                if assay_finaldf.get(col) is not None:\n                    assay_finaldf[col] = [\";\".join(row) for row in assay_finaldf[col]]\n            assay_finaldf[\"SEQ_PIPELINE_ID\"] = assay\n            all_panel_info = pd.concat([all_panel_info, assay_finaldf])\n        return all_panel_info\n\n    def _validate(self, assay_info_df):\n        \"\"\"\n        Validates the values of assay information file\n\n        Args:\n            assay_info_df: assay information dataframe\n\n        Returns:\n            tuple: error and warning\n        \"\"\"\n\n        total_error = \"\"\n        warning = \"\"\n\n        if process_functions.checkColExist(assay_info_df, \"SEQ_ASSAY_ID\"):\n            all_seq_assays = (\n                assay_info_df.SEQ_ASSAY_ID.replace({\"_\": \"-\"}, regex=True)\n                .str.upper()\n                .unique()\n            )\n            if not all([assay.startswith(self.center) for assay in all_seq_assays]):\n                total_error += (\n                    \"Assay_information.yaml: Please make sure all your \"\n                    \"SEQ_ASSAY_IDs start with your center abbreviation.\\n\"\n                )\n\n            uniq_seq_df = extract.get_syntabledf(\n                self.syn,\n                f\"select distinct(SEQ_ASSAY_ID) as seq from {self.genie_config['sample']} \"\n                f\"where CENTER = '{self.center}'\",\n            )\n            # These are all the SEQ_ASSAY_IDs that are in the clinical database\n            # but not in the assay_information file\n            missing_seqs = uniq_seq_df[\"seq\"][\n                ~uniq_seq_df[\"seq\"]\n                .replace({\"_\": \"-\"}, regex=True)\n                .str.upper()\n                .isin(all_seq_assays)\n            ]\n            missing_seqs_str = \", \".join(missing_seqs)\n            if missing_seqs.to_list():\n                total_error += (\n                    \"Assay_information.yaml: You are missing SEQ_ASSAY_IDs: \"\n                    f\"{missing_seqs_str}\\n\"\n                )\n\n        else:\n            total_error += \"Assay_information.yaml: Must have SEQ_ASSAY_ID column.\\n\"\n\n        read_group_dict = process_functions.get_gdc_data_dictionary(\"read_group\")\n        read_group_headers = read_group_dict[\"properties\"]\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"is_paired_end\",\n            [True, False],\n            filename=\"Assay_information.yaml\",\n            required=True,\n        )\n        warning += warn\n        total_error += error\n\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"library_selection\",\n            read_group_headers[\"library_selection\"][\"enum\"],\n            filename=\"Assay_information.yaml\",\n            required=True,\n        )\n        warning += warn\n        total_error += error\n\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"library_strategy\",\n            [\"Targeted Sequencing\", \"WXS\"],\n            filename=\"Assay_information.yaml\",\n            required=True,\n        )\n        warning += warn\n        total_error += error\n\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"platform\",\n            read_group_headers[\"platform\"][\"enum\"],\n            filename=\"Assay_information.yaml\",\n            required=True,\n        )\n        warning += warn\n        total_error += error\n\n        instrument_model = read_group_headers[\"instrument_model\"][\"enum\"]\n        instrument_model.extend([\"Illumina NovaSeq 6000\", None])\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"instrument_model\",\n            instrument_model,\n            filename=\"Assay_information.yaml\",\n            required=True,\n        )\n        warning += warn\n        total_error += error\n\n        if not process_functions.checkColExist(assay_info_df, \"target_capture_kit\"):\n            total_error += (\n                \"Assay_information.yaml: \" \"Must have target_capture_kit column.\\n\"\n            )\n\n        variant_classes = [\n            \"Splice_Site\",\n            \"Nonsense_Mutation\",\n            \"Frame_Shift_Del\",\n            \"Frame_Shift_Ins\",\n            \"Nonstop_Mutation\",\n            \"Translation_Start_Site\",\n            \"In_Frame_Ins\",\n            \"In_Frame_Del\",\n            \"Missense_Mutation\",\n            \"Intron\",\n            \"Splice_Region\",\n            \"Silent\",\n            \"RNA\",\n            \"5'UTR\",\n            \"3'UTR\",\n            \"IGR\",\n            \"5'Flank\",\n            \"3'Flank\",\n            None,\n        ]\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"variant_classifications\",\n            variant_classes,\n            filename=\"Assay_information.yaml\",\n            na_allowed=True,\n            sep=\";\",\n        )\n        warning += warn\n        total_error += error\n\n        if process_functions.checkColExist(assay_info_df, \"read_length\"):\n            if not all(\n                [\n                    process_functions.checkInt(i)\n                    for i in assay_info_df[\"read_length\"]\n                    if i is not None and not pd.isnull(i)\n                ]\n            ):\n                total_error += (\n                    \"Assay_information.yaml: \"\n                    \"Please double check your read_length.  \"\n                    \"It must be an integer or null.\\n\"\n                )\n        else:\n            total_error += \"Assay_information.yaml: \" \"Must have read_length column.\\n\"\n\n        if process_functions.checkColExist(assay_info_df, \"number_of_genes\"):\n            if not all(\n                [\n                    process_functions.checkInt(i)\n                    for i in assay_info_df[\"number_of_genes\"]\n                ]\n            ):\n                total_error += (\n                    \"Assay_information.yaml: \"\n                    \"Please double check your number_of_genes. \"\n                    \"It must be an integer.\\n\"\n                )\n        else:\n            total_error += (\n                \"Assay_information.yaml: \" \"Must have number_of_genes column.\\n\"\n            )\n\n        if process_functions.checkColExist(assay_info_df, \"gene_padding\"):\n            if not all(\n                [\n                    process_functions.checkInt(i)\n                    for i in assay_info_df[\"gene_padding\"]\n                    if i is not None and not pd.isnull(i)\n                ]\n            ):\n                total_error += (\n                    \"Assay_information.yaml: \"\n                    \"Please double check your gene_padding. \"\n                    \"It must be an integer or blank.\\n\"\n                )\n        else:\n            warning += (\n                \"Assay_information.yaml: \"\n                \"gene_padding is by default 10 if not specified.\\n\"\n            )\n\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"calling_strategy\",\n            [\"tumor_only\", \"tumor_normal\", \"plasma_normal\"],\n            filename=\"Assay_information.yaml\",\n            required=True,\n        )\n        warning += warn\n        total_error += error\n\n        if process_functions.checkColExist(assay_info_df, \"specimen_tumor_cellularity\"):\n            if not all(\n                [\n                    i.startswith(\"&gt;\") and i.endswith(\"%\")\n                    for i in assay_info_df[\"specimen_tumor_cellularity\"]\n                ]\n            ):\n                total_error += (\n                    \"Assay_information.yaml: \"\n                    \"Please double check your specimen_tumor_cellularity. \"\n                    \"It must in this format &gt;(num)%. ie. &gt;10%\\n\"\n                )\n        else:\n            total_error += (\n                \"Assay_information.yaml: \"\n                \"Must have specimen_tumor_cellularity column.\\n\"\n            )\n\n        alteration_types = [\n            \"snv\",\n            \"small_indels\",\n            \"gene_level_cna\",\n            \"intragenic_cna\",\n            \"structural_variants\",\n        ]\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"alteration_types\",\n            alteration_types,\n            filename=\"Assay_information.yaml\",\n            required=True,\n            sep=\";\",\n        )\n        warning += warn\n        total_error += error\n\n        preservation_technique = [\"FFPE\", \"fresh_frozen\", \"NA\"]\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"preservation_technique\",\n            preservation_technique,\n            filename=\"Assay_information.yaml\",\n            required=True,\n            sep=\";\",\n        )\n        warning += warn\n        total_error += error\n\n        coverage = [\"hotspot_regions\", \"coding_exons\", \"introns\", \"promoters\"]\n        warn, error = process_functions.check_col_and_values(\n            assay_info_df,\n            \"coverage\",\n            coverage,\n            filename=\"Assay_information.yaml\",\n            required=True,\n            sep=\";\",\n        )\n        warning += warn\n        total_error += error\n\n        return total_error, warning\n</code></pre>"},{"location":"reference/fileformats/assay/#genie_registry.assay.Assayinfo-functions","title":"Functions","text":""},{"location":"reference/fileformats/assay/#genie_registry.assay.Assayinfo.process_steps","title":"<code>process_steps(assay_info_df, newPath, databaseSynId)</code>","text":"<p>Process bed input and update bed database</p> PARAMETER DESCRIPTION <code>assay_info_df</code> <p>Assay information dataframe</p> <p> </p> <code>newPath</code> <p>Path to processed assay information</p> <p> </p> <code>databaseSynId</code> <p>assay information database synapse id</p> <p> </p> RETURNS DESCRIPTION <p>path to assay information dataframe</p> Source code in <code>genie_registry/assay.py</code> <pre><code>def process_steps(self, assay_info_df, newPath, databaseSynId):\n    \"\"\"\n    Process bed input and update bed database\n\n    Args:\n        assay_info_df: Assay information dataframe\n        newPath: Path to processed assay information\n        databaseSynId: assay information database synapse id\n\n    Returns:\n        path to assay information dataframe\n    \"\"\"\n    # Must pass in a list\n    process_assay_info_df = self._process(assay_info_df)\n    load.update_table(\n        syn=self.syn,\n        databaseSynId=databaseSynId,\n        newData=process_assay_info_df,\n        filterBy=self.center,\n        toDelete=True,\n    )\n    process_assay_info_df.to_csv(newPath, sep=\"\\t\", index=False)\n    return newPath\n</code></pre>"},{"location":"reference/fileformats/bed/","title":"Bed","text":""},{"location":"reference/fileformats/bed/#genie_registry.bed","title":"<code>genie_registry.bed</code>","text":"<p>GENIE bed class and functions</p>"},{"location":"reference/fileformats/bed/#genie_registry.bed-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/bed/#genie_registry.bed.LOGGER","title":"<code>LOGGER = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/bed/#genie_registry.bed-classes","title":"Classes","text":""},{"location":"reference/fileformats/bed/#genie_registry.bed.bed","title":"<code>bed</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> <p>GENIE bed format</p> Source code in <code>genie_registry/bed.py</code> <pre><code>class bed(FileTypeFormat):\n    \"\"\"GENIE bed format\"\"\"\n\n    _fileType = \"bed\"\n\n    _process_kwargs = [\"newPath\", \"parentId\", \"databaseSynId\", \"seq_assay_id\"]\n\n    def _get_dataframe(self, filepathlist):\n        \"\"\"\n        Bed files don't have a header\n\n        Args:\n            filePathList: List of files\n        \"\"\"\n        filepath = filepathlist[0]\n        try:\n            beddf = pd.read_csv(filepath, sep=\"\\t\", header=None)\n        except Exception:\n            raise ValueError(\n                \"Can't read in your bed file. \"\n                \"Please make sure the BED file is not binary and \"\n                \"does not contain a comment/header line\"\n            )\n        first_value = str(beddf[0][0])\n        if (\n            not first_value.isdigit()\n            and not first_value.startswith(\"chr\")\n            and first_value not in [\"X\", \"Y\"]\n        ):\n            raise ValueError(\n                \"Please make sure your bed file does not \"\n                \"contain a comment/header line\"\n            )\n        return beddf\n\n    def _validateFilename(self, filepath):\n        \"\"\"\n        Validates filename\n        CENTER-11.bed\n\n        Args:\n            filePath: Path to bedfile\n        \"\"\"\n\n        assert os.path.basename(filepath[0]).startswith(\n            \"%s-\" % self.center\n        ) and os.path.basename(filepath[0]).endswith(\".bed\")\n\n    def create_gene_panel(self, beddf, seq_assay_id, gene_panel_path, parentid):\n        \"\"\"\n        Create bed file and gene panel files from the bed file\n\n        Args:\n            beddf: bed dataframe\n            seq_assay_id: GENIE SEQ_ASSAY_ID\n            gene_panel_path: Gene panel folder path\n            parentid: Synapse id of gene panel folder\n\n        Returns:\n            pd.DataFrame: configured bed dataframe\n        \"\"\"\n        LOGGER.info(\"CREATING GENE PANEL\")\n        if not beddf.empty:\n            exonsdf = beddf[beddf[\"Feature_Type\"] == \"exon\"]\n            # Only include genes that should be included in the panels\n            include_exonsdf = exonsdf[exonsdf[\"includeInPanel\"]]\n            # Write gene panel\n            null_genes = include_exonsdf[\"Hugo_Symbol\"].isnull()\n            unique_genes = set(include_exonsdf[\"Hugo_Symbol\"][~null_genes])\n            gene_panel_text = (\n                \"stable_id: {seq_assay_id}\\n\"\n                \"description: {seq_assay_id}, \"\n                \"Number of Genes - {num_genes}\\n\"\n                \"gene_list:\\t{genelist}\".format(\n                    seq_assay_id=seq_assay_id,\n                    num_genes=len(unique_genes),\n                    genelist=\"\\t\".join(unique_genes),\n                )\n            )\n            gene_panel_name = \"data_gene_panel_\" + seq_assay_id + \".txt\"\n\n            with open(os.path.join(gene_panel_path, gene_panel_name), \"w+\") as f:\n                f.write(gene_panel_text)\n\n            annotations = {\n                \"fileFormat\": \"bed\",\n                \"dataSubType\": \"metadata\",\n                \"cBioFileFormat\": \"genePanel\",\n                \"center\": self.center,\n                \"species\": \"Human\",\n                \"consortium\": \"GENIE\",\n                \"dataType\": \"genomicVariants\",\n                \"fundingAgency\": \"AACR\",\n                \"assay\": \"targetGeneSeq\",\n                \"fileStage\": \"staging\",\n            }\n            load.store_file(\n                syn=self.syn,\n                filepath=os.path.join(gene_panel_path, gene_panel_name),\n                parentid=parentid,\n                annotations=annotations,\n            )\n\n    def _process(self, beddf, seq_assay_id, newpath, parentid, create_panel=True):\n        \"\"\"\n        Process bed file, add feature type\n\n        Args:\n            gene: bed dataframe\n            seq_assay_id: GENIE SEQ_ASSAY_ID\n            newpath: new GENIE path\n            parentid: Synapse id to store the gene panel\n            create_panel: Create gene panel\n\n        Returns:\n            pd.DataFrame: Conigured bed dataframe\n        \"\"\"\n        seq_assay_id = seq_assay_id.upper()\n        seq_assay_id = seq_assay_id.replace(\"_\", \"-\")\n\n        # Add in 6th column which is the clinicalReported\n        if len(beddf.columns) &gt; 5:\n            if all(beddf[5].apply(lambda x: x in [True, False])):\n                beddf[5] = beddf[5].astype(bool)\n            else:\n                beddf[5] = float(\"nan\")\n        else:\n            beddf[5] = float(\"nan\")\n        beddf = beddf[[0, 1, 2, 3, 4, 5]]\n        gene_panel_path = os.path.dirname(newpath)\n        # Must be .astype(bool) because `1, 0 in [True, False]`\n        beddf[4] = beddf[4].astype(bool)\n\n        exon_gtf_path, gene_gtf_path = create_gtf(process_functions.SCRIPT_DIR)\n        LOGGER.info(\"REMAPPING {}\".format(seq_assay_id))\n        # bedname = seq_assay_id + \".bed\"\n        beddf.columns = [\n            \"Chromosome\",\n            \"Start_Position\",\n            \"End_Position\",\n            \"Hugo_Symbol\",\n            \"includeInPanel\",\n            \"clinicalReported\",\n        ]\n        # Validate gene symbols\n        # Gene symbols can be split by ; and _ and : and .\n        beddf[\"Hugo_Symbol\"] = [\n            i.split(\";\")[0].split(\"_\")[0].split(\":\")[0].split(\".\")[0]\n            for i in beddf[\"Hugo_Symbol\"]\n        ]\n        # Replace all chr with blank\n        beddf[\"Chromosome\"] = [str(i).replace(\"chr\", \"\") for i in beddf[\"Chromosome\"]]\n        # Change all start and end to int\n        beddf[\"Start_Position\"] = beddf[\"Start_Position\"].apply(int)\n        beddf[\"End_Position\"] = beddf[\"End_Position\"].apply(int)\n\n        gene_position_table = self.syn.tableQuery(\"SELECT * FROM syn11806563\")\n        gene_positiondf = gene_position_table.asDataFrame()\n        beddf[\"ID\"] = beddf[\"Hugo_Symbol\"]\n        # The apply function of a DataFrame is called twice on the first\n        # row (known pandas behavior)\n        beddf = beddf.apply(lambda x: remap_symbols(x, gene_positiondf), axis=1)\n        beddf[\"SEQ_ASSAY_ID\"] = seq_assay_id\n        temp_bed_path = os.path.join(process_functions.SCRIPT_DIR, \"temp.bed\")\n        beddf.to_csv(temp_bed_path, sep=\"\\t\", index=False, header=None)\n        final_bed = add_feature_type(temp_bed_path, exon_gtf_path, gene_gtf_path)\n        final_bed[\"CENTER\"] = self.center\n        final_bed[\"Chromosome\"] = final_bed[\"Chromosome\"].astype(str)\n        if create_panel:\n            self.create_gene_panel(final_bed, seq_assay_id, gene_panel_path, parentid)\n        return final_bed\n\n    def preprocess(self, newpath):\n        \"\"\"\n        Standardize and grab seq assay id from the bed file path\n\n        Args:\n            newpath: bed file path\n\n        Returns:\n            dict: GENIE seq assay id\n        \"\"\"\n        seq_assay_id = os.path.basename(newpath).replace(\".bed\", \"\")\n        seq_assay_id = seq_assay_id.upper().replace(\"_\", \"-\")\n        return {\"seq_assay_id\": seq_assay_id}\n\n    def process_steps(\n        self,\n        beddf: pd.DataFrame,\n        newPath: str,\n        parentId: str,\n        databaseSynId: str,\n        seq_assay_id: str,\n    ) -&gt; str:\n        \"\"\"Process bed file, update bed database, write bed file to path\n\n        Args:\n            beddf (pd.DataFrame): input bed data\n            newPath (str): Path to new bed file\n            parentId (str): Synapse id to store gene panel file\n            databaseSynId (str): Synapse id of bed database\n            seq_assay_id (str): GENIE seq assay id\n\n        Returns:\n            str: Path to new bed file\n        \"\"\"\n        final_beddf = self._process(\n            beddf=beddf, seq_assay_id=seq_assay_id, newpath=newPath, parentid=parentId\n        )\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=databaseSynId,\n            newData=final_beddf,\n            filterBy=seq_assay_id,\n            filterByColumn=\"SEQ_ASSAY_ID\",\n            toDelete=True,\n        )\n        final_beddf.to_csv(newPath, sep=\"\\t\", index=False)\n        return newPath\n\n    def _validate(self, beddf):\n        \"\"\"\n        Validate bed file\n\n        Args:\n            bed: Bed dataframe\n\n        Returns:\n            total_error: all the errors\n            warning: all the warnings\n        \"\"\"\n        total_error = \"\"\n        warning = \"\"\n        newcols = [\n            \"Chromosome\",\n            \"Start_Position\",\n            \"End_Position\",\n            \"Hugo_Symbol\",\n            \"includeInPanel\",\n        ]\n        if len(beddf.columns) &lt; len(newcols):\n            total_error += (\n                \"BED file: Must at least have five columns in this \"\n                \"order: {}. Make sure there are \"\n                \"no headers.\\n\".format(\", \".join(newcols))\n            )\n        else:\n            newcols.extend(range(0, len(beddf.columns) - len(newcols)))\n            beddf.columns = newcols\n            to_validate_symbol = True\n            if not all(beddf[\"Start_Position\"].apply(lambda x: isinstance(x, int))):\n                total_error += (\n                    \"BED file: \"\n                    \"The Start_Position column must only be \"\n                    \"integers. Make sure there are no headers.\\n\"\n                )\n                to_validate_symbol = False\n            if not all(beddf[\"End_Position\"].apply(lambda x: isinstance(x, int))):\n                total_error += (\n                    \"BED file: \"\n                    \"The End_Position column must only be \"\n                    \"integers. Make sure there are no headers.\\n\"\n                )\n                to_validate_symbol = False\n\n            LOGGER.info(\"VALIDATING GENE SYMBOLS\")\n            if any(beddf[\"Hugo_Symbol\"].isnull()):\n                total_error += \"BED file: You cannot submit any null symbols.\\n\"\n            beddf = beddf[~beddf[\"Hugo_Symbol\"].isnull()]\n            beddf[\"Hugo_Symbol\"] = [\n                str(hugo).split(\";\")[0].split(\"_\")[0].split(\":\")[0]\n                for hugo in beddf[\"Hugo_Symbol\"]\n            ]\n            if (\n                sum(beddf[\"Hugo_Symbol\"] == \"+\") != 0\n                or sum(beddf[\"Hugo_Symbol\"] == \"-\") != 0\n            ):\n                total_error += (\n                    \"BED file: Fourth column must be the \"\n                    \"Hugo_Symbol column, not the strand column\\n\"\n                )\n\n            warn, error = process_functions.check_col_and_values(\n                beddf,\n                \"includeInPanel\",\n                [True, False],\n                filename=\"BED file\",\n                required=True,\n            )\n            warning += warn\n            total_error += error\n\n            if to_validate_symbol:\n                gene_position_table = self.syn.tableQuery(\"SELECT * FROM syn11806563\")\n                gene_positiondf = gene_position_table.asDataFrame()\n                # The apply function of a DataFrame is called twice on the first row (known\n                # pandas behavior)\n                beddf = beddf.apply(lambda x: remap_symbols(x, gene_positiondf), axis=1)\n\n                if any(beddf[\"Hugo_Symbol\"].isnull()):\n                    warning += (\n                        \"BED file: \"\n                        \"Any gene names that can't be \"\n                        \"remapped will be null.\\n\"\n                    )\n                if all(beddf[\"Hugo_Symbol\"].isnull()):\n                    total_error += (\n                        \"BED file: \"\n                        \"You have no correct gene symbols. \"\n                        \"Make sure your gene symbol column (4th \"\n                        \"column) is formatted like so: SYMBOL\"\n                        \"(;optionaltext).  Optional text can be \"\n                        \"semi-colon separated.\\n\"\n                    )\n            # validate on required chromosome col, chr is allowed\n            # as we tackle it later in processing\n            error, warn = validate._validate_chromosome(\n                df=beddf, col=\"Chromosome\", fileformat=\"BED file\", allow_chr=True\n            )\n            total_error += error\n            warning += warn\n\n        return (total_error, warning)\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed.bed-functions","title":"Functions","text":""},{"location":"reference/fileformats/bed/#genie_registry.bed.bed.create_gene_panel","title":"<code>create_gene_panel(beddf, seq_assay_id, gene_panel_path, parentid)</code>","text":"<p>Create bed file and gene panel files from the bed file</p> PARAMETER DESCRIPTION <code>beddf</code> <p>bed dataframe</p> <p> </p> <code>seq_assay_id</code> <p>GENIE SEQ_ASSAY_ID</p> <p> </p> <code>gene_panel_path</code> <p>Gene panel folder path</p> <p> </p> <code>parentid</code> <p>Synapse id of gene panel folder</p> <p> </p> RETURNS DESCRIPTION <p>pd.DataFrame: configured bed dataframe</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def create_gene_panel(self, beddf, seq_assay_id, gene_panel_path, parentid):\n    \"\"\"\n    Create bed file and gene panel files from the bed file\n\n    Args:\n        beddf: bed dataframe\n        seq_assay_id: GENIE SEQ_ASSAY_ID\n        gene_panel_path: Gene panel folder path\n        parentid: Synapse id of gene panel folder\n\n    Returns:\n        pd.DataFrame: configured bed dataframe\n    \"\"\"\n    LOGGER.info(\"CREATING GENE PANEL\")\n    if not beddf.empty:\n        exonsdf = beddf[beddf[\"Feature_Type\"] == \"exon\"]\n        # Only include genes that should be included in the panels\n        include_exonsdf = exonsdf[exonsdf[\"includeInPanel\"]]\n        # Write gene panel\n        null_genes = include_exonsdf[\"Hugo_Symbol\"].isnull()\n        unique_genes = set(include_exonsdf[\"Hugo_Symbol\"][~null_genes])\n        gene_panel_text = (\n            \"stable_id: {seq_assay_id}\\n\"\n            \"description: {seq_assay_id}, \"\n            \"Number of Genes - {num_genes}\\n\"\n            \"gene_list:\\t{genelist}\".format(\n                seq_assay_id=seq_assay_id,\n                num_genes=len(unique_genes),\n                genelist=\"\\t\".join(unique_genes),\n            )\n        )\n        gene_panel_name = \"data_gene_panel_\" + seq_assay_id + \".txt\"\n\n        with open(os.path.join(gene_panel_path, gene_panel_name), \"w+\") as f:\n            f.write(gene_panel_text)\n\n        annotations = {\n            \"fileFormat\": \"bed\",\n            \"dataSubType\": \"metadata\",\n            \"cBioFileFormat\": \"genePanel\",\n            \"center\": self.center,\n            \"species\": \"Human\",\n            \"consortium\": \"GENIE\",\n            \"dataType\": \"genomicVariants\",\n            \"fundingAgency\": \"AACR\",\n            \"assay\": \"targetGeneSeq\",\n            \"fileStage\": \"staging\",\n        }\n        load.store_file(\n            syn=self.syn,\n            filepath=os.path.join(gene_panel_path, gene_panel_name),\n            parentid=parentid,\n            annotations=annotations,\n        )\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed.bed.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>Standardize and grab seq assay id from the bed file path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>bed file path</p> <p> </p> RETURNS DESCRIPTION <code>dict</code> <p>GENIE seq assay id</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    Standardize and grab seq assay id from the bed file path\n\n    Args:\n        newpath: bed file path\n\n    Returns:\n        dict: GENIE seq assay id\n    \"\"\"\n    seq_assay_id = os.path.basename(newpath).replace(\".bed\", \"\")\n    seq_assay_id = seq_assay_id.upper().replace(\"_\", \"-\")\n    return {\"seq_assay_id\": seq_assay_id}\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed.bed.process_steps","title":"<code>process_steps(beddf, newPath, parentId, databaseSynId, seq_assay_id)</code>","text":"<p>Process bed file, update bed database, write bed file to path</p> PARAMETER DESCRIPTION <code>beddf</code> <p>input bed data</p> <p> TYPE: <code>DataFrame</code> </p> <code>newPath</code> <p>Path to new bed file</p> <p> TYPE: <code>str</code> </p> <code>parentId</code> <p>Synapse id to store gene panel file</p> <p> TYPE: <code>str</code> </p> <code>databaseSynId</code> <p>Synapse id of bed database</p> <p> TYPE: <code>str</code> </p> <code>seq_assay_id</code> <p>GENIE seq assay id</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Path to new bed file</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie_registry/bed.py</code> <pre><code>def process_steps(\n    self,\n    beddf: pd.DataFrame,\n    newPath: str,\n    parentId: str,\n    databaseSynId: str,\n    seq_assay_id: str,\n) -&gt; str:\n    \"\"\"Process bed file, update bed database, write bed file to path\n\n    Args:\n        beddf (pd.DataFrame): input bed data\n        newPath (str): Path to new bed file\n        parentId (str): Synapse id to store gene panel file\n        databaseSynId (str): Synapse id of bed database\n        seq_assay_id (str): GENIE seq assay id\n\n    Returns:\n        str: Path to new bed file\n    \"\"\"\n    final_beddf = self._process(\n        beddf=beddf, seq_assay_id=seq_assay_id, newpath=newPath, parentid=parentId\n    )\n    load.update_table(\n        syn=self.syn,\n        databaseSynId=databaseSynId,\n        newData=final_beddf,\n        filterBy=seq_assay_id,\n        filterByColumn=\"SEQ_ASSAY_ID\",\n        toDelete=True,\n    )\n    final_beddf.to_csv(newPath, sep=\"\\t\", index=False)\n    return newPath\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed-functions","title":"Functions","text":""},{"location":"reference/fileformats/bed/#genie_registry.bed.create_gtf","title":"<code>create_gtf(dirname)</code>","text":"<p>Create exon.gtf and gene.gtf from GRCh37 gtf</p> PARAMETER DESCRIPTION <code>dirname</code> <p>Directory where these files should live</p> <p> </p> RETURNS DESCRIPTION <code>exon_gtf_path</code> <p>exon GTF</p> <code>gene_gtf_path</code> <p>gene GTF</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def create_gtf(dirname):\n    \"\"\"\n    Create exon.gtf and gene.gtf from GRCh37 gtf\n\n    Args:\n        dirname: Directory where these files should live\n\n    Returns:\n        exon_gtf_path: exon GTF\n        gene_gtf_path: gene GTF\n    \"\"\"\n    exon_gtf_path = os.path.join(dirname, \"exon.gtf\")\n    gene_gtf_path = os.path.join(dirname, \"gene.gtf\")\n\n    if not os.path.exists(exon_gtf_path) or not os.path.exists(gene_gtf_path):\n        download_cmd = [\n            \"wget\",\n            \"http://ftp.ensembl.org/pub/release-75/gtf/homo_sapiens/Homo_sapiens.GRCh37.75.gtf.gz\",\n            \"-P\",\n            dirname,\n        ]\n        subprocess.check_call(download_cmd)\n        gtfgz_path = os.path.join(dirname, \"Homo_sapiens.GRCh37.75.gtf.gz\")\n        gunzip_cmd = [\"gunzip\", \"-f\", gtfgz_path]\n        subprocess.check_call(gunzip_cmd)\n        gtf_path = os.path.join(dirname, \"Homo_sapiens.GRCh37.75.gtf\")\n\n        exon_awk_cmd = [\"awk\", '$3 == \"exon\" {print}', gtf_path]\n        exon_gtf = subprocess.check_output(exon_awk_cmd, universal_newlines=True)\n        with open(exon_gtf_path, \"w\") as gtf_file:\n            gtf_file.write(exon_gtf)\n        gene_awk_cmd = [\"awk\", '$3 == \"gene\" {print}', gtf_path]\n        gene_gtf = subprocess.check_output(gene_awk_cmd, universal_newlines=True)\n        with open(gene_gtf_path, \"w\") as gtf_file:\n            gtf_file.write(gene_gtf)\n    return (exon_gtf_path, gene_gtf_path)\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed._add_feature_type_tobeddf","title":"<code>_add_feature_type_tobeddf(filepath, featuretype)</code>","text":"<p>Add Feature_Type to dataframe</p> PARAMETER DESCRIPTION <code>filepath</code> <p>path to bed</p> <p> </p> <code>featuretype</code> <p>exon, intron, or intergenic</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>empty dataframe or dataframe with appended feature type</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def _add_feature_type_tobeddf(filepath, featuretype):\n    \"\"\"\n    Add Feature_Type to dataframe\n\n    Args:\n        filepath: path to bed\n        featuretype: exon, intron, or intergenic\n\n    Returns:\n        df: empty dataframe or dataframe with appended feature type\n    \"\"\"\n    bed_columns = [\n        \"Chromosome\",\n        \"Start_Position\",\n        \"End_Position\",\n        \"Hugo_Symbol\",\n        \"includeInPanel\",\n        \"clinicalReported\",\n        \"ID\",\n        \"SEQ_ASSAY_ID\",\n        \"Feature_Type\",\n    ]\n    # No need to add anything if the dataframe is empty\n    if os.stat(filepath).st_size != 0:\n        beddf = pd.read_csv(filepath, sep=\"\\t\", header=None)\n        beddf[\"Feature_Type\"] = featuretype\n        beddf.columns = bed_columns\n    else:\n        beddf = pd.DataFrame(columns=bed_columns)\n    return beddf\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed.add_feature_type","title":"<code>add_feature_type(temp_bed_path, exon_gtf_path, gene_gtf_path)</code>","text":"<p>Add Feature_Type to bed file (exon, intron, intergenic)</p> PARAMETER DESCRIPTION <code>temp_bed_path</code> <p>BED file without feature type</p> <p> </p> <code>exon_gtf_path</code> <p>exon gtf</p> <p> </p> <code>gene_gtf_path</code> <p>gene gtf</p> <p> </p> RETURNS DESCRIPTION <code>genie_combined_path</code> <p>Path to final bed file</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def add_feature_type(temp_bed_path, exon_gtf_path, gene_gtf_path):\n    \"\"\"\n    Add Feature_Type to bed file (exon, intron, intergenic)\n\n    Args:\n        temp_bed_path: BED file without feature type\n        exon_gtf_path: exon gtf\n        gene_gtf_path: gene gtf\n\n    Returns:\n        genie_combined_path: Path to final bed file\n    \"\"\"\n    genie_exon_path = os.path.join(process_functions.SCRIPT_DIR, \"genie_exons.bed\")\n    genie_intron_path = os.path.join(process_functions.SCRIPT_DIR, \"genie_introns.bed\")\n    genie_intergenic_path = os.path.join(\n        process_functions.SCRIPT_DIR, \"genie_intergenic.bed\"\n    )\n    intron_intergenic_path = os.path.join(\n        process_functions.SCRIPT_DIR, \"intron_intergenic.bed\"\n    )\n    gene_path = os.path.join(process_functions.SCRIPT_DIR, \"gene.bed\")\n    # GET EXON REGIONS\n    # Get intersection between true exon regions and submitted bed regions\n    command = [\n        \"bedtools\",\n        \"intersect\",\n        \"-a\",\n        temp_bed_path,\n        \"-b\",\n        exon_gtf_path,\n        \"-wa\",\n        \"|\",\n        \"sort\",\n        \"|\",\n        \"uniq\",\n        \"&gt;\",\n        genie_exon_path,\n    ]\n    subprocess.check_call(\" \".join(command), shell=True)\n    # get intergenic/intron regions\n    # Get opposite of intersection between true exon regions and submitted\n    # bed regions\n    command = [\n        \"bedtools\",\n        \"intersect\",\n        \"-a\",\n        temp_bed_path,\n        \"-b\",\n        exon_gtf_path,\n        \"-wa\",\n        \"-v\",\n        \"|\",\n        \"sort\",\n        \"|\",\n        \"uniq\",\n        \"&gt;\",\n        intron_intergenic_path,\n    ]\n    subprocess.check_call(\" \".join(command), shell=True)\n    # get gene regions\n    # Get intersection between true gene regions and submitted bed regions\n    command = [\n        \"bedtools\",\n        \"intersect\",\n        \"-a\",\n        temp_bed_path,\n        \"-b\",\n        gene_gtf_path,\n        \"-wa\",\n        \"|\",\n        \"sort\",\n        \"|\",\n        \"uniq\",\n        \"&gt;\",\n        gene_path,\n    ]\n    subprocess.check_call(\" \".join(command), shell=True)\n    # GET INTRON REGSIONS\n    # Difference between the gene regions and exon regions will give\n    # intron regions\n    command = [\n        \"diff\",\n        gene_path,\n        genie_exon_path,\n        \"|\",\n        \"grep\",\n        \"'&lt;'\",\n        \"|\",\n        \"sed\",\n        \"'s/&lt; //'\",\n        \"&gt;\",\n        genie_intron_path,\n    ]\n    subprocess.check_call(\" \".join(command), shell=True)\n    # GET INTERGENIC REGIONS\n    # Difference between the intron/intergenic and intron regions will\n    # give intergenic regions\n    command = [\n        \"diff\",\n        intron_intergenic_path,\n        genie_intron_path,\n        \"|\",\n        \"grep\",\n        \"'&lt;'\",\n        \"|\",\n        \"sed\",\n        \"'s/&lt; //'\",\n        \"&gt;\",\n        genie_intergenic_path,\n    ]\n    subprocess.check_call(\" \".join(command), shell=True)\n\n    genie_exondf = _add_feature_type_tobeddf(genie_exon_path, \"exon\")\n    genie_introndf = _add_feature_type_tobeddf(genie_intron_path, \"intron\")\n    genie_intergenicdf = _add_feature_type_tobeddf(genie_intergenic_path, \"intergenic\")\n    genie_combineddf = pd.concat([genie_exondf, genie_introndf, genie_intergenicdf])\n    return genie_combineddf\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed._check_region_overlap","title":"<code>_check_region_overlap(row, gene_positiondf)</code>","text":"<p>Check if the submitted bed symbol + region overlaps with the actual gene's positions</p> PARAMETER DESCRIPTION <code>row</code> <p>row in bed file (genomic region)</p> <p> </p> <code>gene_positiondf</code> <p>Reference gene position dataframe</p> <p> </p> Return <p>True if the region does overlap.</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def _check_region_overlap(row, gene_positiondf):\n    \"\"\"\n    Check if the submitted bed symbol + region overlaps with\n    the actual gene's positions\n\n    Args:\n        row: row in bed file (genomic region)\n        gene_positiondf: Reference gene position dataframe\n\n    Return:\n        True if the region does overlap.\n    \"\"\"\n    matching_symbol_ind = gene_positiondf[\"hgnc_symbol\"] == row[\"Hugo_Symbol\"]\n    match_with_genedb = gene_positiondf[matching_symbol_ind]\n\n    if not match_with_genedb.empty:\n        # We are assuming that there is only one matching gene, but\n        # there are actually duplicated gene symbols\n        start_position = match_with_genedb[\"start_position\"].values[0]\n        end_position = match_with_genedb[\"end_position\"].values[0]\n        # Submitted bed start position in a gene\n        start_in_gene = start_position &lt;= row[\"Start_Position\"] &lt;= end_position\n        # Submitted bed end position in a gene\n        end_in_gene = start_position &lt;= row[\"End_Position\"] &lt;= end_position\n        # Submitted bed region surrounds a gene\n        gene_in_region = (\n            end_position &lt;= row[\"End_Position\"]\n            and start_position &gt;= row[\"Start_Position\"]\n        )\n        return start_in_gene or end_in_gene or gene_in_region\n    return False\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed._get_max_overlap_index","title":"<code>_get_max_overlap_index(overlap, bed_length, boundary)</code>","text":"<p>Calculate the ratio of overlap between the submitted bed region and gene position dataframe and return the index of the max overlapping region</p> PARAMETER DESCRIPTION <code>overlap</code> <p>Possible overlapping region</p> <p> </p> <code>bed_length</code> <p>Length of submitted region</p> <p> </p> <code>boundary</code> <p>specified ratio overlap</p> <p> </p> RETURNS DESCRIPTION <p>Index of regions with maximum overlap or None</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def _get_max_overlap_index(overlap, bed_length, boundary):\n    \"\"\"\n    Calculate the ratio of overlap between the submitted bed\n    region and gene position dataframe and return the index of\n    the max overlapping region\n\n    Args:\n        overlap: Possible overlapping region\n        bed_length: Length of submitted region\n        boundary: specified ratio overlap\n\n    Returns:\n        Index of regions with maximum overlap or None\n    \"\"\"\n    ratio_overlap = overlap / bed_length\n    ratio_overlap = ratio_overlap[ratio_overlap &gt; boundary]\n    ratio_overlap = ratio_overlap[ratio_overlap &lt;= 1]\n    if not ratio_overlap.empty:\n        return ratio_overlap.idxmax()\n    return None\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed._map_position_within_boundary","title":"<code>_map_position_within_boundary(row, positiondf, boundary=0.9)</code>","text":"<p>Map positions and checks if posision is contained within the specified percentage boundary</p> PARAMETER DESCRIPTION <code>row</code> <p>Row in bed file (genomic region)</p> <p> </p> <code>positiondf</code> <p>Reference bed position dataframe</p> <p> </p> <code>boundary</code> <p>Percent boundary defined</p> <p> DEFAULT: <code>0.9</code> </p> Return <p>pd.Series: mapped position</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def _map_position_within_boundary(row, positiondf, boundary=0.9):\n    \"\"\"\n    Map positions and checks if posision is contained within the\n    specified percentage boundary\n\n    Args:\n        row: Row in bed file (genomic region)\n        positiondf: Reference bed position dataframe\n        boundary: Percent boundary defined\n\n    Return:\n        pd.Series: mapped position\n    \"\"\"\n    # First filter just based on whether or not region is within the gene\n    # position dataframe\n    match_chr = positiondf[\"chromosome_name\"] == str(row[\"Chromosome\"])\n    chrom_rows = positiondf[match_chr]\n    match_start = chrom_rows[\"start_position\"] &lt;= row[\"Start_Position\"]\n    start_rows = chrom_rows[match_start]\n    end_rows = start_rows[start_rows[\"end_position\"] &gt;= row[\"End_Position\"]]\n\n    bed_length = row[\"End_Position\"] - row[\"Start_Position\"]\n    # as long as the strand is within the boundary % defined\n    # Start goes over start boundary, but end is contained in position\n    if end_rows.empty:\n        if sum(chrom_rows[\"end_position\"] &gt;= row[\"End_Position\"]) &gt; 0:\n            overlap = row[\"End_Position\"] - chrom_rows[\"start_position\"]\n            # difference =  difference * -1.0\n            max_overlap = _get_max_overlap_index(overlap, bed_length, boundary)\n            if max_overlap is not None:\n                end_rows = pd.concat([end_rows, chrom_rows.loc[[max_overlap]]])\n        # End goes over end boundary, but start is contained in position\n        if sum(chrom_rows[\"start_position\"] &lt;= row[\"Start_Position\"]) &gt; 0:\n            overlap = chrom_rows[\"end_position\"] - row[\"Start_Position\"]\n            max_overlap = _get_max_overlap_index(overlap, bed_length, boundary)\n            if max_overlap is not None:\n                end_rows = pd.concat([end_rows, chrom_rows.loc[[max_overlap]]])\n        # Start and end go over position boundary\n        check = chrom_rows[chrom_rows[\"start_position\"] &gt;= row[\"Start_Position\"]]\n        check = check[check[\"end_position\"] &lt;= row[\"End_Position\"]]\n        if not check.empty:\n            overlap = chrom_rows[\"end_position\"] - chrom_rows[\"start_position\"]\n            max_overlap = _get_max_overlap_index(overlap, bed_length, boundary)\n            if max_overlap is not None:\n                end_rows = pd.concat([end_rows, chrom_rows.loc[[max_overlap]]])\n    return end_rows\n</code></pre>"},{"location":"reference/fileformats/bed/#genie_registry.bed.remap_symbols","title":"<code>remap_symbols(row, gene_positiondf)</code>","text":"<p>Remap hugo symbols if there is no overlap between submitted bed region and gene positions.</p> PARAMETER DESCRIPTION <code>row</code> <p>start and end position</p> <p> </p> <code>gene_positiondf</code> <p>Actual gene position dataframe</p> <p> </p> Return <p>bool or Series: if the gene passed in need to be remapped or                 the remapped gene</p> Source code in <code>genie_registry/bed.py</code> <pre><code>def remap_symbols(row, gene_positiondf):\n    \"\"\"\n    Remap hugo symbols if there is no overlap between submitted bed region and\n    gene positions.\n\n    Args:\n        row: start and end position\n        gene_positiondf: Actual gene position dataframe\n\n    Return:\n        bool or Series: if the gene passed in need to be remapped or\n                        the remapped gene\n    \"\"\"\n    region_overlap = _check_region_overlap(row, gene_positiondf)\n    if not region_overlap:\n        overlap_positions = _map_position_within_boundary(row, gene_positiondf)\n        if overlap_positions.empty:\n            LOGGER.warning(\n                \"{} cannot be remapped. \"\n                \"These rows will have an empty gene symbol\".format(row[\"Hugo_Symbol\"])\n            )\n            row[\"Hugo_Symbol\"] = float(\"nan\")\n        elif len(overlap_positions) &gt; 1:\n            symbol_list = overlap_positions[\"hgnc_symbol\"].tolist()\n            if row[\"Hugo_Symbol\"] not in symbol_list:\n                # if \"MLL4\", then the HUGO symbol should be KMT2D and KMT2B\n                LOGGER.warning(\n                    \"{} can be mapped to different symbols: {}. \"\n                    \"Please correct or it will be removed.\".format(\n                        row[\"Hugo_Symbol\"], \", \".join(symbol_list)\n                    )\n                )\n                row[\"Hugo_Symbol\"] = float(\"nan\")\n        else:\n            symbol = overlap_positions[\"hgnc_symbol\"].values[0]\n            if row[\"Hugo_Symbol\"] != symbol:\n                LOGGER.info(\n                    \"{} will be remapped to {}\".format(row[\"Hugo_Symbol\"], symbol)\n                )\n                row[\"Hugo_Symbol\"] = symbol\n    return row\n</code></pre>"},{"location":"reference/fileformats/clinical/","title":"Clinical","text":""},{"location":"reference/fileformats/clinical/#genie_registry.clinical","title":"<code>genie_registry.clinical</code>","text":"<p>Clinical file format validation and processing</p>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/clinical/#genie_registry.clinical.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/clinical/#genie_registry.clinical-classes","title":"Classes","text":""},{"location":"reference/fileformats/clinical/#genie_registry.clinical.Clinical","title":"<code>Clinical</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/clinical.py</code> <pre><code>class Clinical(FileTypeFormat):\n    _fileType = \"clinical\"\n\n    # _process_kwargs = [\n    #     \"newPath\", \"patientSynId\", \"sampleSynId\",\n    #     \"parentId\", \"retractedSampleSynId\", \"retractedPatientSynId\"]\n    _process_kwargs = [\n        \"newPath\",\n        \"parentId\",\n        \"clinicalTemplate\",\n        \"sample\",\n        \"patient\",\n        \"patientCols\",\n        \"sampleCols\",\n    ]\n\n    # VALIDATE FILE NAME\n    def _validateFilename(self, filePath):\n        if len(filePath) == 1:\n            assert os.path.basename(filePath[0]) == \"data_clinical_supp_{}.txt\".format(\n                self.center\n            )\n        else:\n            required = pd.Series(\n                [\n                    \"data_clinical_supp_sample_{}.txt\".format(self.center),\n                    \"data_clinical_supp_patient_{}.txt\".format(self.center),\n                ]\n            )\n            assert all(required.isin([os.path.basename(i) for i in filePath]))\n\n    # Update clinical file with the correct mappings\n    def update_clinical(self, row):\n        \"\"\"Transform the values of each row of the clinical file\"\"\"\n        # Must create copy or else it will overwrite the original row\n        x = row.copy()\n\n        # BIRTH YEAR\n        if x.get(\"BIRTH_YEAR\") is not None:\n            # BIRTH YEAR (Check if integer)\n            if process_functions.checkInt(x[\"BIRTH_YEAR\"]):\n                x[\"BIRTH_YEAR\"] = int(x[\"BIRTH_YEAR\"])\n\n        # AGE AT SEQ REPORT\n        if x.get(\"AGE_AT_SEQ_REPORT\") is not None:\n            if process_functions.checkInt(x[\"AGE_AT_SEQ_REPORT\"]):\n                x[\"AGE_AT_SEQ_REPORT\"] = int(x[\"AGE_AT_SEQ_REPORT\"])\n\n        # SEQ ASSAY ID\n        if x.get(\"SEQ_ASSAY_ID\") is not None:\n            x[\"SEQ_ASSAY_ID\"] = x[\"SEQ_ASSAY_ID\"].replace(\"_\", \"-\")\n            # standardize all SEQ_ASSAY_ID with uppercase\n            x[\"SEQ_ASSAY_ID\"] = x[\"SEQ_ASSAY_ID\"].upper()\n\n        if x.get(\"SEQ_DATE\") is not None:\n            x[\"SEQ_DATE\"] = x[\"SEQ_DATE\"].title()\n            x[\"SEQ_YEAR\"] = (\n                int(str(x[\"SEQ_DATE\"]).split(\"-\")[1])\n                if str(x[\"SEQ_DATE\"]) != \"Release\"\n                else float(\"nan\")\n            )\n\n        if x.get(\"YEAR_CONTACT\") is not None:\n            if process_functions.checkInt(x[\"YEAR_CONTACT\"]):\n                x[\"YEAR_CONTACT\"] = int(x[\"YEAR_CONTACT\"])\n\n        if x.get(\"YEAR_DEATH\") is not None:\n            if process_functions.checkInt(x[\"YEAR_DEATH\"]):\n                x[\"YEAR_DEATH\"] = int(x[\"YEAR_DEATH\"])\n\n        # TRIM EVERY COLUMN MAKE ALL DASHES\n        for i in x.keys():\n            if isinstance(x[i], str):\n                x[i] = x[i].strip(\" \")\n        return x\n\n    def uploadMissingData(\n        self, df: pd.DataFrame, col: str, dbSynId: str, stagingSynId: str\n    ):\n        \"\"\"Uploads missing clinical samples / patients\n\n        Args:\n            df (pd.DataFrame): dataframe with clinical data\n            col (str): column in dataframe. Usually SAMPLE_ID or PATIENT_ID.\n            dbSynId (str): Synapse table Synapse id\n            stagingSynId (str): Center Synapse staging Id\n        \"\"\"\n        path = os.path.join(\n            process_functions.SCRIPT_DIR, f\"{self._fileType}_missing_{col}.csv\"\n        )\n        # PLFM-7428 - there are limits on a \"not in\" function on Synapse tables\n        center_samples = self.syn.tableQuery(\n            f\"select {col} from {dbSynId} where \" f\"CENTER='{self.center}'\"\n        )\n        center_samples_df = center_samples.asDataFrame()\n        # Get all the samples that are in the database but missing from\n        # the input file\n        missing_df = center_samples_df[col][~center_samples_df[col].isin(df[col])]\n        missing_df.to_csv(path, index=False)\n        self.syn.store(synapseclient.File(path, parent=stagingSynId))\n        os.remove(path)\n\n    def _process(self, clinical, clinicalTemplate):\n        # Capitalize all clinical dataframe columns\n        clinical.columns = [col.upper() for col in clinical.columns]\n        clinical = clinical.fillna(\"\")\n        # clinicalMerged = clinical.merge(clinicalTemplate,how='outer')\n        # Remove unwanted clinical columns prior to update\n        # clinicalMerged = clinicalMerged.drop(clinicalMerged.columns[\n        #    ~clinicalMerged.columns.isin(clinicalTemplate.columns)],1)\n        ethnicity_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['ethnicity_mapping']}\"\n        )\n        race_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['race_mapping']}\"\n        )\n        sex_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['sex_mapping']}\"\n        )\n        sampletype_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['sampletype_mapping']}\"\n        )\n        # Attach MSK to centers\n        # clinicalMerged = clinicalMerged.fillna(\"\")\n        clinical = remap_clinical_values(\n            clinical,\n            sex_mapping,\n            race_mapping,\n            ethnicity_mapping,\n            sampletype_mapping,\n        )\n        remapped_clindf = clinical.apply(self.update_clinical, axis=1)\n        # Some columns may have been added during update,\n        # remove unwanted columns again\n        keep_cols_idx = remapped_clindf.columns.isin(clinicalTemplate.columns)\n        remapped_clindf = remapped_clindf.drop(\n            columns=remapped_clindf.columns[~keep_cols_idx]\n        )\n        remapped_clindf[\"CENTER\"] = self.center\n        return remapped_clindf\n\n    def preprocess(self, newpath):\n        \"\"\"\n        Gather preprocess parameters\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            dict with keys - 'clinicalTemplate', 'sample', 'patient',\n                             'patientCols', 'sampleCols'\n        \"\"\"\n        # These synapse ids for the clinical tier release scope is\n        # hardcoded because it never changes\n        # TODO: Add clinical tier release scope to GENIE config\n        patient_cols_table = self.syn.tableQuery(\n            f\"select fieldName from {self.genie_config['clinical_tier_release_scope']} where \"\n            \"patient is True and inClinicalDb is True\"\n        )\n        patient_cols = patient_cols_table.asDataFrame()[\"fieldName\"].tolist()\n        sample_cols_table = self.syn.tableQuery(\n            f\"select fieldName from {self.genie_config['clinical_tier_release_scope']} where \"\n            \"sample is True and inClinicalDb is True\"\n        )\n        sample_cols = sample_cols_table.asDataFrame()[\"fieldName\"].tolist()\n        clinicalTemplate = pd.DataFrame(columns=list(set(patient_cols + sample_cols)))\n        sample = True\n        patient = True\n\n        return {\n            \"clinicalTemplate\": clinicalTemplate,\n            \"sample\": sample,\n            \"patient\": patient,\n            \"patientCols\": patient_cols,\n            \"sampleCols\": sample_cols,\n        }\n\n    def process_steps(\n        self,\n        clinicalDf,\n        newPath,\n        parentId,\n        clinicalTemplate,\n        sample,\n        patient,\n        patientCols,\n        sampleCols,\n    ):\n        \"\"\"Process clincial file, redact PHI values, upload to clinical\n        database\n        \"\"\"\n        patient_synid = self.genie_config[\"patient\"]\n        sample_synid = self.genie_config[\"sample\"]\n\n        newClinicalDf = self._process(clinicalDf, clinicalTemplate)\n        newClinicalDf = redact_phi(newClinicalDf)\n\n        if patient:\n            cols = newClinicalDf.columns[newClinicalDf.columns.isin(patientCols)]\n            patientClinical = newClinicalDf[cols].drop_duplicates(\"PATIENT_ID\")\n            self.uploadMissingData(\n                patientClinical, \"PATIENT_ID\", patient_synid, parentId\n            )\n            load.update_table(\n                syn=self.syn,\n                databaseSynId=patient_synid,\n                newData=patientClinical,\n                filterBy=self.center,\n                toDelete=True,\n                col=cols.tolist(),\n            )\n        if sample:\n            cols = newClinicalDf.columns[newClinicalDf.columns.isin(sampleCols)]\n            if sum(newClinicalDf[\"SAMPLE_ID\"].duplicated()) &gt; 0:\n                logger.error(\n                    \"There are duplicated samples, \" \"and the duplicates are removed\"\n                )\n            sampleClinical = newClinicalDf[cols].drop_duplicates(\"SAMPLE_ID\")\n            # Exclude all clinical samples with wrong oncotree codes\n            oncotree_mapping = pd.DataFrame()\n            oncotree_mapping_dict = process_functions.get_oncotree_code_mappings(\n                self.genie_config[\"oncotreeLink\"]\n            )\n            # Add in unknown key for oncotree code\n            oncotree_mapping_dict[\"UNKNOWN\"] = {}\n            oncotree_mapping[\"ONCOTREE_CODE\"] = list(oncotree_mapping_dict.keys())\n            # Make oncotree codes uppercase (SpCC/SPCC)\n            sampleClinical[\"ONCOTREE_CODE\"] = (\n                sampleClinical[\"ONCOTREE_CODE\"].astype(str).str.upper()\n            )\n            sampleClinical = sampleClinical[\n                sampleClinical[\"ONCOTREE_CODE\"].isin(oncotree_mapping[\"ONCOTREE_CODE\"])\n            ]\n            self.uploadMissingData(sampleClinical, \"SAMPLE_ID\", sample_synid, parentId)\n            load.update_table(\n                syn=self.syn,\n                databaseSynId=sample_synid,\n                newData=sampleClinical,\n                filterBy=self.center,\n                toDelete=True,\n                col=cols.tolist(),\n            )\n        newClinicalDf.to_csv(newPath, sep=\"\\t\", index=False)\n        return newPath\n\n    @staticmethod\n    def _validate_oncotree_code_mapping(\n        clinicaldf: pd.DataFrame, oncotree_mapping: pd.DataFrame\n    ) -&gt; pd.Index:\n        \"\"\"Checks that the oncotree codes in the input clinical\n        data is a valid oncotree code from the official oncotree site\n\n        Args:\n            clinicaldf (pd.DataFrame): clinical input data to validate\n            oncotree_mapping (pd.DataFrame): table of official oncotree\n                mappings\n\n        Returns:\n            pd.Index: row indices of unmapped oncotree codes in the\n            input clinical data\n        \"\"\"\n        # Make oncotree codes uppercase (SpCC/SPCC)\n        clinicaldf[\"ONCOTREE_CODE\"] = (\n            clinicaldf[\"ONCOTREE_CODE\"].astype(str).str.upper()\n        )\n\n        unmapped_oncotrees = clinicaldf[\n            (clinicaldf[\"ONCOTREE_CODE\"] != \"UNKNOWN\")\n            &amp; ~(clinicaldf[\"ONCOTREE_CODE\"].isin(oncotree_mapping[\"ONCOTREE_CODE\"]))\n        ]\n        return unmapped_oncotrees.index\n\n    @staticmethod\n    def _validate_oncotree_code_mapping_message(\n        clinicaldf: pd.DataFrame,\n        unmapped_oncotree_indices: pd.DataFrame,\n    ) -&gt; Tuple[str, str]:\n        \"\"\"This function returns the error and warning messages\n        if the input clinical data has row indices with unmapped\n        oncotree codes\n\n        Args:\n            clinicaldf (pd.DataFrame): input clinical data\n            unmapped_oncotree_indices (pd.DataFrame): row indices of the\n                input clinical data with unmapped oncotree codes\n\n        Returns:\n            Tuple[str, str]: error message that tells you how many\n                samples AND the unique unmapped oncotree codes that your\n                input clinical data has\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        if len(unmapped_oncotree_indices) &gt; 0:\n            # sort the unique unmapped oncotree codes\n            unmapped_oncotree_codes = sorted(\n                set(clinicaldf.loc[unmapped_oncotree_indices][\"ONCOTREE_CODE\"])\n            )\n            errors = (\n                \"Sample Clinical File: Please double check that all your \"\n                \"ONCOTREE CODES exist in the mapping. You have {} samples \"\n                \"that don't map. These are the codes that \"\n                \"don't map: {}\\n\".format(\n                    len(unmapped_oncotree_indices), \",\".join(unmapped_oncotree_codes)\n                )\n            )\n        return errors, warnings\n\n    # VALIDATION\n    def _validate(self, clinicaldf):\n        \"\"\"\n        This function validates the clinical file to make sure it adhere\n        to the clinical SOP.\n\n        Args:\n            clinicalDF: Merged clinical file with patient and sample\n                        information\n\n        Returns:\n            Error message\n        \"\"\"\n        total_error = StringIO()\n        warning = StringIO()\n\n        clinicaldf.columns = [col.upper() for col in clinicaldf.columns]\n        # CHECK: for empty rows\n        empty_rows = clinicaldf.isnull().values.all(axis=1)\n        if empty_rows.any():\n            total_error.write(\"Clinical file(s): No empty rows allowed.\\n\")\n            # Remove completely empty rows to speed up processing\n            clinicaldf = clinicaldf[~empty_rows]\n\n        clinicaldf = clinicaldf.fillna(\"\")\n\n        oncotree_mapping_dict = process_functions.get_oncotree_code_mappings(\n            self.genie_config[\"oncotreeLink\"]\n        )\n        oncotree_mapping = pd.DataFrame(\n            {\"ONCOTREE_CODE\": list(oncotree_mapping_dict.keys())}\n        )\n\n        ethnicity_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['ethnicity_mapping']}\"\n        )\n        race_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['race_mapping']}\"\n        )\n        sex_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['sex_mapping']}\"\n        )\n        sampletype_mapping = extract.get_syntabledf(\n            self.syn, f\"select * from {self.genie_config['sampletype_mapping']}\"\n        )\n        # CHECK: SAMPLE_ID\n        sample_id = \"SAMPLE_ID\"\n        haveSampleColumn = process_functions.checkColExist(clinicaldf, sample_id)\n\n        if not haveSampleColumn:\n            total_error.write(\"Sample Clinical File: Must have SAMPLE_ID column.\\n\")\n        else:\n            if sum(clinicaldf[sample_id].duplicated()) &gt; 0:\n                total_error.write(\n                    \"Sample Clinical File: No duplicated SAMPLE_ID \"\n                    \"allowed.\\nIf there are no duplicated \"\n                    \"SAMPLE_IDs, and both sample and patient files are \"\n                    \"uploaded, then please check to make sure no duplicated \"\n                    \"PATIENT_IDs exist in the patient clinical file.\\n\"\n                )\n            error = process_functions.validate_genie_identifier(\n                identifiers=clinicaldf[sample_id],\n                center=self.center,\n                filename=\"Sample Clinical File\",\n                col=\"SAMPLE_ID\",\n            )\n            total_error.write(error)\n\n        # CHECK: PATIENT_ID\n        patientId = \"PATIENT_ID\"\n        # #CHECK: PATIENT_ID IN SAMPLE FILE\n        havePatientColumn = process_functions.checkColExist(clinicaldf, patientId)\n\n        if not havePatientColumn:\n            total_error.write(\"Patient Clinical File: Must have PATIENT_ID column.\\n\")\n        else:\n            if not all(clinicaldf[patientId].str.startswith(f\"GENIE-{self.center}\")):\n                total_error.write(\n                    \"Patient Clinical File: \"\n                    f\"PATIENT_ID must start with GENIE-{self.center}\\n\"\n                )\n            if any(clinicaldf[patientId].str.len() &gt;= 50):\n                total_error.write(\n                    \"Patient Clinical File: PATIENT_ID must have less than \"\n                    \"50 characters.\\n\"\n                )\n        # CHECK: within the sample file that the sample ids match\n        # the patient ids\n        if haveSampleColumn and havePatientColumn:\n            # Make sure sample and patient ids are string cols\n            clinicaldf[sample_id] = clinicaldf[sample_id].astype(str)\n            clinicaldf[patientId] = clinicaldf[patientId].astype(str)\n            if not all(\n                [\n                    patient in sample\n                    for sample, patient in zip(\n                        clinicaldf[sample_id], clinicaldf[patientId]\n                    )\n                ]\n            ):\n                total_error.write(\n                    \"Sample Clinical File: PATIENT_ID's much be contained in \"\n                    \"the SAMPLE_ID's (ex. SAGE-1 &lt;-&gt; SAGE-1-2)\\n\"\n                )\n            # #CHECK: All samples must have associated patient data\n            # (GENIE requires patient data)\n            if not all(clinicaldf[patientId] != \"\"):\n                total_error.write(\n                    \"Patient Clinical File: All samples must have associated \"\n                    \"patient information and no null patient ids allowed. \"\n                    \"These samples are missing patient data: {}\\n\".format(\n                        \", \".join(\n                            clinicaldf[sample_id][clinicaldf[patientId] == \"\"].unique()\n                        )\n                    )\n                )\n\n            # CHECK: All patients should have associated sample data\n            if not all(clinicaldf[sample_id] != \"\"):\n                # ## MAKE WARNING FOR NOW###\n                warning.write(\n                    \"Sample Clinical File: All patients must have associated \"\n                    \"sample information. These patients are missing sample \"\n                    \"data: {}\\n\".format(\n                        \", \".join(\n                            clinicaldf[patientId][clinicaldf[sample_id] == \"\"].unique()\n                        )\n                    )\n                )\n\n        # CHECK: AGE_AT_SEQ_REPORT\n        age = \"AGE_AT_SEQ_REPORT\"\n        haveColumn = process_functions.checkColExist(clinicaldf, age)\n        if haveColumn:\n            # Deal with HIPAA converted rows from DFCI\n            # First for loop can't int(text) because there\n            # are instances that have &lt;3435\n            age_seq_report_df = clinicaldf[\n                ~clinicaldf[age].isin([\"Unknown\", \"&gt;32485\", \"&lt;6570\"])\n            ]\n\n            # age_seq_report_df[age] = \\\n            #     remove_greaterthan_lessthan_str(age_seq_report_df[age])\n\n            if not all([process_functions.checkInt(i) for i in age_seq_report_df[age]]):\n                total_error.write(\n                    \"Sample Clinical File: Please double check your \"\n                    \"AGE_AT_SEQ_REPORT. It must be an integer, 'Unknown', \"\n                    \"'&gt;32485', '&lt;6570'.\\n\"\n                )\n            else:\n                age_seq_report_df[age] = age_seq_report_df[age].astype(int)\n                median_age = age_seq_report_df[age].median()\n                if median_age &lt; 100:\n                    total_error.write(\n                        \"Sample Clinical File: Please double check your \"\n                        \"AGE_AT_SEQ_REPORT. You may be reporting this value \"\n                        \"in YEARS, please report in DAYS.\\n\"\n                    )\n        else:\n            total_error.write(\n                \"Sample Clinical File: Must have AGE_AT_SEQ_REPORT column.\\n\"\n            )\n\n        # CHECK: ONCOTREE_CODE\n        haveColumn = process_functions.checkColExist(clinicaldf, \"ONCOTREE_CODE\")\n        maleOncoCodes = [\"TESTIS\", \"PROSTATE\", \"PENIS\"]\n        womenOncoCodes = [\"CERVIX\", \"VULVA\", \"UTERUS\", \"OVARY\"]\n        if haveColumn:\n            unmapped_indices = self._validate_oncotree_code_mapping(\n                clinicaldf, oncotree_mapping\n            )\n            errors, warnings = self._validate_oncotree_code_mapping_message(\n                clinicaldf, unmapped_indices\n            )\n            total_error.write(errors)\n            # Should add the SEX mismatch into the dashboard file\n            if (\n                process_functions.checkColExist(clinicaldf, \"SEX\")\n                and \"oncotree_mapping_dict\" in locals()\n                and havePatientColumn\n                and haveSampleColumn\n            ):\n                wrongCodeSamples = []\n                # This is to check if oncotree codes match the sex,\n                # returns list of samples that have conflicting codes and sex\n                for code, patient, sample in zip(\n                    clinicaldf[\"ONCOTREE_CODE\"],\n                    clinicaldf[\"PATIENT_ID\"],\n                    clinicaldf[\"SAMPLE_ID\"],\n                ):\n                    if (\n                        oncotree_mapping_dict.get(code) is not None\n                        and sum(clinicaldf[\"PATIENT_ID\"] == patient) &gt; 0\n                    ):\n                        primaryCode = oncotree_mapping_dict[code][\n                            \"ONCOTREE_PRIMARY_NODE\"\n                        ]\n\n                        sex = clinicaldf[\"SEX\"][\n                            clinicaldf[\"PATIENT_ID\"] == patient\n                        ].values[0]\n                        sex = float(\"nan\") if sex == \"\" else float(sex)\n                        if (\n                            oncotree_mapping_dict[code][\"ONCOTREE_PRIMARY_NODE\"]\n                            in maleOncoCodes\n                            and sex != 1.0\n                        ):\n                            wrongCodeSamples.append(sample)\n                        if (\n                            oncotree_mapping_dict[code][\"ONCOTREE_PRIMARY_NODE\"]\n                            in womenOncoCodes\n                            and sex != 2.0\n                        ):\n                            wrongCodeSamples.append(sample)\n                if len(wrongCodeSamples) &gt; 0:\n                    warning.write(\n                        \"Sample Clinical File: Some SAMPLE_IDs have \"\n                        \"conflicting SEX and ONCOTREE_CODES: {}\\n\".format(\n                            \",\".join(wrongCodeSamples)\n                        )\n                    )\n        else:\n            total_error.write(\"Sample Clinical File: Must have ONCOTREE_CODE column.\\n\")\n\n        warn, error = process_functions.check_col_and_values(\n            clinicaldf,\n            \"SAMPLE_TYPE\",\n            sampletype_mapping[\"CODE\"].tolist(),\n            \"Sample Clinical File\",\n            required=True,\n        )\n        total_error.write(error)\n\n        # CHECK: SEQ_ASSAY_ID\n        haveColumn = process_functions.checkColExist(clinicaldf, \"SEQ_ASSAY_ID\")\n        if haveColumn:\n            if not all([i != \"\" for i in clinicaldf[\"SEQ_ASSAY_ID\"]]):\n                total_error.write(\n                    \"Sample Clinical File: Please double check your \"\n                    \"SEQ_ASSAY_ID columns, there are empty rows.\\n\"\n                )\n            # must remove empty seq assay ids first\n            # Checking if seq assay ids start with the center name\n            empty_seq_idx = clinicaldf.SEQ_ASSAY_ID != \"\"\n            seqassay_ids = clinicaldf.SEQ_ASSAY_ID[empty_seq_idx]\n            uniq_seqassay_ids = seqassay_ids.unique()\n            invalid_seqassay = []\n            for seqassay in uniq_seqassay_ids:\n                # SEQ Ids are all capitalized now, so no need to check\n                # for differences in case\n                if not seqassay.upper().startswith(self.center):\n                    invalid_seqassay.append(seqassay)\n            if invalid_seqassay:\n                total_error.write(\n                    \"Sample Clinical File: Please make sure your \"\n                    \"SEQ_ASSAY_IDs start with your center \"\n                    \"abbreviation: {}.\\n\".format(\", \".join(invalid_seqassay))\n                )\n        else:\n            total_error.write(\"Sample Clinical File: Must have SEQ_ASSAY_ID column.\\n\")\n\n        haveColumn = process_functions.checkColExist(clinicaldf, \"SEQ_DATE\")\n        seq_date_error = (\n            \"Sample Clinical File: SEQ_DATE must be one of five values- \"\n            \"For Jan-March: use Jan-YEAR. \"\n            \"For Apr-June: use Apr-YEAR. \"\n            \"For July-Sep: use Jul-YEAR. \"\n            \"For Oct-Dec: use Oct-YEAR. (ie. Apr-2017) \"\n            \"For values that don't have SEQ_DATES that \"\n            \"you want released use 'release'.\\n\"\n        )\n\n        if haveColumn:\n            clinicaldf[\"SEQ_DATE\"] = [\n                i.title() for i in clinicaldf[\"SEQ_DATE\"].astype(str)\n            ]\n\n            seqdate = clinicaldf[\"SEQ_DATE\"][clinicaldf[\"SEQ_DATE\"] != \"Release\"]\n            if sum(clinicaldf[\"SEQ_DATE\"] == \"\") &gt; 0:\n                total_error.write(\n                    \"Sample Clinical File: Samples without SEQ_DATEs will \"\n                    \"NOT be released.\\n\"\n                )\n            try:\n                if not seqdate.empty:\n                    seqdate.apply(\n                        lambda date: datetime.datetime.strptime(date, \"%b-%Y\")\n                    )\n                    if not seqdate.str.startswith((\"Jan\", \"Apr\", \"Jul\", \"Oct\")).all():\n                        total_error.write(seq_date_error)\n            except ValueError:\n                total_error.write(seq_date_error)\n        else:\n            total_error.write(\"Sample Clinical File: Must have SEQ_DATE column.\\n\")\n\n        # CHECK: BIRTH_YEAR\n        error = _check_year(\n            clinicaldf=clinicaldf,\n            year_col=\"BIRTH_YEAR\",\n            filename=\"Patient Clinical File\",\n            allowed_string_values=[\"Unknown\", \"&gt;89\", \"&lt;18\"],\n        )\n        total_error.write(error)\n\n        # CHECK: YEAR DEATH\n        error = _check_year(\n            clinicaldf=clinicaldf,\n            year_col=\"YEAR_DEATH\",\n            filename=\"Patient Clinical File\",\n            allowed_string_values=[\n                \"Unknown\",\n                \"Not Collected\",\n                \"Not Applicable\",\n                \"Not Released\",\n                \"&gt;89\",\n                \"&lt;18\",\n            ],\n        )\n        total_error.write(error)\n\n        # CHECK: YEAR CONTACT\n        error = _check_year(\n            clinicaldf=clinicaldf,\n            year_col=\"YEAR_CONTACT\",\n            filename=\"Patient Clinical File\",\n            allowed_string_values=[\n                \"Unknown\",\n                \"Not Collected\",\n                \"Not Released\",\n                \"&gt;89\",\n                \"&lt;18\",\n            ],\n        )\n        total_error.write(error)\n\n        # CHECK: YEAR DEATH against YEAR CONTACT\n        has_death_and_contact_years = process_functions.checkColExist(\n            clinicaldf, [\"YEAR_DEATH\", \"YEAR_CONTACT\"]\n        )\n        if has_death_and_contact_years:\n            invalid_year_death_indices = _check_year_death_validity(clinicaldf)\n            errors, warnings = _check_year_death_validity_message(\n                invalid_year_death_indices\n            )\n            total_error.write(errors)\n\n        # CHECK: INT CONTACT\n        haveColumn = process_functions.checkColExist(clinicaldf, \"INT_CONTACT\")\n        if haveColumn:\n            if not all(\n                [\n                    process_functions.checkInt(i)\n                    for i in clinicaldf.INT_CONTACT\n                    if i\n                    not in [\n                        \"&gt;32485\",\n                        \"&lt;6570\",\n                        \"Unknown\",\n                        \"Not Collected\",\n                        \"Not Released\",\n                    ]\n                ]\n            ):\n                total_error.write(\n                    \"Patient Clinical File: Please double check your \"\n                    \"INT_CONTACT column, it must be an integer, '&gt;32485', \"\n                    \"'&lt;6570', 'Unknown', 'Not Released' or 'Not Collected'.\\n\"\n                )\n        else:\n            total_error.write(\"Patient Clinical File: Must have INT_CONTACT column.\\n\")\n\n        # INT DOD\n        haveColumn = process_functions.checkColExist(clinicaldf, \"INT_DOD\")\n        if haveColumn:\n            if not all(\n                [\n                    process_functions.checkInt(i)\n                    for i in clinicaldf.INT_DOD\n                    if i\n                    not in [\n                        \"&gt;32485\",\n                        \"&lt;6570\",\n                        \"Unknown\",\n                        \"Not Collected\",\n                        \"Not Applicable\",\n                        \"Not Released\",\n                    ]\n                ]\n            ):\n                total_error.write(\n                    \"Patient Clinical File: Please double check your INT_DOD \"\n                    \"column, it must be an integer, '&gt;32485', '&lt;6570', \"\n                    \"'Unknown', 'Not Collected', 'Not Released' or \"\n                    \"'Not Applicable'.\\n\"\n                )\n        else:\n            total_error.write(\"Patient Clinical File: Must have INT_DOD column.\\n\")\n\n        haveColumn = process_functions.checkColExist(clinicaldf, \"DEAD\")\n        if haveColumn:\n            # Need to have check_bool function\n            if not all(\n                [\n                    str(i).upper() in [\"TRUE\", \"FALSE\"]\n                    for i in clinicaldf.DEAD\n                    if i not in [\"Unknown\", \"Not Collected\", \"Not Released\"]\n                ]\n            ):\n                total_error.write(\n                    \"Patient Clinical File: Please double check your \"\n                    \"DEAD column, it must be True, False, 'Unknown', \"\n                    \"'Not Released' or 'Not Collected'.\\n\"\n                )\n        else:\n            total_error.write(\"Patient Clinical File: Must have DEAD column.\\n\")\n\n        # CHECK: INT DOD against INT CONTACT\n        has_int_dod_and_contact = process_functions.checkColExist(\n            clinicaldf, [\"INT_DOD\", \"INT_CONTACT\"]\n        )\n        if has_int_dod_and_contact:\n            invalid_int_dod_indices = _check_int_dod_validity(clinicaldf)\n            errors, warnings = _check_int_dod_validity_message(invalid_int_dod_indices)\n            total_error.write(errors)\n\n        # CHECK: contact vital status value consistency\n        contact_error = _check_int_year_consistency(\n            clinicaldf=clinicaldf,\n            cols=[\"YEAR_CONTACT\", \"INT_CONTACT\"],\n            string_vals=[\"Not Collected\", \"Unknown\", \"Not Released\"],\n        )\n        total_error.write(contact_error)\n\n        # CHECK: death vital status value consistency\n        death_error = _check_int_year_consistency(\n            clinicaldf=clinicaldf,\n            cols=[\"YEAR_DEATH\", \"INT_DOD\"],\n            string_vals=[\n                \"Not Collected\",\n                \"Unknown\",\n                \"Not Applicable\",\n                \"Not Released\",\n            ],\n        )\n        total_error.write(death_error)\n        death_error = _check_int_dead_consistency(clinicaldf=clinicaldf)\n        total_error.write(death_error)\n\n        # CHECK: SAMPLE_CLASS is optional attribute\n        have_column = process_functions.checkColExist(clinicaldf, \"SAMPLE_CLASS\")\n        if have_column:\n            sample_class_vals = pd.Series(clinicaldf[\"SAMPLE_CLASS\"].unique().tolist())\n            if not sample_class_vals.isin([\"Tumor\", \"cfDNA\"]).all():\n                total_error.write(\n                    \"Sample Clinical File: SAMPLE_CLASS column must \"\n                    \"be 'Tumor', or 'cfDNA'\\n\"\n                )\n\n        # CHECK: PRIMARY_RACE\n        warn, error = process_functions.check_col_and_values(\n            clinicaldf,\n            \"PRIMARY_RACE\",\n            race_mapping[\"CODE\"].tolist(),\n            \"Patient Clinical File\",\n        )\n        warning.write(warn)\n        total_error.write(error)\n\n        # CHECK: SECONDARY_RACE\n        warn, error = process_functions.check_col_and_values(\n            clinicaldf,\n            \"SECONDARY_RACE\",\n            race_mapping[\"CODE\"].tolist(),\n            \"Patient Clinical File\",\n        )\n        warning.write(warn)\n        total_error.write(error)\n\n        # CHECK: TERTIARY_RACE\n        warn, error = process_functions.check_col_and_values(\n            clinicaldf,\n            \"TERTIARY_RACE\",\n            race_mapping[\"CODE\"].tolist(),\n            \"Patient Clinical File\",\n        )\n        warning.write(warn)\n        total_error.write(error)\n\n        # CHECK: SEX\n        warn, error = process_functions.check_col_and_values(\n            clinicaldf,\n            \"SEX\",\n            sex_mapping[\"CODE\"].tolist(),\n            \"Patient Clinical File\",\n            required=True,\n        )\n        warning.write(warn)\n        total_error.write(error)\n\n        # CHECK: ETHNICITY\n        warn, error = process_functions.check_col_and_values(\n            clinicaldf,\n            \"ETHNICITY\",\n            ethnicity_mapping[\"CODE\"].tolist(),\n            \"Patient Clinical File\",\n        )\n        warning.write(warn)\n        total_error.write(error)\n\n        return total_error.getvalue(), warning.getvalue()\n\n    def _get_dataframe(self, filePathList):\n        clinicaldf = pd.read_csv(filePathList[0], sep=\"\\t\", comment=\"#\")\n        clinicaldf.columns = [col.upper() for col in clinicaldf.columns]\n\n        if len(filePathList) &gt; 1:\n            other_clinicaldf = pd.read_csv(filePathList[1], sep=\"\\t\", comment=\"#\")\n            other_clinicaldf.columns = [col.upper() for col in other_clinicaldf.columns]\n\n            try:\n                clinicaldf = clinicaldf.merge(other_clinicaldf, on=\"PATIENT_ID\")\n            except Exception:\n                raise ValueError(\n                    (\n                        \"If submitting separate patient and sample files, \"\n                        \"they both must have the PATIENT_ID column\"\n                    )\n                )\n            # Must figure out which is sample and which is patient\n            if \"sample\" in filePathList[0]:\n                sample = clinicaldf\n                patient = other_clinicaldf\n            else:\n                sample = other_clinicaldf\n                patient = clinicaldf\n\n            if not all(sample[\"PATIENT_ID\"].isin(patient[\"PATIENT_ID\"])):\n                raise ValueError(\n                    (\n                        \"Patient Clinical File: All samples must have associated \"\n                        \"patient information\"\n                    )\n                )\n\n        return clinicaldf\n\n    def _cross_validate_bed_files_exist(self, clinicaldf) -&gt; list:\n        \"\"\"Check that a bed file exist per SEQ_ASSAY_ID value in clinical file\"\"\"\n        missing_files = []\n        exception_params = {\"ignore_case\": True, \"allow_underscore\": True}\n\n        # standardize and get unique seq assay ids before searching bed files\n        seq_assay_ids = set(\n            [\n                validate.standardize_string_for_validation(sq_id, **exception_params)\n                for sq_id in clinicaldf[\"SEQ_ASSAY_ID\"].unique()\n            ]\n        )\n\n        for seq_assay_id in seq_assay_ids:\n            bed_files = validate.parse_file_info_in_nested_list(\n                nested_list=self.ancillary_files,\n                search_str=f\"{seq_assay_id}.bed\",  # type: ignore[arg-type]\n                **exception_params,\n            )\n            if not bed_files[\"files\"]:\n                missing_files.append(f\"{seq_assay_id.upper()}.bed\")\n        return missing_files\n\n    def _cross_validate_bed_files_exist_message(self, missing_bed_files: list) -&gt; tuple:\n        \"\"\"Gets the warning/error messages given the missing bed files list\n\n        Args:\n            missing_bed_files (list): list of missing bed files\n\n        Returns:\n            tuple: error + warning\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        if missing_bed_files:\n            errors = (\n                \"At least one SEQ_ASSAY_ID in your clinical file does not have an associated BED file. \"\n                \"Please update your file(s) to be consistent.\\n\"\n                f\"Missing BED files: {', '.join(missing_bed_files)}\\n\"\n            )\n        return errors, warnings\n\n    def _cross_validate_assay_info_has_seq(self, clinicaldf: pd.DataFrame) -&gt; tuple:\n        \"\"\"Cross validates that assay information file has all the\n        SEQ_ASSAY_IDs present in the clinical file\n        TODO: Refactor this function (similar to _cross_validate in maf)\n        once the clinical files have been taken care of\n        so that it can be generalized to any file type\n\n        Args:\n            clinicaldf (pd.DataFrame): input clinical data\n\n        Returns:\n            tuple: errors and warnings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        col_to_validate = \"SEQ_ASSAY_ID\"\n        # This section can be removed once we remove the list of lists\n        assay_files = validate.parse_file_info_in_nested_list(\n            nested_list=self.ancillary_files, search_str=\"assay_information\"  # type: ignore[arg-type]\n        )\n        assay_file_paths = assay_files[\"file_info\"][\"path\"]\n\n        if assay_files[\"files\"]:\n            try:\n                assay_df = process_functions.get_assay_dataframe(\n                    filepath_list=assay_file_paths\n                )\n                has_file_read_error = False\n            except Exception:\n                has_file_read_error = True\n\n            if not has_file_read_error:\n                if process_functions.checkColExist(assay_df, col_to_validate):\n                    errors, warnings = validate.check_values_between_two_df(\n                        df1=clinicaldf,\n                        df1_filename=\"clinical file\",\n                        df1_id_to_check=col_to_validate,\n                        df2=assay_df,\n                        df2_filename=\"assay information\",\n                        df2_id_to_check=col_to_validate,\n                        ignore_case=True,\n                        allow_underscore=True,\n                    )\n        return errors, warnings\n\n    def _cross_validate(self, clinicaldf) -&gt; tuple:\n        \"\"\"Cross-validation for clinical file(s)\"\"\"\n        errors_assay, warnings_assay = self._cross_validate_assay_info_has_seq(\n            clinicaldf\n        )\n        missing_bed_files = self._cross_validate_bed_files_exist(clinicaldf)\n        errors_bed, warnings_bed = self._cross_validate_bed_files_exist_message(\n            missing_bed_files\n        )\n\n        errors = errors_assay + errors_bed\n        warnings = warnings_assay + warnings_bed\n        return errors, warnings\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical.Clinical-functions","title":"Functions","text":""},{"location":"reference/fileformats/clinical/#genie_registry.clinical.Clinical.update_clinical","title":"<code>update_clinical(row)</code>","text":"<p>Transform the values of each row of the clinical file</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def update_clinical(self, row):\n    \"\"\"Transform the values of each row of the clinical file\"\"\"\n    # Must create copy or else it will overwrite the original row\n    x = row.copy()\n\n    # BIRTH YEAR\n    if x.get(\"BIRTH_YEAR\") is not None:\n        # BIRTH YEAR (Check if integer)\n        if process_functions.checkInt(x[\"BIRTH_YEAR\"]):\n            x[\"BIRTH_YEAR\"] = int(x[\"BIRTH_YEAR\"])\n\n    # AGE AT SEQ REPORT\n    if x.get(\"AGE_AT_SEQ_REPORT\") is not None:\n        if process_functions.checkInt(x[\"AGE_AT_SEQ_REPORT\"]):\n            x[\"AGE_AT_SEQ_REPORT\"] = int(x[\"AGE_AT_SEQ_REPORT\"])\n\n    # SEQ ASSAY ID\n    if x.get(\"SEQ_ASSAY_ID\") is not None:\n        x[\"SEQ_ASSAY_ID\"] = x[\"SEQ_ASSAY_ID\"].replace(\"_\", \"-\")\n        # standardize all SEQ_ASSAY_ID with uppercase\n        x[\"SEQ_ASSAY_ID\"] = x[\"SEQ_ASSAY_ID\"].upper()\n\n    if x.get(\"SEQ_DATE\") is not None:\n        x[\"SEQ_DATE\"] = x[\"SEQ_DATE\"].title()\n        x[\"SEQ_YEAR\"] = (\n            int(str(x[\"SEQ_DATE\"]).split(\"-\")[1])\n            if str(x[\"SEQ_DATE\"]) != \"Release\"\n            else float(\"nan\")\n        )\n\n    if x.get(\"YEAR_CONTACT\") is not None:\n        if process_functions.checkInt(x[\"YEAR_CONTACT\"]):\n            x[\"YEAR_CONTACT\"] = int(x[\"YEAR_CONTACT\"])\n\n    if x.get(\"YEAR_DEATH\") is not None:\n        if process_functions.checkInt(x[\"YEAR_DEATH\"]):\n            x[\"YEAR_DEATH\"] = int(x[\"YEAR_DEATH\"])\n\n    # TRIM EVERY COLUMN MAKE ALL DASHES\n    for i in x.keys():\n        if isinstance(x[i], str):\n            x[i] = x[i].strip(\" \")\n    return x\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical.Clinical.uploadMissingData","title":"<code>uploadMissingData(df, col, dbSynId, stagingSynId)</code>","text":"<p>Uploads missing clinical samples / patients</p> PARAMETER DESCRIPTION <code>df</code> <p>dataframe with clinical data</p> <p> TYPE: <code>DataFrame</code> </p> <code>col</code> <p>column in dataframe. Usually SAMPLE_ID or PATIENT_ID.</p> <p> TYPE: <code>str</code> </p> <code>dbSynId</code> <p>Synapse table Synapse id</p> <p> TYPE: <code>str</code> </p> <code>stagingSynId</code> <p>Center Synapse staging Id</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def uploadMissingData(\n    self, df: pd.DataFrame, col: str, dbSynId: str, stagingSynId: str\n):\n    \"\"\"Uploads missing clinical samples / patients\n\n    Args:\n        df (pd.DataFrame): dataframe with clinical data\n        col (str): column in dataframe. Usually SAMPLE_ID or PATIENT_ID.\n        dbSynId (str): Synapse table Synapse id\n        stagingSynId (str): Center Synapse staging Id\n    \"\"\"\n    path = os.path.join(\n        process_functions.SCRIPT_DIR, f\"{self._fileType}_missing_{col}.csv\"\n    )\n    # PLFM-7428 - there are limits on a \"not in\" function on Synapse tables\n    center_samples = self.syn.tableQuery(\n        f\"select {col} from {dbSynId} where \" f\"CENTER='{self.center}'\"\n    )\n    center_samples_df = center_samples.asDataFrame()\n    # Get all the samples that are in the database but missing from\n    # the input file\n    missing_df = center_samples_df[col][~center_samples_df[col].isin(df[col])]\n    missing_df.to_csv(path, index=False)\n    self.syn.store(synapseclient.File(path, parent=stagingSynId))\n    os.remove(path)\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical.Clinical.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>Gather preprocess parameters</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <p>dict with keys - 'clinicalTemplate', 'sample', 'patient',              'patientCols', 'sampleCols'</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    Gather preprocess parameters\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        dict with keys - 'clinicalTemplate', 'sample', 'patient',\n                         'patientCols', 'sampleCols'\n    \"\"\"\n    # These synapse ids for the clinical tier release scope is\n    # hardcoded because it never changes\n    # TODO: Add clinical tier release scope to GENIE config\n    patient_cols_table = self.syn.tableQuery(\n        f\"select fieldName from {self.genie_config['clinical_tier_release_scope']} where \"\n        \"patient is True and inClinicalDb is True\"\n    )\n    patient_cols = patient_cols_table.asDataFrame()[\"fieldName\"].tolist()\n    sample_cols_table = self.syn.tableQuery(\n        f\"select fieldName from {self.genie_config['clinical_tier_release_scope']} where \"\n        \"sample is True and inClinicalDb is True\"\n    )\n    sample_cols = sample_cols_table.asDataFrame()[\"fieldName\"].tolist()\n    clinicalTemplate = pd.DataFrame(columns=list(set(patient_cols + sample_cols)))\n    sample = True\n    patient = True\n\n    return {\n        \"clinicalTemplate\": clinicalTemplate,\n        \"sample\": sample,\n        \"patient\": patient,\n        \"patientCols\": patient_cols,\n        \"sampleCols\": sample_cols,\n    }\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical.Clinical.process_steps","title":"<code>process_steps(clinicalDf, newPath, parentId, clinicalTemplate, sample, patient, patientCols, sampleCols)</code>","text":"<p>Process clincial file, redact PHI values, upload to clinical database</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def process_steps(\n    self,\n    clinicalDf,\n    newPath,\n    parentId,\n    clinicalTemplate,\n    sample,\n    patient,\n    patientCols,\n    sampleCols,\n):\n    \"\"\"Process clincial file, redact PHI values, upload to clinical\n    database\n    \"\"\"\n    patient_synid = self.genie_config[\"patient\"]\n    sample_synid = self.genie_config[\"sample\"]\n\n    newClinicalDf = self._process(clinicalDf, clinicalTemplate)\n    newClinicalDf = redact_phi(newClinicalDf)\n\n    if patient:\n        cols = newClinicalDf.columns[newClinicalDf.columns.isin(patientCols)]\n        patientClinical = newClinicalDf[cols].drop_duplicates(\"PATIENT_ID\")\n        self.uploadMissingData(\n            patientClinical, \"PATIENT_ID\", patient_synid, parentId\n        )\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=patient_synid,\n            newData=patientClinical,\n            filterBy=self.center,\n            toDelete=True,\n            col=cols.tolist(),\n        )\n    if sample:\n        cols = newClinicalDf.columns[newClinicalDf.columns.isin(sampleCols)]\n        if sum(newClinicalDf[\"SAMPLE_ID\"].duplicated()) &gt; 0:\n            logger.error(\n                \"There are duplicated samples, \" \"and the duplicates are removed\"\n            )\n        sampleClinical = newClinicalDf[cols].drop_duplicates(\"SAMPLE_ID\")\n        # Exclude all clinical samples with wrong oncotree codes\n        oncotree_mapping = pd.DataFrame()\n        oncotree_mapping_dict = process_functions.get_oncotree_code_mappings(\n            self.genie_config[\"oncotreeLink\"]\n        )\n        # Add in unknown key for oncotree code\n        oncotree_mapping_dict[\"UNKNOWN\"] = {}\n        oncotree_mapping[\"ONCOTREE_CODE\"] = list(oncotree_mapping_dict.keys())\n        # Make oncotree codes uppercase (SpCC/SPCC)\n        sampleClinical[\"ONCOTREE_CODE\"] = (\n            sampleClinical[\"ONCOTREE_CODE\"].astype(str).str.upper()\n        )\n        sampleClinical = sampleClinical[\n            sampleClinical[\"ONCOTREE_CODE\"].isin(oncotree_mapping[\"ONCOTREE_CODE\"])\n        ]\n        self.uploadMissingData(sampleClinical, \"SAMPLE_ID\", sample_synid, parentId)\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=sample_synid,\n            newData=sampleClinical,\n            filterBy=self.center,\n            toDelete=True,\n            col=cols.tolist(),\n        )\n    newClinicalDf.to_csv(newPath, sep=\"\\t\", index=False)\n    return newPath\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical-functions","title":"Functions","text":""},{"location":"reference/fileformats/clinical/#genie_registry.clinical._check_year","title":"<code>_check_year(clinicaldf, year_col, filename, allowed_string_values=None)</code>","text":"<p>Check year columns</p> PARAMETER DESCRIPTION <code>clinicaldf</code> <p>Clinical dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>year_col</code> <p>YEAR column</p> <p> TYPE: <code>int</code> </p> <code>filename</code> <p>Name of file</p> <p> TYPE: <code>str</code> </p> <code>allowed_string_values</code> <p>list of other allowed string values</p> <p> TYPE: <code>Optional[list]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Error message</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def _check_year(\n    clinicaldf: pd.DataFrame,\n    year_col: int,\n    filename: str,\n    allowed_string_values: Optional[list] = None,\n) -&gt; str:\n    \"\"\"Check year columns\n\n    Args:\n        clinicaldf: Clinical dataframe\n        year_col: YEAR column\n        filename: Name of file\n        allowed_string_values: list of other allowed string values\n\n    Returns:\n        Error message\n    \"\"\"\n    error = \"\"\n    if allowed_string_values is None:\n        allowed_string_values = []\n    if process_functions.checkColExist(clinicaldf, year_col):\n        # Deal with pre-redacted values and other allowed strings\n        # first because can't int(text) because there are\n        # instances that have &lt;YYYY\n        year_series = clinicaldf[year_col][\n            ~clinicaldf[year_col].isin(allowed_string_values)\n        ]\n        year_now = datetime.datetime.utcnow().year\n        try:\n            years = year_series.apply(\n                lambda x: datetime.datetime.strptime(str(int(x)), \"%Y\").year &gt; year_now\n            )\n            # Make sure that none of the years are greater than the current\n            # year.  It can be the same, but can't future years.\n            assert not years.any()\n        except Exception:\n            error = (\n                f\"{filename}: Please double check your {year_col} \"\n                \"column, it must be an integer in YYYY format \"\n                f\"&lt;= {year_now}\"\n            )\n            # Tack on allowed string values\n            if allowed_string_values:\n                error += \" or '{}'.\\n\".format(\"', '\".join(allowed_string_values))\n            else:\n                error += \".\\n\"\n    else:\n        error = f\"{filename}: Must have {year_col} column.\\n\"\n\n    return error\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical._check_int_dead_consistency","title":"<code>_check_int_dead_consistency(clinicaldf)</code>","text":"<p>Check if vital status interval and dead column are consistent</p> PARAMETER DESCRIPTION <code>clinicaldf</code> <p>Clinical Data Frame</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Error message if values and inconsistent or blank string</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def _check_int_dead_consistency(clinicaldf: pd.DataFrame) -&gt; str:\n    \"\"\"Check if vital status interval and dead column are consistent\n\n    Args:\n        clinicaldf: Clinical Data Frame\n\n    Returns:\n        Error message if values and inconsistent or blank string\n    \"\"\"\n    cols = [\"INT_DOD\", \"DEAD\"]\n    for col in cols:\n        # Return empty string is columns don't exist because this error\n        # is already handled.\n        if not process_functions.checkColExist(clinicaldf, col):\n            return \"\"\n    is_dead = clinicaldf[\"DEAD\"].astype(str) == \"True\"\n    is_alive = clinicaldf[\"DEAD\"].astype(str) == \"False\"\n    allowed_str = [\n        \"Unknown\",\n        \"Not Collected\",\n        \"Not Applicable\",\n        \"Not Released\",\n    ]\n    is_str = clinicaldf[\"DEAD\"].isin(allowed_str)\n    # Check that all string values are equal each other\n    is_equal = all(clinicaldf.loc[is_str, \"DEAD\"] == clinicaldf.loc[is_str, \"INT_DOD\"])\n    # If dead, int column can't be Not Applicable\n    # If alive, int column must be Not Applicable\n    if (\n        any(clinicaldf.loc[is_dead, \"INT_DOD\"] == \"Not Applicable\")\n        or not all(clinicaldf.loc[is_alive, \"INT_DOD\"] == \"Not Applicable\")\n        or not is_equal\n    ):\n        return (\n            \"Patient Clinical File: DEAD value is inconsistent with INT_DOD \"\n            \"for at least one patient.\\n\"\n        )\n    return \"\"\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical._check_int_year_consistency","title":"<code>_check_int_year_consistency(clinicaldf, cols, string_vals)</code>","text":"<p>Check if vital status interval and year columns are consistent in their values.</p> What determines text consistency <ul> <li>IF exactly one column in the columns being checked is \"Unknown\" in that row,     THEN that column with the \"Unknown\" value has to be the interval column</li> <li>OTHERWISE values must be either all numeric or the SAME string value in     for each row for all cols that are being checked</li> </ul> <p>Here we will show examples of invalid vs valid text consistency:</p> <pre><code>Note: INT_CONTACT and YEAR_CONTACT are the columns being checked, and INT_CONTACT\nis the interval column while YEAR_CONTACT is the year column.\n</code></pre> VALID Examples INT_CONTACT YEAR_CONTACT Unknown 2012 INT_CONTACT YEAR_CONTACT Unknown Unknown INT_CONTACT YEAR_CONTACT 2012 2012 INT_CONTACT YEAR_CONTACT Not Collected Not Collected INVALID Examples INT_CONTACT YEAR_CONTACT 2012 Unknown INT_CONTACT YEAR_CONTACT 2012 Not Collected INT_CONTACT YEAR_CONTACT Not collected Not Released PARAMETER DESCRIPTION <code>clinicaldf</code> <p>input Clinical Data Frame</p> <p> TYPE: <code>DataFrame</code> </p> <code>cols</code> <p>Columns in the clinical data frame to be checked for consistency</p> <p> TYPE: <code>List[str]</code> </p> <code>string_vals</code> <p>String values that aren't integers</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Error message if values are inconsistent or blank string</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def _check_int_year_consistency(\n    clinicaldf: pd.DataFrame, cols: List[str], string_vals: List[str]\n) -&gt; str:\n    \"\"\"\n    Check if vital status interval and year columns are consistent in\n    their values.\n\n    What determines text consistency:\n        - IF exactly one column in the columns being checked is \"Unknown\" in that row,\n            THEN that column with the \"Unknown\" value has to be the interval column\n        - OTHERWISE values must be either all numeric or the SAME string value in\n            for each row for all cols that are being checked\n\n    Here we will show examples of invalid vs valid text consistency:\n\n    ```\n    Note: INT_CONTACT and YEAR_CONTACT are the columns being checked, and INT_CONTACT\n    is the interval column while YEAR_CONTACT is the year column.\n    ```\n\n    Example: VALID Examples\n        | INT_CONTACT | YEAR_CONTACT |\n        | ----------- | -------------|\n        |   Unknown   |      2012    |\n\n        | INT_CONTACT | YEAR_CONTACT |\n        | ----------- | -------------|\n        |   Unknown   |    Unknown   |\n\n        | INT_CONTACT | YEAR_CONTACT |\n        | ----------- | -------------|\n        |     2012    |      2012    |\n\n        | INT_CONTACT   | YEAR_CONTACT   |\n        | --------------| ---------------|\n        | Not Collected | Not Collected  |\n\n    Example: INVALID Examples\n        | INT_CONTACT | YEAR_CONTACT |\n        | ----------- | -------------|\n        |     2012    |    Unknown   |\n\n        | INT_CONTACT   | YEAR_CONTACT   |\n        | --------------| ---------------|\n        |     2012      | Not Collected  |\n\n        | INT_CONTACT   | YEAR_CONTACT   |\n        | --------------| ---------------|\n        | Not collected | Not Released   |\n\n    Args:\n        clinicaldf (pd.DataFrame): input Clinical Data Frame\n        cols (List[str]): Columns in the clinical data frame to be\n            checked for consistency\n        string_vals (List[str]): String values that aren't integers\n\n    Returns:\n        Error message if values are inconsistent or blank string\n    \"\"\"\n    interval_col = \"\"\n    year_col = \"\"\n    for col in cols:\n        # This is assuming that interval and year columns start with\n        # INT/YEAR\n        interval_col = col if col.startswith(\"INT\") else interval_col\n        year_col = col if col.startswith(\"YEAR\") else year_col\n        # Return empty string if columns don't exist because this error\n        # is already handled.\n        if not process_functions.checkColExist(clinicaldf, col):\n            return \"\"\n\n    is_text_inconsistent = False\n    # Get index of all rows that have 'missing' values\n    for str_val in string_vals:\n        # n string values per row\n        n_str = (clinicaldf[cols] == str_val).sum(axis=1)\n        # year can be known with unknown interval value\n        # otherwise must be all numeric or the same text value\n        if str_val == \"Unknown\":\n            if ((n_str == 1) &amp; (clinicaldf[interval_col] != \"Unknown\")).any():\n                is_text_inconsistent = True\n        else:\n            if n_str.between(0, len(cols), inclusive=\"neither\").any():\n                is_text_inconsistent = True\n\n    is_redaction_inconsistent = False\n    # Check that the redacted values are consistent\n    is_redacted_int_89 = clinicaldf[interval_col] == \"&gt;32485\"\n    is_redacted_year_89 = clinicaldf[year_col] == \"&gt;89\"\n    is_redacted_int = clinicaldf[interval_col] == \"&lt;6570\"\n    is_redacted_year = clinicaldf[year_col] == \"&lt;18\"\n    if any(is_redacted_int != is_redacted_year) or any(\n        is_redacted_int_89 != is_redacted_year_89\n    ):\n        is_redaction_inconsistent = True\n\n    col_strs = \", \".join(cols)\n    if is_text_inconsistent and is_redaction_inconsistent:\n        return (\n            \"Patient: you have inconsistent redaction and text \"\n            f\"values in {col_strs}.\\n\"\n        )\n    if is_redaction_inconsistent:\n        return f\"Patient: you have inconsistent redaction values in {col_strs}.\\n\"\n    if is_text_inconsistent:\n        return f\"Patient: you have inconsistent text values in {col_strs}.\\n\"\n\n    return \"\"\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical._check_year_death_validity","title":"<code>_check_year_death_validity(clinicaldf)</code>","text":"<p>YEAR_DEATH should alway be greater than or equal to YEAR_CONTACT when they are both available. This function checks if YEAR_DEATH &gt;= YEAR_CONTACT and returns row indices of invalid YEAR_DEATH rows.</p> PARAMETER DESCRIPTION <code>clinicaldf</code> <p>Clinical Data Frame</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Index</code> <p>pd.Index: The row indices of the row with YEAR_DEATH &lt; YEAR_CONTACT in the input clinical data</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def _check_year_death_validity(clinicaldf: pd.DataFrame) -&gt; pd.Index:\n    \"\"\"\n    YEAR_DEATH should alway be greater than or equal to YEAR_CONTACT when they are both available.\n    This function checks if YEAR_DEATH &gt;= YEAR_CONTACT and returns row indices of invalid YEAR_DEATH rows.\n\n    Args:\n        clinicaldf: Clinical Data Frame\n\n    Returns:\n        pd.Index: The row indices of the row with YEAR_DEATH &lt; YEAR_CONTACT in the input clinical data\n    \"\"\"\n    # Generate temp dataframe to handle datatype mismatch in a column\n    temp = clinicaldf[[\"YEAR_DEATH\", \"YEAR_CONTACT\"]].copy()\n    # Convert YEAR_DEATH and YEAR_CONTACT to numeric, coercing errors to NaN\n    temp[\"YEAR_DEATH\"] = pd.to_numeric(temp[\"YEAR_DEATH\"], errors=\"coerce\")\n    temp[\"YEAR_CONTACT\"] = pd.to_numeric(temp[\"YEAR_CONTACT\"], errors=\"coerce\")\n    # Compare rows with numeric values in both YEAR_DEATH and YEAR_CONTACT columns\n    temp[\"check_result\"] = np.where(\n        (pd.isna(temp[\"YEAR_DEATH\"]) | pd.isna(temp[\"YEAR_CONTACT\"])),\n        \"N/A\",\n        temp[\"YEAR_DEATH\"] &gt;= temp[\"YEAR_CONTACT\"],\n    )\n    invalid_year_death = temp[temp[\"check_result\"] == \"False\"]\n    return invalid_year_death.index\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical._check_year_death_validity_message","title":"<code>_check_year_death_validity_message(invalid_year_death_indices)</code>","text":"<p>This function returns the error and warning messages if the input clinical data has row with YEAR_DEATH &lt; YEAR_CONTACT</p> PARAMETER DESCRIPTION <code>invalid_year_death_indices</code> <p>The row indices of the rows with YEAR_DEATH &lt; YEAR_CONTACT in the input clinical data</p> <p> TYPE: <code>Index</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Tuple[str, str]: The error message that tells you how many patients with invalid YEAR_DEATH values that your</p> <code>str</code> <p>input clinical data has</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def _check_year_death_validity_message(\n    invalid_year_death_indices: pd.Index,\n) -&gt; Tuple[str, str]:\n    \"\"\"This function returns the error and warning messages\n    if the input clinical data has row with YEAR_DEATH &lt; YEAR_CONTACT\n\n    Args:\n        invalid_year_death_indices: The row indices of the rows with YEAR_DEATH &lt; YEAR_CONTACT in the input clinical data\n\n    Returns:\n        Tuple[str, str]: The error message that tells you how many patients with invalid YEAR_DEATH values that your\n        input clinical data has\n    \"\"\"\n    error = \"\"\n    warning = \"\"\n    if len(invalid_year_death_indices) &gt; 0:\n        error = (\n            \"Patient Clinical File: Please double check your YEAR_DEATH and YEAR_CONTACT columns. \"\n            \"YEAR_DEATH must be &gt;= YEAR_CONTACT.\\n\"\n        )\n    return error, warning\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical._check_int_dod_validity","title":"<code>_check_int_dod_validity(clinicaldf)</code>","text":"<p>INT_DOD should alway be greater than or equal to INT_CONTACT when they are both available. This function checks if INT_DOD &gt;= INT_CONTACT and returns row indices of invalid INT_DOD rows.</p> PARAMETER DESCRIPTION <code>clinicaldf</code> <p>Clinical Data Frame</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Index</code> <p>pd.Index: The row indices of the row with INT_DOD &lt; INT_CONTACT in the input clinical data</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def _check_int_dod_validity(clinicaldf: pd.DataFrame) -&gt; pd.Index:\n    \"\"\"\n    INT_DOD should alway be greater than or equal to INT_CONTACT when they are both available.\n    This function checks if INT_DOD &gt;= INT_CONTACT and returns row indices of invalid INT_DOD rows.\n\n    Args:\n        clinicaldf: Clinical Data Frame\n\n    Returns:\n        pd.Index: The row indices of the row with INT_DOD &lt; INT_CONTACT in the input clinical data\n    \"\"\"\n    # Generate temp dataframe to handle datatype mismatch in a column\n    temp = clinicaldf[[\"INT_DOD\", \"INT_CONTACT\"]].copy()\n    # Convert INT_DOD and INT_CONTACT to numeric, coercing errors to NaN\n    temp[\"INT_DOD\"] = pd.to_numeric(temp[\"INT_DOD\"], errors=\"coerce\")\n    temp[\"INT_CONTACT\"] = pd.to_numeric(temp[\"INT_CONTACT\"], errors=\"coerce\")\n    # Compare rows with numeric values in both INT_DOD and INT_CONTACT columns\n    temp[\"check_result\"] = np.where(\n        (pd.isna(temp[\"INT_DOD\"]) | pd.isna(temp[\"INT_CONTACT\"])),\n        \"N/A\",\n        temp[\"INT_DOD\"] &gt;= temp[\"INT_CONTACT\"],\n    )\n    invalid_int_dod = temp[temp[\"check_result\"] == \"False\"]\n    return invalid_int_dod.index\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical._check_int_dod_validity_message","title":"<code>_check_int_dod_validity_message(invalid_int_dod_indices)</code>","text":"<p>This function returns the error and warning messages if the input clinical data has row with INT_DOD &lt; INT_CONTACT</p> PARAMETER DESCRIPTION <code>invalid_int_dod_indices</code> <p>The row indices of the rows with INT_DOD &lt; INT_CONTACT in the input clinical data</p> <p> TYPE: <code>Index</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Tuple[str, str]: The error message that tells you how many patients with invalid INT_DOD values that your</p> <code>str</code> <p>input clinical data has</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def _check_int_dod_validity_message(\n    invalid_int_dod_indices: pd.Index,\n) -&gt; Tuple[str, str]:\n    \"\"\"This function returns the error and warning messages\n    if the input clinical data has row with INT_DOD &lt; INT_CONTACT\n\n    Args:\n        invalid_int_dod_indices: The row indices of the rows with INT_DOD &lt; INT_CONTACT in the input clinical data\n\n    Returns:\n        Tuple[str, str]: The error message that tells you how many patients with invalid INT_DOD values that your\n        input clinical data has\n    \"\"\"\n    error = \"\"\n    warning = \"\"\n    if len(invalid_int_dod_indices) &gt; 0:\n        error = (\n            \"Patient Clinical File: Please double check your INT_DOD and INT_CONTACT columns. \"\n            \"INT_DOD must be &gt;= INT_CONTACT.\\n\"\n        )\n    return error, warning\n</code></pre>"},{"location":"reference/fileformats/clinical/#genie_registry.clinical.remap_clinical_values","title":"<code>remap_clinical_values(clinicaldf, sex_mapping, race_mapping, ethnicity_mapping, sampletype_mapping)</code>","text":"<p>Remap clinical attributes from integer to string values</p> PARAMETER DESCRIPTION <code>clinicaldf</code> <p>Clinical data</p> <p> TYPE: <code>DataFrame</code> </p> <code>sex_mapping</code> <p>Sex mapping data</p> <p> TYPE: <code>DataFrame</code> </p> <code>race_mapping</code> <p>Race mapping data</p> <p> TYPE: <code>DataFrame</code> </p> <code>ethnicity_mapping</code> <p>Ethnicity mapping data</p> <p> TYPE: <code>DataFrame</code> </p> <code>sample_type</code> <p>Sample type mapping data</p> <p> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Mapped clinical dataframe</p> Source code in <code>genie_registry/clinical.py</code> <pre><code>def remap_clinical_values(\n    clinicaldf: pd.DataFrame,\n    sex_mapping: pd.DataFrame,\n    race_mapping: pd.DataFrame,\n    ethnicity_mapping: pd.DataFrame,\n    sampletype_mapping: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Remap clinical attributes from integer to string values\n\n    Args:\n        clinicaldf: Clinical data\n        sex_mapping: Sex mapping data\n        race_mapping: Race mapping data\n        ethnicity_mapping: Ethnicity mapping data\n        sample_type: Sample type mapping data\n\n    Returns:\n        Mapped clinical dataframe\n    \"\"\"\n\n    race_mapping.index = race_mapping[\"CODE\"]\n    race_dict = race_mapping.to_dict()\n\n    ethnicity_mapping.index = ethnicity_mapping[\"CODE\"]\n    ethnicity_dict = ethnicity_mapping.to_dict()\n\n    sex_mapping.index = sex_mapping[\"CODE\"]\n    sex_dict = sex_mapping.to_dict()\n\n    sampletype_mapping.index = sampletype_mapping[\"CODE\"]\n    sampletype_dict = sampletype_mapping.to_dict()\n\n    for column in [\n        \"PRIMARY_RACE\",\n        \"SECONDARY_RACE\",\n        \"TERTIARY_RACE\",\n        \"SEX\",\n        \"ETHNICITY\",\n        \"SAMPLE_TYPE\",\n    ]:\n        if column in clinicaldf.columns:\n            clinicaldf[f\"{column}_DETAILED\"] = clinicaldf[column]\n\n    # Use pandas mapping feature\n    clinicaldf = clinicaldf.replace(\n        {\n            \"PRIMARY_RACE\": race_dict[\"CBIO_LABEL\"],\n            \"SECONDARY_RACE\": race_dict[\"CBIO_LABEL\"],\n            \"TERTIARY_RACE\": race_dict[\"CBIO_LABEL\"],\n            \"SAMPLE_TYPE\": sampletype_dict[\"CBIO_LABEL\"],\n            \"SEX\": sex_dict[\"CBIO_LABEL\"],\n            \"ETHNICITY\": ethnicity_dict[\"CBIO_LABEL\"],\n            \"PRIMARY_RACE_DETAILED\": race_dict[\"DESCRIPTION\"],\n            \"SECONDARY_RACE_DETAILED\": race_dict[\"DESCRIPTION\"],\n            \"TERTIARY_RACE_DETAILED\": race_dict[\"DESCRIPTION\"],\n            \"SAMPLE_TYPE_DETAILED\": sampletype_dict[\"DESCRIPTION\"],\n            \"SEX_DETAILED\": sex_dict[\"DESCRIPTION\"],\n            \"ETHNICITY_DETAILED\": ethnicity_dict[\"DESCRIPTION\"],\n        }\n    )\n\n    return clinicaldf\n</code></pre>"},{"location":"reference/fileformats/cna/","title":"Cna","text":""},{"location":"reference/fileformats/cna/#genie_registry.cna","title":"<code>genie_registry.cna</code>","text":""},{"location":"reference/fileformats/cna/#genie_registry.cna-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/cna/#genie_registry.cna.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/cna/#genie_registry.cna-classes","title":"Classes","text":""},{"location":"reference/fileformats/cna/#genie_registry.cna.cna","title":"<code>cna</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/cna.py</code> <pre><code>class cna(FileTypeFormat):\n    _fileType = \"cna\"\n\n    _process_kwargs = [\"newPath\"]\n\n    _validation_kwargs = [\"nosymbol_check\"]\n\n    # VALIDATE FILENAME\n    def _validateFilename(self, filePath):\n        assert os.path.basename(filePath[0]) == \"data_CNA_{}.txt\".format(self.center)\n\n    def _process(self, cnaDf):\n        cnaDf.rename(columns={cnaDf.columns[0]: cnaDf.columns[0].upper()}, inplace=True)\n        cnaDf.rename(columns={\"HUGO_SYMBOL\": \"Hugo_Symbol\"}, inplace=True)\n\n        index = [\n            i for i, col in enumerate(cnaDf.columns) if col.upper() == \"ENTREZ_GENE_ID\"\n        ]\n        if len(index) &gt; 0:\n            del cnaDf[cnaDf.columns[index][0]]\n\n        bedSynId = self.genie_config[\"bed\"]\n        bed = self.syn.tableQuery(\n            f\"select Hugo_Symbol, ID from {bedSynId} where CENTER = '{self.center}'\"\n        )\n        bedDf = bed.asDataFrame()\n        cnaDf[\"Hugo_Symbol\"] = cnaDf[\"Hugo_Symbol\"].apply(\n            lambda x: validateSymbol(x, bedDf)\n        )\n        order = cnaDf.columns\n        cnaDf = cnaDf[~cnaDf[\"Hugo_Symbol\"].isnull()]\n        # cnaDf = cnaDf.applymap(str)\n        duplicatedGenes = pd.DataFrame()\n        duplicated_symbols = cnaDf[\"Hugo_Symbol\"][\n            cnaDf[\"Hugo_Symbol\"].duplicated()\n        ].unique()\n        for i in duplicated_symbols:\n            dups = cnaDf[cnaDf[\"Hugo_Symbol\"] == i]\n            newVal = dups[dups.columns[dups.columns != \"Hugo_Symbol\"]].apply(\n                mergeCNAvalues\n            )\n            temp = pd.DataFrame(newVal).transpose()\n            temp[\"Hugo_Symbol\"] = i\n            duplicatedGenes = pd.concat([duplicatedGenes, temp], sort=False)\n        cnaDf.drop_duplicates(\"Hugo_Symbol\", keep=False, inplace=True)\n        cnaDf = pd.concat([cnaDf, duplicatedGenes], sort=False)\n        cnaDf = cnaDf[order]\n        return cnaDf\n\n    def process_steps(self, cnaDf, newPath):\n        newCNA = self._process(cnaDf)\n\n        centerMafSynId = self.genie_config[\"centerMaf\"]\n        if not newCNA.empty:\n            cnaText = process_functions.removePandasDfFloat(newCNA)\n            # Replace blank with NA's\n            cnaText = (\n                cnaText.replace(\"\\t\\t\", \"\\tNA\\t\")\n                .replace(\"\\t\\t\", \"\\tNA\\t\")\n                .replace(\"\\t\\n\", \"\\tNA\\n\")\n            )\n            with open(newPath, \"w\") as cnaFile:\n                cnaFile.write(cnaText)\n            self.syn.store(synapseclient.File(newPath, parent=centerMafSynId))\n        return newPath\n\n    def _validate(self, cnvDF, nosymbol_check):\n        total_error = \"\"\n        warning = \"\"\n        cnvDF.columns = [col.upper() for col in cnvDF.columns]\n\n        if cnvDF.columns[0] != \"HUGO_SYMBOL\":\n            total_error += \"Your cnv file's first column must be Hugo_Symbol\\n\"\n        haveColumn = process_functions.checkColExist(cnvDF, \"HUGO_SYMBOL\")\n        if haveColumn:\n            keepSymbols = cnvDF[\"HUGO_SYMBOL\"]\n            cnvDF.drop(\"HUGO_SYMBOL\", axis=1, inplace=True)\n\n        # if sum(cnvDF.apply(lambda x: sum(x.isnull()))) &gt; 0:\n        #   total_error += \"Your cnv file must not have any empty values\\n\"\n\n        if process_functions.checkColExist(cnvDF, \"ENTREZ_GENE_ID\"):\n            del cnvDF[\"ENTREZ_GENE_ID\"]\n        error = process_functions.validate_genie_identifier(\n            identifiers=cnvDF.columns, center=self.center, filename=\"cnv\", col=\"samples\"\n        )\n        total_error += error\n        # cnvDF = cnvDF.fillna('')\n        allowed_values = [\n            \"-2.0\",\n            \"-2\",\n            \"-1.5\",\n            \"-1.0\",\n            \"-1\",\n            \"0.0\",\n            \"0\",\n            \"0.5\",\n            \"1.0\",\n            \"1\",\n            \"1.5\",\n            \"2\",\n            \"2.0\",\n            \"nan\",\n        ]\n        if not all(cnvDF.applymap(lambda x: str(x) in allowed_values).all()):\n            total_error += (\n                \"All values must be NA/blank, -2, -1.5, -1, -0.5, \"\n                \"0, 0.5, 1, 1.5, or 2.\\n\"\n            )\n        else:\n            cnvDF[\"HUGO_SYMBOL\"] = keepSymbols\n            if haveColumn and not nosymbol_check:\n                bedSynId = self.genie_config[\"bed\"]\n                bed = self.syn.tableQuery(\n                    f\"select Hugo_Symbol, ID from {bedSynId} \"\n                    f\"where CENTER = '{self.center}'\"\n                )\n                bedDf = bed.asDataFrame()\n                cnvDF[\"remapped\"] = cnvDF[\"HUGO_SYMBOL\"].apply(\n                    lambda x: validateSymbol(x, bedDf)\n                )\n                cnvDF = cnvDF[~cnvDF[\"remapped\"].isnull()]\n\n                # Do not allow any duplicated genes after symbols\n                # have been remapped\n                if sum(cnvDF[\"remapped\"].duplicated()) &gt; 0:\n                    duplicated = cnvDF[\"remapped\"].duplicated(keep=False)\n                    total_error += (\n                        \"Your CNA file has duplicated Hugo_Symbols \"\n                        \"(After remapping of genes): {} -&gt; {}.\\n\".format(\n                            \",\".join(cnvDF[\"HUGO_SYMBOL\"][duplicated]),\n                            \",\".join(cnvDF[\"remapped\"][duplicated]),\n                        )\n                    )\n        return (total_error, warning)\n</code></pre>"},{"location":"reference/fileformats/cna/#genie_registry.cna-functions","title":"Functions","text":""},{"location":"reference/fileformats/cna/#genie_registry.cna.validateSymbol","title":"<code>validateSymbol(gene, bedDf, returnMappedDf=True)</code>","text":"<p>Validates the gene symbol against the gene symbol in the bed database. Note that gene symbols in the bed database have gone through processing and have been remapped to allowed actual genes if needed.</p> Two conditions must be met for the gene to be VALID <ol> <li> <p>The gene exists in the bed database table's Hugo_Symbol column</p> </li> <li> <p>The gene exists in the bed database table's ID column. Under this condition, the gene in the cna file will be REMAPPED temporarily to the bed database table's Hugo_Symbol value for the purpose of validation. The ID column is the original Hugo_Symbol column of the bed files before the Hugo_Symbol column gets mapped to valid possible gene values in the Actual Gene Positions (GRCh37) database table. See the bed fileformat module's remap_symbols function and how it gets used in processing for more info on this.</p> </li> </ol> <p>The validation throws a WARNING if the gene doesn't satisfy either of the above two conditions</p> PARAMETER DESCRIPTION <code>gene</code> <p>Gene name</p> <p> TYPE: <code>str</code> </p> <code>bedDf</code> <p>The bed database table as a pandas dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>returnMappedDf</code> <p>Return a mapped gene. Defaults to True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Union[str, float, bool]</code> <p>Union[str, float, bool]:</p> <code>Union[str, float, bool]</code> <p>Returns gene symbol (str if valid, a float(\"nan\") if invalid) if returnMappedDf is True</p> <code>Union[str, float, bool]</code> <p>Returns boolean for whether a gene is valid if returnMappedDf is False</p> Source code in <code>genie_registry/cna.py</code> <pre><code>def validateSymbol(\n    gene: str, bedDf: pd.DataFrame, returnMappedDf: bool = True\n) -&gt; Union[str, float, bool]:\n    \"\"\"\n    Validates the gene symbol against the gene symbol in the bed database.\n    Note that gene symbols in the bed database have gone through processing and\n    have been remapped to allowed actual genes if needed.\n\n    Two conditions must be met for the gene to be VALID:\n        1. The gene exists in the bed database table's Hugo_Symbol column\n\n        2. The gene exists in the bed database table's ID column. Under this condition,\n        the gene in the cna file will be REMAPPED temporarily to the bed database\n        table's Hugo_Symbol value for the purpose of validation. The ID column is the\n        original Hugo_Symbol column of the bed files before the Hugo_Symbol column gets\n        mapped to valid possible gene values in the Actual Gene Positions (GRCh37)\n        database table. See the bed fileformat module's remap_symbols function and\n        how it gets used in processing for more info on this.\n\n    The validation throws a WARNING if the gene doesn't satisfy\n    either of the above two conditions\n\n    Args:\n        gene: Gene name\n        bedDf: The bed database table as a pandas dataframe\n        returnMappedDf: Return a mapped gene. Defaults to True\n\n    Returns:\n        Union[str, float, bool]:\n        Returns gene symbol (str if valid, a float(\"nan\") if invalid) if returnMappedDf is True\n        Returns boolean for whether a gene is valid if returnMappedDf is False\n    \"\"\"\n    valid = False\n    if sum(bedDf[\"Hugo_Symbol\"] == gene) &gt; 0:\n        valid = True\n    elif sum(bedDf[\"ID\"] == gene) &gt; 0:\n        mismatch = bedDf[bedDf[\"ID\"] == gene]\n        mismatch.drop_duplicates(inplace=True)\n        logger.info(\n            \"{} will be remapped to {}\".format(gene, mismatch[\"Hugo_Symbol\"].values[0])\n        )\n        gene = mismatch[\"Hugo_Symbol\"].values[0]\n    else:\n        logger.warning(\n            \"{} cannot be remapped and will not be released. The symbol \"\n            \"must exist in your seq assay ids (bed files) and must be \"\n            \"mappable to a gene.\".format(gene)\n        )\n        gene = float(\"nan\")\n    if returnMappedDf:\n        return gene\n    else:\n        return valid\n</code></pre>"},{"location":"reference/fileformats/cna/#genie_registry.cna.makeCNARow","title":"<code>makeCNARow(row, symbols)</code>","text":"<p>Make CNA Row (Deprecated function)</p> <p>CNA values are no longer stored in the database</p> PARAMETER DESCRIPTION <code>row</code> <p>one row in the CNA file</p> <p> </p> <code>symbols</code> <p>list of Gene symbols</p> <p> </p> Source code in <code>genie_registry/cna.py</code> <pre><code>def makeCNARow(row, symbols):\n    \"\"\"\n    Make CNA Row (Deprecated function)\n\n    CNA values are no longer stored in the database\n\n    Args:\n        row: one row in the CNA file\n        symbols:  list of Gene symbols\n    \"\"\"\n    totalrow = \"{symbols}\\n{values}\".format(\n        symbols=\",\".join(symbols), values=\",\".join(row.astype(str))\n    )\n    totalrow = totalrow.replace(\".0\", \"\")\n    return totalrow\n</code></pre>"},{"location":"reference/fileformats/cna/#genie_registry.cna.mergeCNAvalues","title":"<code>mergeCNAvalues(x)</code>","text":"<p>Merge CNA values, make sure if there are two rows that are the same gene, the values are merged</p> Source code in <code>genie_registry/cna.py</code> <pre><code>def mergeCNAvalues(x):\n    \"\"\"Merge CNA values, make sure if there are two rows that are the\n    same gene, the values are merged\"\"\"\n    # Change into its own series, because sometimes doing an apply\n    # will cause there to be a missing index value which will\n    # cause dropna() to fail.\n    values = pd.Series(x.values)\n    values.dropna(inplace=True)\n    uniqueValues = set(values.unique())\n    if len(uniqueValues) == 1:\n        returnVal = x.tolist()[0]\n    elif len(uniqueValues) &lt;= 2:\n        uniqueValues.discard(0)\n        if len(uniqueValues) == 1:\n            returnVal = list(uniqueValues)[0]\n        else:\n            returnVal = float(\"nan\")\n    else:\n        returnVal = float(\"nan\")\n    return returnVal\n</code></pre>"},{"location":"reference/fileformats/cna/#genie_registry.cna.checkIfOneZero","title":"<code>checkIfOneZero(x)</code>","text":"Source code in <code>genie_registry/cna.py</code> <pre><code>def checkIfOneZero(x):\n    assert len(set(x.tolist())) == 1, \"Can only be one unique value\"\n</code></pre>"},{"location":"reference/fileformats/fileformat/","title":"Fileformat","text":""},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format","title":"<code>genie.example_filetype_format</code>","text":"<p>TODO: Rename this to model.py This contains the GENIE model objects</p>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format-classes","title":"Classes","text":""},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.ValidationResults","title":"<code>ValidationResults</code>  <code>dataclass</code>","text":"<p>Validation results</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>@dataclass\nclass ValidationResults:\n    \"\"\"Validation results\"\"\"\n\n    errors: str\n    warnings: str\n    detailed: Optional[str] = None\n\n    def is_valid(self) -&gt; bool:\n        \"\"\"True if file is valid\"\"\"\n        return self.errors == \"\"\n\n    def collect_errors_and_warnings(self) -&gt; str:\n        \"\"\"Aggregates error and warnings into a string.\n\n        Returns:\n            str: A message containing errors and warnings.\n        \"\"\"\n        # Complete error message\n        message = \"----------------ERRORS----------------\\n\"\n        if self.errors == \"\":\n            message = \"YOUR FILE IS VALIDATED!\\n\"\n            logger.info(message)\n        else:\n            for error in self.errors.split(\"\\n\"):\n                if error != \"\":\n                    logger.error(error)\n            message += self.errors\n        if self.warnings != \"\":\n            for warning in self.warnings.split(\"\\n\"):\n                if warning != \"\":\n                    logger.warning(warning)\n            message += \"-------------WARNINGS-------------\\n\" + self.warnings\n        return message\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.ValidationResults-functions","title":"Functions","text":""},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.ValidationResults.is_valid","title":"<code>is_valid()</code>","text":"<p>True if file is valid</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def is_valid(self) -&gt; bool:\n    \"\"\"True if file is valid\"\"\"\n    return self.errors == \"\"\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.ValidationResults.collect_errors_and_warnings","title":"<code>collect_errors_and_warnings()</code>","text":"<p>Aggregates error and warnings into a string.</p> RETURNS DESCRIPTION <code>str</code> <p>A message containing errors and warnings.</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def collect_errors_and_warnings(self) -&gt; str:\n    \"\"\"Aggregates error and warnings into a string.\n\n    Returns:\n        str: A message containing errors and warnings.\n    \"\"\"\n    # Complete error message\n    message = \"----------------ERRORS----------------\\n\"\n    if self.errors == \"\":\n        message = \"YOUR FILE IS VALIDATED!\\n\"\n        logger.info(message)\n    else:\n        for error in self.errors.split(\"\\n\"):\n            if error != \"\":\n                logger.error(error)\n        message += self.errors\n    if self.warnings != \"\":\n        for warning in self.warnings.split(\"\\n\"):\n            if warning != \"\":\n                logger.warning(warning)\n        message += \"-------------WARNINGS-------------\\n\" + self.warnings\n    return message\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/fileformat/#genie.example_filetype_format.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/maf/","title":"Maf","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf","title":"<code>genie_registry.maf</code>","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf-classes","title":"Classes","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.maf","title":"<code>maf</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> <p>MAF file format validation / processing</p> Source code in <code>genie_registry/maf.py</code> <pre><code>class maf(FileTypeFormat):\n    \"\"\"\n    MAF file format validation / processing\n    \"\"\"\n\n    _fileType = \"maf\"\n\n    _process_kwargs = []\n    _allele_cols = [\"REFERENCE_ALLELE\", \"TUMOR_SEQ_ALLELE1\", \"TUMOR_SEQ_ALLELE2\"]\n    _allowed_comb_alleles = [\"A\", \"T\", \"C\", \"G\", \"N\"]\n    _allowed_ind_alleles = [\"-\"]\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Validates filename.  Should be\n        data_mutations_extended_CENTER.txt\n        \"\"\"\n        assert os.path.basename(filePath[0]) == \"data_mutations_extended_{}.txt\".format(\n            self.center\n        )\n\n    def process_steps(self, df):\n        \"\"\"The processing of maf files is specific to GENIE, so\n        not included in this function\"\"\"\n        logger.info(\n            \"Please run with `--process mutation` parameter \"\n            \"if you want to reannotate the mutation files\"\n        )\n        return None\n\n    def _validate(self, mutationDF):\n        \"\"\"\n        This function validates the mutation file to make sure it\n        adheres to the mutation SOP.\n\n        t_depth: This column is conditionally optional.\n        1. If this column is missing, the data must include the t_ref_count column. Otherwise, it will cause a validation error.\n        2. If this column is present, it must have one of the following:\n            - A mix of numeric values and NAs\n            - All NAs\n            - All numeric values\n\n        There are no other checks on the actual values in this column.\n\n        t_ref_count: This column is conditionally optional.\n        1. If this column is missing, the data must include the t_depth column. Otherwise, it will cause a validation error.\n        2. If this column is present, it must have one of the following:\n            - A mix of numeric values and NAs\n            - All NAs\n            - All numeric values\n\n        There are no other checks on the actual values in this column.\n\n        t_alt_count: This column is entirely optional.\n        1. If this column is present, it must have one of the following:\n            - A mix of numeric values and NAs\n            - All NAs\n            - All numeric values\n\n        There are no other checks on the actual values in this column.\n\n        Args:\n            mutationDF: mutation dataframe\n\n        Returns:\n            Text with all the errors in the mutation file\n        \"\"\"\n\n        first_header = [\"CHROMOSOME\", \"HUGO_SYMBOL\", \"TUMOR_SAMPLE_BARCODE\"]\n        SP = self._fileType == \"mafSP\"\n        if SP:\n            correct_column_headers = [\n                \"CHROMOSOME\",\n                \"START_POSITION\",\n                \"REFERENCE_ALLELE\",\n                \"TUMOR_SAMPLE_BARCODE\",\n                \"TUMOR_SEQ_ALLELE2\",\n            ]\n            # T_REF_COUNT + T_ALT_COUNT = T_DEPTH\n        else:\n            correct_column_headers = [\n                \"CHROMOSOME\",\n                \"START_POSITION\",\n                \"REFERENCE_ALLELE\",\n                \"TUMOR_SAMPLE_BARCODE\",\n                \"T_ALT_COUNT\",\n                \"TUMOR_SEQ_ALLELE2\",\n            ]\n            # T_REF_COUNT + T_ALT_COUNT = T_DEPTH\n        optional_headers = [\"T_REF_COUNT\", \"N_DEPTH\", \"N_REF_COUNT\", \"N_ALT_COUNT\"]\n\n        mutationDF.columns = [col.upper() for col in mutationDF.columns]\n\n        # total_error = \"\"\n        total_error = StringIO()\n        warning = StringIO()\n\n        # CHECK: Everything in correct_column_headers must be in mutation file\n        if not all(\n            [\n                process_functions.checkColExist(mutationDF, i)\n                for i in correct_column_headers\n            ]\n        ):\n            total_error.write(\n                \"maf: Must at least have these headers: {}. \"\n                \"If you are writing your maf file with R, please make\"\n                \"sure to specify the 'quote=FALSE' parameter.\\n\".format(\n                    \",\".join(\n                        [\n                            i\n                            for i in correct_column_headers\n                            if i not in mutationDF.columns.values\n                        ]\n                    )\n                )\n            )\n        else:\n            # CHECK: First column must be in the first_header list\n            if mutationDF.columns[0] not in first_header:\n                total_error.write(\n                    \"maf: First column header must be \"\n                    \"one of these: {}.\\n\".format(\", \".join(first_header))\n                )\n            # No duplicated values\n            primary_cols = [\n                \"CHROMOSOME\",\n                \"START_POSITION\",\n                \"REFERENCE_ALLELE\",\n                \"TUMOR_SAMPLE_BARCODE\",\n                \"TUMOR_SEQ_ALLELE2\",\n            ]\n            # Strip white space if string column\n            for col in primary_cols:\n                if mutationDF[col].dtype == object:\n                    mutationDF[col] = mutationDF[col].str.strip()\n            duplicated_idx = mutationDF.duplicated(primary_cols)\n            # Find samples with duplicated variants\n            duplicated_variants = (\n                mutationDF[\"TUMOR_SAMPLE_BARCODE\"][duplicated_idx]\n                .unique()\n                .astype(str)\n                .tolist()\n            )\n\n            if duplicated_idx.any():\n                total_error.write(\n                    \"maf: Must not have duplicated variants. \"\n                    \"Samples with duplicated variants: \"\n                    f\"{', '.join(duplicated_variants)}\\n\"\n                )\n\n        t_depth_exists = process_functions.checkColExist(mutationDF, \"T_DEPTH\")\n        t_ref_exists = process_functions.checkColExist(mutationDF, \"T_REF_COUNT\")\n        if not t_depth_exists and not t_ref_exists and not SP:\n            total_error.write(\"maf: If missing T_DEPTH, must have T_REF_COUNT!\\n\")\n        numerical_cols = [\n            \"T_DEPTH\",\n            \"T_ALT_COUNT\",\n            \"T_REF_COUNT\",\n            \"N_DEPTH\",\n            \"N_REF_COUNT\",\n            \"N_ALT_COUNT\",\n            \"START_POSITION\",\n            \"END_POSITION\",\n        ]\n        actual_numerical_cols = []\n        for col in numerical_cols:\n            col_exists = process_functions.checkColExist(mutationDF, col)\n            if col_exists:\n                # Attempt to convert column to float\n                try:\n                    mutationDF[col] = mutationDF[col].astype(float)\n                except ValueError:\n                    pass\n                if mutationDF[col].dtype not in [int, float]:\n                    total_error.write(f\"maf: {col} must be a numerical column.\\n\")\n                else:\n                    actual_numerical_cols.append(col)\n\n        # CHECK: Must have TUMOR_SEQ_ALLELE2\n        error, warn = _check_allele_col(mutationDF, \"TUMOR_SEQ_ALLELE2\")\n        total_error.write(error)\n        warning.write(warn)\n\n        # CHECK: Mutation file would benefit from columns in optional_headers\n        if (\n            not all(\n                [\n                    process_functions.checkColExist(mutationDF, i)\n                    for i in optional_headers\n                ]\n            )\n            and not SP\n        ):\n            warning.write(\n                \"maf: Does not have the column headers that can give extra \"\n                \"information to the processed maf: {}.\\n\".format(\n                    \", \".join(\n                        [\n                            i\n                            for i in optional_headers\n                            if i not in mutationDF.columns.values\n                        ]\n                    )\n                )\n            )\n\n        # CHECK: Must have REFERENCE_ALLELE\n        error, warn = _check_allele_col(mutationDF, \"REFERENCE_ALLELE\")\n        total_error.write(error)\n        warning.write(warn)\n\n        error, warn = validate._validate_chromosome(\n            df=mutationDF, col=\"CHROMOSOME\", fileformat=\"maf\", allow_chr=False\n        )\n        total_error.write(error)\n        warning.write(warn)\n        # if process_functions.checkColExist(mutationDF, \"CHROMOSOME\"):\n        #     # CHECK: Chromosome column can't have any values that start\n        #     # with chr or have any WT values\n        #     invalid_values = [\n        #         str(i).startswith(\"chr\") or str(i) == \"WT\"\n        #         for i in mutationDF[\"CHROMOSOME\"]\n        #     ]\n        #     if sum(invalid_values) &gt; 0:\n        #         total_error.write(\n        #             \"maf: CHROMOSOME column cannot have any values that \"\n        #             \"start with 'chr' or any 'WT' values.\\n\"\n        #         )\n\n        error = _check_allele_col_validity(mutationDF)\n        total_error.write(error)\n\n        if process_functions.checkColExist(mutationDF, \"TUMOR_SAMPLE_BARCODE\"):\n            error = process_functions.validate_genie_identifier(\n                identifiers=mutationDF[\"TUMOR_SAMPLE_BARCODE\"],\n                center=self.center,\n                filename=\"maf\",\n                col=\"TUMOR_SAMPLE_BARCODE\",\n            )\n            total_error.write(error)\n\n        # only check end position as start position is required col\n        if (\n            process_functions.checkColExist(mutationDF, \"START_POSITION\")\n            and process_functions.checkColExist(mutationDF, \"END_POSITION\")\n            and set([\"START_POSITION\", \"END_POSITION\"]) &lt;= set(actual_numerical_cols)\n        ):\n            errors, warnings = validate.check_variant_start_and_end_positions(\n                input_df=mutationDF,\n                start_pos_col=\"START_POSITION\",\n                end_pos_col=\"END_POSITION\",\n                filename=\"maf\",\n            )\n            total_error.write(errors)\n            warning.write(warnings)\n\n        for allele_col in self._allele_cols:\n            if process_functions.checkColExist(mutationDF, allele_col):\n                invalid_indices = validate.get_invalid_allele_rows(\n                    mutationDF,\n                    allele_col,\n                    allowed_comb_alleles=self._allowed_comb_alleles,\n                    allowed_ind_alleles=self._allowed_ind_alleles,\n                    ignore_case=True,\n                    allow_na=False,\n                )\n                errors, warnings = validate.get_allele_validation_message(\n                    invalid_indices,\n                    invalid_col=allele_col,\n                    allowed_comb_alleles=self._allowed_comb_alleles,\n                    allowed_ind_alleles=self._allowed_ind_alleles,\n                    fileformat=self._fileType,\n                )\n                total_error.write(errors)\n                warning.write(warnings)\n\n        return total_error.getvalue(), warning.getvalue()\n\n    def _cross_validate(self, mutationDF: pd.DataFrame) -&gt; tuple:\n        \"\"\"This function cross-validates the mutation file to make sure it\n        adheres to the mutation SOP.\n\n        Args:\n            mutationDF (pd.DataFrame): mutation dataframe\n\n        Returns:\n            Text with all the errors in the mutation file\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n\n        # This section can be removed once we remove the list of lists\n        clinical_files = validate.parse_file_info_in_nested_list(\n            nested_list=self.ancillary_files, search_str=\"data_clinical_supp\"  # type: ignore[arg-type]\n        )\n        clinical_file_paths = clinical_files[\"file_info\"][\"path\"]\n\n        if clinical_files[\"files\"]:\n            try:\n                clinical_sample_df = process_functions.get_clinical_dataframe(\n                    filePathList=clinical_file_paths\n                )\n                has_file_read_error = False\n            except Exception:\n                has_file_read_error = True\n\n            if not has_file_read_error:\n                if process_functions.checkColExist(clinical_sample_df, \"SAMPLE_ID\"):\n                    errors, warnings = validate.check_values_between_two_df(\n                        df1=mutationDF,\n                        df1_filename=\"MAF\",\n                        df1_id_to_check=\"TUMOR_SAMPLE_BARCODE\",\n                        df2=clinical_sample_df,\n                        df2_filename=\"sample clinical\",\n                        df2_id_to_check=\"SAMPLE_ID\",\n                    )\n        return errors, warnings\n\n    def _get_dataframe(self, filePathList: List[str]) -&gt; pd.DataFrame:\n        \"\"\"Get mutation dataframe\n\n        1) Starts reading the first line in the file\n        2) Skips lines that starts with #\n        3) Reads in second line\n        4) Checks that first line fields matches second line. Must do this because\n        pandas.read_csv will allow for a file to have more column headers than content.\n        E.g)  A,B,C,D,E\n              1,2\n              2,3\n\n        5) We keep the 'NA', 'nan', and 'NaN' as strings in the data because\n        these are valid allele values\n        then convert the ones in the non-allele columns back to actual NAs\n\n        NOTE: Because allele columns are case-insensitive in maf data, we must\n        standardize the case of the columns when checking for the non-allele columns\n        to convert the NA strings to NAs\n\n        NOTE: This code allows empty dataframes to pass through\n        without errors\n\n        Args:\n            filePathList (List[str]): list of filepath(s)\n\n        Raises:\n            ValueError: First line fields doesn't match second line fields in file\n\n        Returns:\n            pd.DataFrame: mutation data\n        \"\"\"\n        with open(filePathList[0], \"r\") as maf_f:\n            firstline = maf_f.readline()\n            if firstline.startswith(\"#\"):\n                firstline = maf_f.readline()\n            secondline = maf_f.readline()\n\n        if len(firstline.split(\"\\t\")) != len(secondline.split(\"\\t\")):\n            raise ValueError(\n                \"Number of fields in a line do not match the \"\n                \"expected number of columns\"\n            )\n\n        read_csv_params = {\n            \"filepath_or_buffer\": filePathList[0],\n            \"sep\": \"\\t\",\n            \"comment\": \"#\",\n            \"keep_default_na\": False,\n            \"na_values\": [\n                \"-1.#IND\",\n                \"1.#QNAN\",\n                \"1.#IND\",\n                \"-1.#QNAN\",\n                \"#N/A N/A\",\n                \"#N/A\",\n                \"N/A\",\n                \"#NA\",\n                \"NULL\",\n                \"-NaN\",\n                \"-nan\",\n                \"\",\n            ],\n            # This is to check if people write files\n            # with R, quote=T\n            \"quoting\": 3,\n            # Retain completely blank lines so that\n            # validator will cause the file to fail\n            \"skip_blank_lines\": False,\n        }\n\n        mutationdf = transform._convert_df_with_mixed_dtypes(read_csv_params)\n\n        mutationdf = transform._convert_values_to_na(\n            input_df=mutationdf,\n            values_to_replace=[\"NA\", \"nan\", \"NaN\"],\n            columns_to_convert=[\n                col\n                for col in mutationdf.columns\n                if col.upper() not in self._allele_cols\n            ],\n        )\n        return mutationdf\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf.maf-functions","title":"Functions","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf.maf.process_steps","title":"<code>process_steps(df)</code>","text":"<p>The processing of maf files is specific to GENIE, so not included in this function</p> Source code in <code>genie_registry/maf.py</code> <pre><code>def process_steps(self, df):\n    \"\"\"The processing of maf files is specific to GENIE, so\n    not included in this function\"\"\"\n    logger.info(\n        \"Please run with `--process mutation` parameter \"\n        \"if you want to reannotate the mutation files\"\n    )\n    return None\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf-functions","title":"Functions","text":""},{"location":"reference/fileformats/maf/#genie_registry.maf._check_allele_col_validity","title":"<code>_check_allele_col_validity(df)</code>","text":"<p>This function checks specific columns in a MAF (Mutation Annotation Format) file for certain conditions.</p> The following conditions must be met <p>If the MAF file has all three of these columns</p> <pre><code>- TUMOR_SEQ_ALLELE1 (TSA1)\n- TUMOR_SEQ_ALLELE2 (TSA2)\n- REFERENCE_ALLELE (REF)\n</code></pre> <p>Then, one of the following must be true</p> <pre><code>- Every value in TSA1 must be the same as the value in REF\n- Every value in TSA1 must be the same as the value in TSA2\n</code></pre> <p>Additionally, if the MAF file has at least these two columns</p> <pre><code>- REFERENCE_ALLELE (REF)\n- TUMOR_SEQ_ALLELE2 (TSA2)\n</code></pre> <p>Then</p> <pre><code>NO values in REF can match TSA2\n</code></pre> <p>These rules are important because Genome Nexus (GN) uses <code>TSA1</code> to annotate data when it's not clear which variant to use. So, there can't be a mix of rows where some have <code>TSA1</code> equal to <code>REF</code> and some have <code>TSA1</code> equal to <code>TSA2</code>.</p> Valid Examples <pre><code>| REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n| ---------------- | ----------------- | ----------------- |\n| C                | C                 | A                 |\n| T                | T                 | C                 |\n</code></pre> <pre><code>| REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n| ---------------- | ----------------- | ----------------- |\n| C                | A                 | A                 |\n| T                | C                 | C                 |\n</code></pre> <pre><code>| REFERENCE_ALLELE | TUMOR_SEQ_ALLELE2 |\n| ---------------- | ----------------- |\n| C                | A                 |\n| T                | C                 |\n</code></pre> Invalid Examples <pre><code>| REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n| ---------------- | ----------------- | ----------------- |\n| C                | C                 | A                 |\n| C                | A                 | A                 |\n</code></pre> <pre><code>| REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n| ---------------- | ----------------- | ----------------- |\n| A                | C                 | A                 |\n| T                | C                 | T                 |\n</code></pre> <pre><code>| REFERENCE_ALLELE | TUMOR_SEQ_ALLELE2 |\n| ---------------- | ----------------- |\n| C                | C                 |\n| T                | C                 |\n</code></pre> <p>See this Genome Nexus issue for more background regarding why this validation rule was implemented.</p> PARAMETER DESCRIPTION <code>df</code> <p>input mutation dataframe</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>str</code> <p>the error message</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie_registry/maf.py</code> <pre><code>def _check_allele_col_validity(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    This function checks specific columns in a MAF (Mutation Annotation Format)\n    file for certain conditions.\n\n    The following conditions must be met:\n        **If the MAF file has all three of these columns**\n\n            - TUMOR_SEQ_ALLELE1 (TSA1)\n            - TUMOR_SEQ_ALLELE2 (TSA2)\n            - REFERENCE_ALLELE (REF)\n\n        **Then, one of the following must be true**\n\n            - Every value in TSA1 must be the same as the value in REF\n            - Every value in TSA1 must be the same as the value in TSA2\n\n        **Additionally, if the MAF file has at least these two columns**\n\n            - REFERENCE_ALLELE (REF)\n            - TUMOR_SEQ_ALLELE2 (TSA2)\n\n        **Then**\n\n            NO values in REF can match TSA2\n\n        These rules are important because Genome Nexus (GN) uses `TSA1` to annotate data\n        when it's not clear which variant to use. So, there can't be a mix of rows where\n        some have `TSA1` equal to `REF` and some have `TSA1` equal to `TSA2`.\n\n    Example: Valid Examples\n        ```\n        | REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n        | ---------------- | ----------------- | ----------------- |\n        | C                | C                 | A                 |\n        | T                | T                 | C                 |\n        ```\n\n        ```\n        | REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n        | ---------------- | ----------------- | ----------------- |\n        | C                | A                 | A                 |\n        | T                | C                 | C                 |\n        ```\n\n        ```\n        | REFERENCE_ALLELE | TUMOR_SEQ_ALLELE2 |\n        | ---------------- | ----------------- |\n        | C                | A                 |\n        | T                | C                 |\n        ```\n\n\n    Example: Invalid Examples\n        ```\n        | REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n        | ---------------- | ----------------- | ----------------- |\n        | C                | C                 | A                 |\n        | C                | A                 | A                 |\n        ```\n\n        ```\n        | REFERENCE_ALLELE | TUMOR_SEQ_ALLELE1 | TUMOR_SEQ_ALLELE2 |\n        | ---------------- | ----------------- | ----------------- |\n        | A                | C                 | A                 |\n        | T                | C                 | T                 |\n        ```\n\n        ```\n        | REFERENCE_ALLELE | TUMOR_SEQ_ALLELE2 |\n        | ---------------- | ----------------- |\n        | C                | C                 |\n        | T                | C                 |\n        ```\n\n\n    See this [Genome Nexus issue](https://github.com/genome-nexus/annotation-tools/issues/26) for\n    more background regarding why this validation rule was implemented.\n\n    Args:\n        df: input mutation dataframe\n\n    Returns:\n        str: the error message\n    \"\"\"\n    tsa2_col_exist = process_functions.checkColExist(df, \"TUMOR_SEQ_ALLELE2\")\n    tsa1_col_exist = process_functions.checkColExist(df, \"TUMOR_SEQ_ALLELE1\")\n    ref_col_exist = process_functions.checkColExist(df, \"REFERENCE_ALLELE\")\n    error = \"\"\n    if tsa2_col_exist and tsa1_col_exist and ref_col_exist:\n        tsa1_eq_ref = all(df[\"TUMOR_SEQ_ALLELE1\"] == df[\"REFERENCE_ALLELE\"])\n        tsa1_eq_tsa2 = all(df[\"TUMOR_SEQ_ALLELE1\"] == df[\"TUMOR_SEQ_ALLELE2\"])\n        if not (tsa1_eq_ref or tsa1_eq_tsa2):\n            error = (\n                \"maf: Contains both \"\n                \"TUMOR_SEQ_ALLELE1 and TUMOR_SEQ_ALLELE2 columns. \"\n                \"All values in TUMOR_SEQ_ALLELE1 must match all values in \"\n                \"REFERENCE_ALLELE or all values in TUMOR_SEQ_ALLELE2.\\n\"\n            )\n    if (\n        tsa2_col_exist\n        and ref_col_exist\n        and any(df[\"REFERENCE_ALLELE\"] == df[\"TUMOR_SEQ_ALLELE2\"])\n    ):\n        error = (\n            f\"{error}maf: Contains instances where values in REFERENCE_ALLELE match values in TUMOR_SEQ_ALLELE2. \"\n            \"This is invalid. Please correct.\\n\"\n        )\n        row_index = df[df[\"REFERENCE_ALLELE\"] == df[\"TUMOR_SEQ_ALLELE2\"]].index.values\n    return error\n</code></pre>"},{"location":"reference/fileformats/maf/#genie_registry.maf._check_allele_col","title":"<code>_check_allele_col(df, col)</code>","text":"<p>Check the Allele column is correctly formatted.</p> PARAMETER DESCRIPTION <code>df</code> <p>mutation dataframe</p> <p> </p> <code>col</code> <p>Column header name</p> <p> </p> RETURNS DESCRIPTION <p>error, warning</p> Source code in <code>genie_registry/maf.py</code> <pre><code>def _check_allele_col(df, col):\n    \"\"\"\n    Check the Allele column is correctly formatted.\n\n    Args:\n        df: mutation dataframe\n        col: Column header name\n\n    Returns:\n        error, warning\n\n    \"\"\"\n    col_exist = process_functions.checkColExist(df, col)\n    error = \"\"\n    warning = \"\"\n    if col_exist:\n        # CHECK: There can't be any null values\n        if sum(df[col].isnull()) &gt; 0:\n            error = f\"maf: {col} can't have any blank or null values.\\n\"\n\n    return error, warning\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/","title":"Mutations In Cis","text":""},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis","title":"<code>genie_registry.mutationsInCis</code>","text":""},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis-classes","title":"Classes","text":""},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/mutations_in_cis/#genie_registry.mutationsInCis.mutationsInCis","title":"<code>mutationsInCis</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/mutationsInCis.py</code> <pre><code>class mutationsInCis(FileTypeFormat):\n    _fileType = \"mutationsInCis\"\n\n    _validation_kwargs = []\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        Mutation In Cis is a csv file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, comment=\"#\")\n        return df\n\n    # VALIDATE FILENAME\n    def _validateFilename(self, filePath):\n        assert os.path.basename(filePath[0]) == \"mutationsInCis_filtered_samples.csv\"\n\n    # PROCESS\n    def process_steps(self, mutationInCis, newPath, databaseSynId):\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=databaseSynId,\n            newData=mutationInCis,\n            filterBy=self.center,\n            filterByColumn=\"Center\",\n            toDelete=True,\n        )\n        mutationInCis.to_csv(newPath, sep=\"\\t\", index=False)\n        return newPath\n\n    def _validate(self, mutationInCisDf):\n        mutationInCisSynId = self.genie_config[\"mutationsInCis\"]\n        # Pull down the correct database\n        existingMergeCheck = self.syn.tableQuery(\n            \"select * from {} where Center = '{}'\".format(\n                mutationInCisSynId, self.center\n            )\n        )\n        existingMergeCheckDf = existingMergeCheck.asDataFrame()\n\n        total_error = \"\"\n        warning = \"\"\n        required_headers = pd.Series(\n            [\n                \"Flag\",\n                \"Center\",\n                \"Tumor_Sample_Barcode\",\n                \"Hugo_Symbol\",\n                \"HGVSp_Short\",\n                \"Variant_Classification\",\n                \"Chromosome\",\n                \"Start_Position\",\n                \"Reference_Allele\",\n                \"Tumor_Seq_Allele2\",\n                \"t_alt_count_num\",\n                \"t_depth\",\n            ]\n        )\n        primaryKeys = [\n            \"Tumor_Sample_Barcode\",\n            \"HGVSp_Short\",\n            \"Start_Position\",\n            \"Reference_Allele\",\n            \"Tumor_Seq_Allele2\",\n        ]\n        if not all(required_headers.isin(mutationInCisDf.columns)):\n            missing_headers = required_headers[\n                ~required_headers.isin(mutationInCisDf.columns)\n            ]\n            total_error += (\n                \"Mutations In Cis Filter File: \"\n                \"Must at least have these headers: %s.\\n\" % \",\".join(missing_headers)\n            )\n        else:\n            new = mutationInCisDf[primaryKeys].fillna(\"\")\n            existing = existingMergeCheckDf[primaryKeys].fillna(\"\")\n\n            existing[\"primaryAll\"] = [\n                \" \".join(values.astype(str)) for i, values in existing.iterrows()\n            ]\n            new[\"primaryAll\"] = [\n                \" \".join(values.astype(str)) for i, values in new.iterrows()\n            ]\n            if not all(new.primaryAll.isin(existing.primaryAll)):\n                total_error += (\n                    \"Mutations In Cis Filter File: \"\n                    \"All variants must come from the original \"\n                    \"mutationInCis_filtered_samples.csv file in \"\n                    \"each institution's staging folder.\\n\"\n                )\n\n        if process_functions.checkColExist(mutationInCisDf, \"Tumor_Sample_Barcode\"):\n            error = process_functions.validate_genie_identifier(\n                identifiers=mutationInCisDf[\"Tumor_Sample_Barcode\"],\n                center=self.center,\n                filename=\"Mutations In Cis Filter File\",\n                col=\"TUMOR_SAMPLE_BARCODE\",\n            )\n            total_error += error\n\n        return total_error, warning\n</code></pre>"},{"location":"reference/fileformats/patient_retraction/","title":"Patient retraction","text":""},{"location":"reference/fileformats/patient_retraction/#genie_registry.patientRetraction","title":"<code>genie_registry.patientRetraction</code>","text":""},{"location":"reference/fileformats/patient_retraction/#genie_registry.patientRetraction-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/patient_retraction/#genie_registry.patientRetraction.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/patient_retraction/#genie_registry.patientRetraction-classes","title":"Classes","text":""},{"location":"reference/fileformats/patient_retraction/#genie_registry.patientRetraction.sampleRetraction","title":"<code>sampleRetraction</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/sampleRetraction.py</code> <pre><code>class sampleRetraction(FileTypeFormat):\n    _fileType = \"sampleRetraction\"\n\n    _process_kwargs = [\"newPath\", \"databaseSynId\", \"fileSynId\"]\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, header=None)\n        return df\n\n    def _validateFilename(self, filePath):\n        assert os.path.basename(filePath[0]) == \"%s.csv\" % self._fileType\n\n    def _process(self, deleteSamplesDf, modifiedOn):\n        col = (\n            \"genieSampleId\"\n            if self._fileType == \"sampleRetraction\"\n            else \"geniePatientId\"\n        )\n        deleteSamplesDf.rename(columns={0: col}, inplace=True)\n        modifiedOn = process_functions.to_unix_epoch_time_utc(\n            datetime.datetime.strptime(modifiedOn, \"%Y-%m-%dT%H:%M:%S\")\n        )\n        deleteSamplesDf[\"retractionDate\"] = modifiedOn\n        deleteSamplesDf[\"center\"] = self.center\n        return deleteSamplesDf\n\n    def process_steps(self, deleteSamples, fileSynId, databaseSynId, newPath):\n        info = self.syn.get(fileSynId, downloadFile=False)\n        deleteSamples = self._process(deleteSamples, info.modifiedOn.split(\".\")[0])\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=databaseSynId,\n            newData=deleteSamples,\n            filterBy=self.center,\n            filterByColumn=\"center\",\n            toDelete=True,\n        )\n        return newPath\n</code></pre>"},{"location":"reference/fileformats/patient_retraction/#genie_registry.patientRetraction.patientRetraction","title":"<code>patientRetraction</code>","text":"<p>               Bases: <code>sampleRetraction</code></p> Source code in <code>genie_registry/patientRetraction.py</code> <pre><code>class patientRetraction(sampleRetraction):\n    _fileType = \"patientRetraction\"\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/","title":"Sample retraction","text":""},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction","title":"<code>genie_registry.sampleRetraction</code>","text":""},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction-classes","title":"Classes","text":""},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/sample_retraction/#genie_registry.sampleRetraction.sampleRetraction","title":"<code>sampleRetraction</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/sampleRetraction.py</code> <pre><code>class sampleRetraction(FileTypeFormat):\n    _fileType = \"sampleRetraction\"\n\n    _process_kwargs = [\"newPath\", \"databaseSynId\", \"fileSynId\"]\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, header=None)\n        return df\n\n    def _validateFilename(self, filePath):\n        assert os.path.basename(filePath[0]) == \"%s.csv\" % self._fileType\n\n    def _process(self, deleteSamplesDf, modifiedOn):\n        col = (\n            \"genieSampleId\"\n            if self._fileType == \"sampleRetraction\"\n            else \"geniePatientId\"\n        )\n        deleteSamplesDf.rename(columns={0: col}, inplace=True)\n        modifiedOn = process_functions.to_unix_epoch_time_utc(\n            datetime.datetime.strptime(modifiedOn, \"%Y-%m-%dT%H:%M:%S\")\n        )\n        deleteSamplesDf[\"retractionDate\"] = modifiedOn\n        deleteSamplesDf[\"center\"] = self.center\n        return deleteSamplesDf\n\n    def process_steps(self, deleteSamples, fileSynId, databaseSynId, newPath):\n        info = self.syn.get(fileSynId, downloadFile=False)\n        deleteSamples = self._process(deleteSamples, info.modifiedOn.split(\".\")[0])\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=databaseSynId,\n            newData=deleteSamples,\n            filterBy=self.center,\n            filterByColumn=\"center\",\n            toDelete=True,\n        )\n        return newPath\n</code></pre>"},{"location":"reference/fileformats/seg/","title":"Segmented","text":""},{"location":"reference/fileformats/seg/#genie_registry.seg","title":"<code>genie_registry.seg</code>","text":""},{"location":"reference/fileformats/seg/#genie_registry.seg-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/seg/#genie_registry.seg.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/seg/#genie_registry.seg-classes","title":"Classes","text":""},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/seg/#genie_registry.seg.seg","title":"<code>seg</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/seg.py</code> <pre><code>class seg(FileTypeFormat):\n    _fileType = \"seg\"\n\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    def _validateFilename(self, filePath):\n        assert os.path.basename(filePath[0]) == \"genie_data_cna_hg19_%s.%s\" % (\n            self.center,\n            self._fileType,\n        )\n\n    def _process(self, seg):\n        seg.columns = [col.upper() for col in seg.columns]\n        seg = seg.drop_duplicates()\n        seg = seg.rename(\n            columns={\n                \"LOC.START\": \"LOCSTART\",\n                \"LOC.END\": \"LOCEND\",\n                \"SEG.MEAN\": \"SEGMEAN\",\n                \"NUM.MARK\": \"NUMMARK\",\n            }\n        )\n        seg[\"CHROM\"] = [str(chrom).replace(\"chr\", \"\") for chrom in seg[\"CHROM\"]]\n        seg[\"CENTER\"] = self.center\n        seg[\"LOCSTART\"] = seg[\"LOCSTART\"].astype(int)\n        seg[\"LOCEND\"] = seg[\"LOCEND\"].astype(int)\n        seg[\"NUMMARK\"] = seg[\"NUMMARK\"].astype(int)\n        return seg\n\n    def process_steps(self, seg, newPath, databaseSynId):\n        seg = self._process(seg)\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=databaseSynId,\n            newData=seg,\n            filterBy=self.center,\n            toDelete=True,\n        )\n        seg.to_csv(newPath, sep=\"\\t\", index=False)\n        return newPath\n\n    def _validate(self, segDF):\n        total_error = \"\"\n        warning = \"\"\n        segDF.columns = [col.upper() for col in segDF.columns]\n\n        REQUIRED_HEADERS = pd.Series(\n            [\"ID\", \"CHROM\", \"LOC.START\", \"LOC.END\", \"NUM.MARK\", \"SEG.MEAN\"]\n        )\n\n        if not all(REQUIRED_HEADERS.isin(segDF.columns)):\n            total_error += \"Your seg file is missing these headers: %s.\\n\" % \", \".join(\n                REQUIRED_HEADERS[~REQUIRED_HEADERS.isin(segDF.columns)]\n            )\n        else:\n            intCols = [\"LOC.START\", \"LOC.END\", \"NUM.MARK\"]\n            nonInts = [col for col in intCols if segDF[col].dtype != int]\n            if len(nonInts) &gt; 0:\n                total_error += (\n                    \"Seg: Only integars allowed in these column(s): %s.\\n\"\n                    % \", \".join(sorted(nonInts))\n                )\n            if segDF[\"SEG.MEAN\"].dtype not in [float, int]:\n                total_error += \"Seg: Only numerical values allowed in SEG.MEAN.\\n\"\n\n            error, warn = validate._validate_chromosome(\n                df=segDF, col=\"CHROM\", fileformat=\"Seg\"\n            )\n            total_error += error\n            warning += warn\n\n        checkNA = segDF.isna().apply(sum)\n        nullCols = [ind for ind in checkNA.index if checkNA[ind] &gt; 0]\n        if len(nullCols) &gt; 0:\n            total_error += (\n                \"Seg: No null or empty values allowed in column(s): %s.\\n\"\n                % \", \".join(sorted(nullCols))\n            )\n\n        if process_functions.checkColExist(segDF, \"ID\"):\n            error = process_functions.validate_genie_identifier(\n                identifiers=segDF[\"ID\"], center=self.center, filename=\"Seg\", col=\"ID\"\n            )\n            total_error += error\n\n        return (total_error, warning)\n</code></pre>"},{"location":"reference/fileformats/structural_variant/","title":"Structural variant","text":""},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant","title":"<code>genie_registry.structural_variant</code>","text":""},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant-classes","title":"Classes","text":""},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/structural_variant/#genie_registry.structural_variant.StructuralVariant","title":"<code>StructuralVariant</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/structural_variant.py</code> <pre><code>class StructuralVariant(FileTypeFormat):\n    _fileType = \"sv\"\n\n    # _validation_kwargs = [\"nosymbol_check\", \"project_id\"]\n\n    # VALIDATE FILENAME\n    def _validateFilename(self, filePath):\n        assert os.path.basename(filePath[0]) == \"data_sv.txt\"\n\n    def _process(self, sv_df: DataFrame) -&gt; DataFrame:\n        \"\"\"Transformation code for SV\n\n        Args:\n            sv_df (DataFrame): SV dataframe\n\n        Returns:\n            DataFrame: Transformed dataframe\n        \"\"\"\n        sv_df.columns = [col.upper() for col in sv_df.columns]\n        # Add center column\n        center = [sample_id.split(\"-\")[1] for sample_id in sv_df[\"SAMPLE_ID\"]]\n        sv_df[\"CENTER\"] = center\n        # Add in not required primary key columns so that the update code doesn't fail\n        not_required_cols = [\n            \"SITE1_HUGO_SYMBOL\",\n            \"SITE2_HUGO_SYMBOL\",\n            \"SITE1_POSITION\",\n            \"SITE2_POSITION\",\n            \"EVENT_INFO\",\n            \"ANNOTATION\",\n        ]\n        # Fill with blank columns\n        for col in not_required_cols:\n            if not process_functions.checkColExist(sv_df, col):\n                sv_df[col] = \"\"\n        return sv_df\n\n    def process_steps(self, sv_df: DataFrame, newPath: str, databaseSynId: str) -&gt; str:  # type: ignore[override]\n        sv_df = self._process(sv_df)\n        # TODO: test the col parameter\n        load.update_table(\n            syn=self.syn,\n            databaseSynId=databaseSynId,\n            newData=sv_df,\n            filterBy=self.center,\n            toDelete=True,\n            col=sv_df.columns.to_list(),\n        )\n        sv_df.to_csv(newPath, sep=\"\\t\", index=False)\n        return newPath\n\n    # def _validate(self, sv_df, nosymbol_check, project_id):\n    def _validate(self, sv_df):\n        total_error = StringIO()\n        total_warning = StringIO()\n        sv_df.columns = [col.upper() for col in sv_df.columns]\n\n        have_sample_col = process_functions.checkColExist(sv_df, \"SAMPLE_ID\")\n        if not have_sample_col:\n            total_error.write(\"Structural Variant: Must have SAMPLE_ID column.\\n\")\n        else:\n            # if sv_df[\"SAMPLE_ID\"].duplicated().any():\n            #     total_error.write(\n            #         \"Structural Variant: No duplicated SAMPLE_ID allowed.\\n\"\n            #     )\n            # TODO: switch to validate_genie_identifier function\n            # After GH-444 is merged\n            errors = process_functions.validate_genie_identifier(\n                identifiers=sv_df[\"SAMPLE_ID\"],\n                center=self.center,\n                filename=\"Structural Variant\",\n                col=\"SAMPLE_ID\",\n            )\n            total_error.write(errors)\n\n        if sv_df.duplicated().any():\n            total_error.write(\"Structural Variant: No duplicated rows allowed.\\n\")\n\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"SV_STATUS\",\n            possible_values=[\"SOMATIC\"],\n            filename=\"Structural Variant\",\n            required=True,\n        )\n        total_warning.write(warn)\n        total_error.write(error)\n\n        # have_hugo_1 = process_functions.checkColExist(sv_df, \"SITE1_HUGO_SYMBOL\")\n        # have_hugo_2 = process_functions.checkColExist(sv_df, \"SITE2_HUGO_SYMBOL\")\n        # have_entrez_1 = process_functions.checkColExist(sv_df, \"SITE1_ENTREZ_GENE_ID\")\n        # have_entrez_2 = process_functions.checkColExist(sv_df, \"SITE2_ENTREZ_GENE_ID\")\n\n        # if not ((have_hugo_1 or have_entrez_1) and (have_hugo_2 or have_entrez_2)):\n        #     total_error.write(\n        #         \"Structural Variant: Either SITE1_HUGO_SYMBOL/SITE1_ENTREZ_GENE_ID \"\n        #         \"or SITE2_HUGO_SYMBOL/SITE2_ENTREZ_GENE_ID is required.\\n\"\n        #     )\n\n        # optional_columns = [\n        #     \"SITE1_REGION_NUMBER\",\n        #     \"SITE2_REGION_NUMBER\",\n        #     \"SITE1_REGION\",\n        #     \"SITE2_REGION\",\n        #     \"SITE1_CHROMOSOME\",\n        #     \"SITE2_CHROMOSOME\",\n        #     \"SITE1_CONTIG\",\n        #     \"SITE2_CONTIG\",\n        #     \"SITE1_POSITION\",\n        #     \"SITE2_POSITION\",\n        #     \"SITE1_DESCRIPTION\",\n        #     \"SITE2_DESCRIPTION\",\n        #     \"SITE2_EFFECT_ON_FRAME\",\n        #     \"NCBI_BUILD\",\n        #     \"CLASS\",\n        #     \"TUMOR_SPLIT_READ_COUNT\",\n        #     \"TUMOR_PAIRED_END_READ_COUNT\",\n        #     \"EVENT_INFO\",\n        #     \"BREAKPOINT_TYPE\",\n        #     \"CONNECTION_TYPE\",\n        #     \"ANNOTATION\",\n        #     \"DNA_SUPPORT\",\n        #     \"RNA_SUPPORT\",\n        #     \"SV_LENGTH\",\n        #     \"NORMAL_READ_COUNT\",\n        #     \"TUMOR_READ_COUNT\",\n        #     \"NORMAL_VARIANT_COUNT\",\n        #     \"TUMOR_VARIANT_COUNT\",\n        #     \"NORMAL_PAIRED_END_READ_COUNT\",\n        #     \"NORMAL_SPLIT_READ_COUNT\",\n        #     \"COMMENTS\",\n        # ]\n        # Check for columns that should be integar columsn\n        int_cols = [\n            \"SITE1_ENTREZ_GENE_ID\",\n            \"SITE2_ENTREZ_GENE_ID\",\n            \"SITE1_REGION_NUMBER\",\n            \"SITE2_REGION_NUMBER\",\n            \"SITE1_POSITION\",\n            \"SITE2_POSITION\",\n            \"TUMOR_SPLIT_READ_COUNT\",\n            \"TUMOR_PAIRED_END_READ_COUNT\",\n            \"SV_LENGTH\",\n            \"NORMAL_READ_COUNT\",\n            \"TUMOR_READ_COUNT\",\n            \"NORMAL_VARIANT_COUNT\",\n            \"TUMOR_VARIANT_COUNT\",\n            \"NORMAL_PAIRED_END_READ_COUNT\",\n            \"NORMAL_SPLIT_READ_COUNT\",\n        ]\n        # Get all columns that are non integers.\n        # Allow for NA's\n        non_ints = [\n            col\n            for col in int_cols\n            if sv_df.get(col) is not None\n            and not sv_df[col].dropna().apply(process_functions.checkInt).all()\n        ]\n        if non_ints:\n            total_error.write(\n                \"Structural Variant: Only integers allowed in these \"\n                \"column(s): {}.\\n\".format(\", \".join(non_ints))\n            )\n\n        region_allow_vals = [\n            \"5_prime_UTR\",\n            \"3_prime_UTR\",\n            \"Promoter\",\n            \"Exon\",\n            \"Intron\",\n            \"5'UTR\",\n            \"3'UTR\",\n            \"5-UTR\",\n            \"3-UTR\",\n            \"5_Prime_UTR Intron\",\n            \"3_Prime_UTR Intron\",\n            \"Downstream\",\n            \"Upstream\",\n            \"Intergenic\",\n            \"IGR\",\n        ]\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"SITE1_REGION\",\n            possible_values=region_allow_vals,\n            filename=\"Structural Variant\",\n            na_allowed=True,\n            required=False,\n        )\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"SITE2_REGION\",\n            possible_values=region_allow_vals,\n            filename=\"Structural Variant\",\n            na_allowed=True,\n            required=False,\n        )\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"NCBI_BUILD\",\n            possible_values=[\"GRCh37\", \"GRCh38\"],\n            filename=\"Structural Variant\",\n            na_allowed=True,\n            required=False,\n        )\n        # total_warning.write(warn)\n        total_error.write(error)\n\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"BREAKPOINT_TYPE\",\n            possible_values=[\"PRECISE\", \"IMPRECISE\"],\n            filename=\"Structural Variant\",\n            na_allowed=True,\n            required=False,\n        )\n        # total_warning.write(warn)\n        total_error.write(error)\n\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"CONNECTION_TYPE\",\n            possible_values=[\"3to5\", \"5to3\", \"5to5\", \"3to3\"],\n            filename=\"Structural Variant\",\n            na_allowed=True,\n            required=False,\n        )\n        # total_warning.write(warn)\n        total_error.write(error)\n\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"DNA_SUPPORT\",\n            possible_values=[\"Yes\", \"No\", \"Unknown\"],\n            filename=\"Structural Variant\",\n            na_allowed=True,\n            required=False,\n        )\n        # total_warning.write(warn)\n        total_error.write(error)\n        warn, error = process_functions.check_col_and_values(\n            df=sv_df,\n            col=\"RNA_SUPPORT\",\n            possible_values=[\"Yes\", \"No\", \"Unknown\"],\n            filename=\"Structural Variant\",\n            na_allowed=True,\n            required=False,\n        )\n        # total_warning.write(warn)\n        total_error.write(error)\n        # check for chromosome columns and don't allow 'chr' for now\n        # since in the database there\u2019s nothing with CHR\n        chrom_cols = [\"SITE1_CHROMOSOME\", \"SITE2_CHROMOSOME\"]\n        for chrom_col in chrom_cols:\n            error, warn = validate._validate_chromosome(\n                df=sv_df,\n                col=chrom_col,\n                fileformat=\"Structural Variant\",\n                allow_chr=False,\n                allow_na=True,\n            )\n            total_error.write(error)\n\n        return total_error.getvalue(), total_warning.getvalue()\n</code></pre>"},{"location":"reference/fileformats/vcf/","title":"Vcf","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf","title":"<code>genie_registry.vcf</code>","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf-classes","title":"Classes","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.vcf","title":"<code>vcf</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/vcf.py</code> <pre><code>class vcf(FileTypeFormat):\n    _fileType = \"vcf\"\n\n    _process_kwargs = []\n    _allele_cols = [\"REF\"]\n    _allowed_comb_alleles = [\"A\", \"T\", \"C\", \"G\", \"N\"]\n    _allowed_ind_alleles = []\n\n    def _validateFilename(self, filePath):\n        basename = os.path.basename(filePath[0])\n        startswith_genie = basename.startswith(\"GENIE-{}-\".format(self.center))\n        endswith_vcf = basename.endswith(\".vcf\")\n        assert startswith_genie and endswith_vcf\n\n    def _get_dataframe(self, filePathList: List[str]) -&gt; pd.DataFrame:\n        \"\"\"Get mutation dataframe\n\n        1) Looks for the line in the file starting with #CHROM, that will be\n        the header line (columns).\n\n        2) When reading in the data, we keep the 'NA', 'nan', and 'NaN'\n        as strings in the data because these are valid allele values\n        then convert the ones in the non-allele columns back to actual NAs\n\n        Args:\n            filePathList (List[str]): list of filepath(s)\n\n        Raises:\n            ValueError: when line with #CHROM doesn't exist in file\n\n        Returns:\n            pd.DataFrame: mutation data\n        \"\"\"\n        headers = None\n        filepath = filePathList[0]\n        with open(filepath, \"r\") as vcffile:\n            for row in vcffile:\n                if row.startswith(\"#CHROM\"):\n                    headers = row.replace(\"\\n\", \"\").replace(\"\\r\", \"\").split(\"\\t\")\n                    break\n        if headers is not None:\n            vcfdf = pd.read_csv(\n                filepath,\n                sep=\"\\t\",\n                comment=\"#\",\n                header=None,\n                names=headers,\n                keep_default_na=False,\n                na_values=[\n                    \"-1.#IND\",\n                    \"1.#QNAN\",\n                    \"1.#IND\",\n                    \"-1.#QNAN\",\n                    \"#N/A N/A\",\n                    \"#N/A\",\n                    \"N/A\",\n                    \"#NA\",\n                    \"NULL\",\n                    \"-NaN\",\n                    \"-nan\",\n                    \"\",\n                ],\n            )\n        else:\n            raise ValueError(\"Your vcf must start with the header #CHROM\")\n\n        vcfdf = transform._convert_values_to_na(\n            input_df=vcfdf,\n            values_to_replace=[\"NA\", \"nan\", \"NaN\"],\n            columns_to_convert=[\n                col for col in vcfdf.columns if col not in self._allele_cols\n            ],\n        )\n        return vcfdf\n\n    def process_steps(self, df):\n        \"\"\"The processing of vcf files is specific to GENIE, so\n        not included in this function\"\"\"\n        logger.info(\n            \"Please run with `--process mutation` parameter \"\n            \"if you want to reannotate the mutation files\"\n        )\n        return None\n\n    def _validate(self, vcfdf):\n        \"\"\"\n        Validates the content of a vcf file\n\n        Args:\n            vcfdf: pandas dataframe containing vcf content\n\n        Returns:\n            total_error - error messages\n            warning - warning messages\n        \"\"\"\n        required_headers = pd.Series(\n            [\"#CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\"]\n        )\n        total_error = \"\"\n        warning = \"\"\n        if not all(required_headers.isin(vcfdf.columns)):\n            total_error += (\n                \"vcf: Must have these headers: \"\n                \"CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO.\\n\"\n            )\n        else:\n            # No duplicated values\n            primary_cols = [\"#CHROM\", \"POS\", \"REF\", \"ALT\"]\n            if vcfdf.duplicated(primary_cols).any():\n                total_error += \"vcf: Must not have duplicate variants.\\n\"\n\n            if vcfdf[[\"#CHROM\", \"POS\"]].isnull().values.any():\n                total_error += (\n                    \"vcf: May contain rows that are \"\n                    \"space delimited instead of tab delimited.\\n\"\n                )\n        # Vcf can only have max of 11 columns\n        if len(vcfdf.columns) &gt; 11:\n            total_error += (\n                \"vcf: Should not have more than 11 columns. Only \"\n                \"single sample or matched tumor normal vcf files are accepted.\\n\"\n            )\n        elif len(vcfdf.columns) &gt; 8:\n            # If there are greater than 8 columns, there must be the FORMAT column\n            if \"FORMAT\" not in vcfdf.columns:\n                total_error += \"vcf: Must have FORMAT header if sample columns exist.\\n\"\n            # If 11 columns, this is assumed to be a tumor normal vcf\n            if len(vcfdf.columns) == 11:\n                sample_id = vcfdf.columns[-2]\n                normal_id = vcfdf.columns[-1]\n                error = process_functions.validate_genie_identifier(\n                    identifiers=pd.Series([sample_id]),\n                    center=self.center,\n                    filename=\"vcf\",\n                    col=\"tumor sample column\",\n                )\n                total_error += error\n                error = process_functions.validate_genie_identifier(\n                    identifiers=pd.Series([normal_id]),\n                    center=self.center,\n                    filename=\"vcf\",\n                    col=\"normal sample column\",\n                )\n                total_error += error\n            else:\n                # Everything else above 8 columns that isn't 11 columns\n                # will be assumed to be a single sample vcf.\n                # if TUMOR is not the sample column header, then validate\n                # the sample column header.\n                if \"TUMOR\" not in vcfdf.columns:\n                    sample_id = vcfdf.columns[-1]\n                    error = process_functions.validate_genie_identifier(\n                        identifiers=pd.Series([sample_id]),\n                        center=self.center,\n                        filename=\"vcf\",\n                        col=\"tumor sample column\",\n                    )\n                    if error:\n                        error = error.replace(\"\\n\", \"\")\n                        error += \" if vcf represents a single sample and TUMOR is not the sample column header.\\n\"\n                        total_error += error\n\n        # Require that they report variants mapped to\n        # either GRCh37 or hg19 without\n        # the chr-prefix.\n        error, warn = validate._validate_chromosome(\n            df=vcfdf, col=\"#CHROM\", fileformat=\"vcf\"\n        )\n        total_error += error\n        warning += warn\n\n        for allele_col in self._allele_cols:\n            if process_functions.checkColExist(vcfdf, allele_col):\n                invalid_indices = validate.get_invalid_allele_rows(\n                    vcfdf,\n                    input_col=allele_col,\n                    allowed_comb_alleles=self._allowed_comb_alleles,\n                    allowed_ind_alleles=self._allowed_ind_alleles,\n                    ignore_case=True,\n                    allow_na=False,\n                )\n                errors, warnings = validate.get_allele_validation_message(\n                    invalid_indices,\n                    invalid_col=allele_col,\n                    allowed_comb_alleles=self._allowed_comb_alleles,\n                    allowed_ind_alleles=self._allowed_ind_alleles,\n                    fileformat=self._fileType,\n                )\n                total_error += errors\n                warning += warnings\n\n        # No white spaces\n        white_space = vcfdf.apply(lambda x: contains_whitespace(x), axis=1)\n        if sum(white_space) &gt; 0:\n            warning += \"vcf: Should not have any white spaces in any of the columns.\\n\"\n\n        # I can also recommend a `bcftools query` command that\n        # will parse a VCF in a detailed way,\n        # and output with warnings or errors if the format is not adhered too\n        return total_error, warning\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf.vcf-functions","title":"Functions","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf.vcf.process_steps","title":"<code>process_steps(df)</code>","text":"<p>The processing of vcf files is specific to GENIE, so not included in this function</p> Source code in <code>genie_registry/vcf.py</code> <pre><code>def process_steps(self, df):\n    \"\"\"The processing of vcf files is specific to GENIE, so\n    not included in this function\"\"\"\n    logger.info(\n        \"Please run with `--process mutation` parameter \"\n        \"if you want to reannotate the mutation files\"\n    )\n    return None\n</code></pre>"},{"location":"reference/fileformats/vcf/#genie_registry.vcf-functions","title":"Functions","text":""},{"location":"reference/fileformats/vcf/#genie_registry.vcf.contains_whitespace","title":"<code>contains_whitespace(row)</code>","text":"<p>Gets the total number of whitespaces from each column of a row</p> Source code in <code>genie_registry/vcf.py</code> <pre><code>def contains_whitespace(row):\n    \"\"\"Gets the total number of whitespaces from each column of a row\"\"\"\n    return sum([\" \" in i for i in row if isinstance(i, str)])\n</code></pre>"},{"location":"reference/fileformats/workflow/","title":"Workflow","text":""},{"location":"reference/fileformats/workflow/#genie_registry.workflow","title":"<code>genie_registry.workflow</code>","text":""},{"location":"reference/fileformats/workflow/#genie_registry.workflow-attributes","title":"Attributes","text":""},{"location":"reference/fileformats/workflow/#genie_registry.workflow.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fileformats/workflow/#genie_registry.workflow-classes","title":"Classes","text":""},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat","title":"<code>FileTypeFormat</code>","text":"Source code in <code>genie/example_filetype_format.py</code> <pre><code>class FileTypeFormat(metaclass=ABCMeta):\n    _process_kwargs = [\"newPath\", \"databaseSynId\"]\n\n    _fileType = \"fileType\"\n\n    _validation_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        center: str,\n        genie_config: Optional[dict] = None,\n        ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn (synapseclient.Synapse): a synapseclient.Synapse object\n            center (str): The participating center name.\n            genie_config (dict): The configurations needed for the GENIE codebase.\n                GENIE table type/name to Synapse Id. Defaults to None.\n            ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n        \"\"\"\n        self.syn = syn\n        self.center = center\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n        # self.pool = multiprocessing.Pool(poolSize)\n\n    def _get_dataframe(self, filePathList):\n        \"\"\"\n        This function by defaults assumes the filePathList is length of 1\n        and is a tsv file.  Could change depending on file type.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        filePath = filePathList[0]\n        df = pd.read_csv(filePath, sep=\"\\t\", comment=\"#\")\n        return df\n\n    def read_file(self, filePathList):\n        \"\"\"\n        Each file is to be read in for validation and processing.\n        This is not to be changed in any functions.\n\n        Args:\n            filePathList:  A list of file paths (Max is 2 for the two\n                           clinical files)\n\n        Returns:\n            df: Pandas dataframe of file\n        \"\"\"\n        df = self._get_dataframe(filePathList)\n        return df\n\n    def _validateFilename(self, filePath):\n        \"\"\"\n        Function that changes per file type for validating its filename\n        Expects an assertion error.\n\n        Args:\n            filePath: Path to file\n        \"\"\"\n        # assert True\n        raise NotImplementedError\n\n    def validateFilename(self, filePath):\n        \"\"\"\n        Validation of file name.  The filename is what maps the file\n        to its validation and processing.\n\n        Args:\n            filePath: Path to file\n\n        Returns:\n            str: file type defined by self._fileType\n        \"\"\"\n        self._validateFilename(filePath)\n        return self._fileType\n\n    def process_steps(self, df, **kwargs):\n        \"\"\"\n        This function is modified for every single file.\n        It reformats the file and stores the file into database and Synapse.\n        \"\"\"\n        pass\n\n    def preprocess(self, newpath):\n        \"\"\"\n        This is for any preprocessing that has to occur to the entity name\n        to add to kwargs for processing.  entity name is included in\n        the new path\n\n        Args:\n            newpath: Path to file\n        \"\"\"\n        return dict()\n\n    def process(self, filePath, **kwargs):\n        \"\"\"\n        This is the main processing function.\n\n        Args:\n            filePath: Path to file\n            kwargs: The kwargs are determined by self._process_kwargs\n\n        Returns:\n            str: file path of processed file\n        \"\"\"\n        preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n        kwargs.update(preprocess_args)\n        mykwargs = {}\n        for required_parameter in self._process_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n        logger.info(\"PROCESSING %s\" % filePath)\n        # This is done because the clinical files are being merged into a list\n        if self._fileType == \"clinical\":\n            path_or_df = self.read_file(filePath)\n        # If file type is vcf or maf file, processing requires a filepath\n        elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n            path_or_df = self.read_file([filePath])\n        else:\n            path_or_df = filePath\n        path = self.process_steps(path_or_df, **mykwargs)\n        return path\n\n    def _validate(self, df: pd.DataFrame, **kwargs) -&gt; tuple:\n        \"\"\"\n        This is the base validation function.\n        By default, no validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def _cross_validate(self, df: pd.DataFrame) -&gt; tuple:\n        \"\"\"\n        This is the base cross-validation function.\n        By default, no cross-validation occurs.\n\n        Args:\n            df (pd.DataFrame): A dataframe of the file\n\n        Returns:\n            tuple: The errors and warnings as a file from cross-validation.\n                   Defaults to blank strings\n        \"\"\"\n        errors = \"\"\n        warnings = \"\"\n        logger.info(\"NO CROSS-VALIDATION for %s files\" % self._fileType)\n        return errors, warnings\n\n    def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n        \"\"\"\n        This is the main validation function.\n        Every file type calls self._validate, which is different.\n\n        Args:\n            filePathList: A list of file paths.\n            kwargs: The kwargs are determined by self._validation_kwargs\n\n        Returns:\n            tuple: The errors and warnings as a file from validation.\n        \"\"\"\n        mykwargs = {}\n        for required_parameter in self._validation_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n\n        errors = \"\"\n\n        try:\n            df = self.read_file(filePathList)\n        except Exception as e:\n            errors = (\n                f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n            )\n            warnings = \"\"\n\n        if not errors:\n            logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n            errors, warnings = self._validate(df, **mykwargs)\n            # only cross-validate if validation passes or ancillary files exist\n            # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n            # extract.get_center_input_files\n            if not errors and (\n                isinstance(self.ancillary_files, list) and self.ancillary_files\n            ):\n                logger.info(\n                    \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n                )\n                errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                    df\n                )\n                errors += errors_cross_validate\n                warnings += warnings_cross_validate\n\n        result_cls = ValidationResults(errors=errors, warnings=warnings)\n        return result_cls\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat-functions","title":"Functions","text":""},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat.__init__","title":"<code>__init__(syn, center, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>genie_config</code> <p>The configurations needed for the GENIE codebase. GENIE table type/name to Synapse Id. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation. Defaults to None.</p> <p> TYPE: <code>List[List[Entity]]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    center: str,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[List[List[synapseclient.Entity]]] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn (synapseclient.Synapse): a synapseclient.Synapse object\n        center (str): The participating center name.\n        genie_config (dict): The configurations needed for the GENIE codebase.\n            GENIE table type/name to Synapse Id. Defaults to None.\n        ancillary_files (List[List[synapseclient.Entity]]): all files downloaded for validation. Defaults to None.\n    \"\"\"\n    self.syn = syn\n    self.center = center\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat.read_file","title":"<code>read_file(filePathList)</code>","text":"<p>Each file is to be read in for validation and processing. This is not to be changed in any functions.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths (Max is 2 for the two            clinical files)</p> <p> </p> RETURNS DESCRIPTION <code>df</code> <p>Pandas dataframe of file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def read_file(self, filePathList):\n    \"\"\"\n    Each file is to be read in for validation and processing.\n    This is not to be changed in any functions.\n\n    Args:\n        filePathList:  A list of file paths (Max is 2 for the two\n                       clinical files)\n\n    Returns:\n        df: Pandas dataframe of file\n    \"\"\"\n    df = self._get_dataframe(filePathList)\n    return df\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat.validateFilename","title":"<code>validateFilename(filePath)</code>","text":"<p>Validation of file name.  The filename is what maps the file to its validation and processing.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>file type defined by self._fileType</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validateFilename(self, filePath):\n    \"\"\"\n    Validation of file name.  The filename is what maps the file\n    to its validation and processing.\n\n    Args:\n        filePath: Path to file\n\n    Returns:\n        str: file type defined by self._fileType\n    \"\"\"\n    self._validateFilename(filePath)\n    return self._fileType\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat.process_steps","title":"<code>process_steps(df, **kwargs)</code>","text":"<p>This function is modified for every single file. It reformats the file and stores the file into database and Synapse.</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process_steps(self, df, **kwargs):\n    \"\"\"\n    This function is modified for every single file.\n    It reformats the file and stores the file into database and Synapse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat.preprocess","title":"<code>preprocess(newpath)</code>","text":"<p>This is for any preprocessing that has to occur to the entity name to add to kwargs for processing.  entity name is included in the new path</p> PARAMETER DESCRIPTION <code>newpath</code> <p>Path to file</p> <p> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def preprocess(self, newpath):\n    \"\"\"\n    This is for any preprocessing that has to occur to the entity name\n    to add to kwargs for processing.  entity name is included in\n    the new path\n\n    Args:\n        newpath: Path to file\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat.process","title":"<code>process(filePath, **kwargs)</code>","text":"<p>This is the main processing function.</p> PARAMETER DESCRIPTION <code>filePath</code> <p>Path to file</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._process_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>file path of processed file</p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def process(self, filePath, **kwargs):\n    \"\"\"\n    This is the main processing function.\n\n    Args:\n        filePath: Path to file\n        kwargs: The kwargs are determined by self._process_kwargs\n\n    Returns:\n        str: file path of processed file\n    \"\"\"\n    preprocess_args = self.preprocess(kwargs.get(\"newPath\"))\n    kwargs.update(preprocess_args)\n    mykwargs = {}\n    for required_parameter in self._process_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n    logger.info(\"PROCESSING %s\" % filePath)\n    # This is done because the clinical files are being merged into a list\n    if self._fileType == \"clinical\":\n        path_or_df = self.read_file(filePath)\n    # If file type is vcf or maf file, processing requires a filepath\n    elif self._fileType not in [\"vcf\", \"maf\", \"mafSP\", \"md\"]:\n        path_or_df = self.read_file([filePath])\n    else:\n        path_or_df = filePath\n    path = self.process_steps(path_or_df, **mykwargs)\n    return path\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.FileTypeFormat.validate","title":"<code>validate(filePathList, **kwargs)</code>","text":"<p>This is the main validation function. Every file type calls self._validate, which is different.</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>A list of file paths.</p> <p> </p> <code>kwargs</code> <p>The kwargs are determined by self._validation_kwargs</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from validation.</p> <p> TYPE: <code>ValidationResults</code> </p> Source code in <code>genie/example_filetype_format.py</code> <pre><code>def validate(self, filePathList, **kwargs) -&gt; ValidationResults:\n    \"\"\"\n    This is the main validation function.\n    Every file type calls self._validate, which is different.\n\n    Args:\n        filePathList: A list of file paths.\n        kwargs: The kwargs are determined by self._validation_kwargs\n\n    Returns:\n        tuple: The errors and warnings as a file from validation.\n    \"\"\"\n    mykwargs = {}\n    for required_parameter in self._validation_kwargs:\n        assert required_parameter in kwargs.keys(), (\n            \"%s not in parameter list\" % required_parameter\n        )\n        mykwargs[required_parameter] = kwargs[required_parameter]\n\n    errors = \"\"\n\n    try:\n        df = self.read_file(filePathList)\n    except Exception as e:\n        errors = (\n            f\"The file(s) ({filePathList}) cannot be read. Original error: {str(e)}\"\n        )\n        warnings = \"\"\n\n    if not errors:\n        logger.info(\"VALIDATING %s\" % os.path.basename(\",\".join(filePathList)))\n        errors, warnings = self._validate(df, **mykwargs)\n        # only cross-validate if validation passes or ancillary files exist\n        # Assumes that self.ancillary_files won't be [[]] due to whats returned from\n        # extract.get_center_input_files\n        if not errors and (\n            isinstance(self.ancillary_files, list) and self.ancillary_files\n        ):\n            logger.info(\n                \"CROSS-VALIDATING %s\" % os.path.basename(\",\".join(filePathList))\n            )\n            errors_cross_validate, warnings_cross_validate = self._cross_validate(\n                df\n            )\n            errors += errors_cross_validate\n            warnings += warnings_cross_validate\n\n    result_cls = ValidationResults(errors=errors, warnings=warnings)\n    return result_cls\n</code></pre>"},{"location":"reference/fileformats/workflow/#genie_registry.workflow.workflow","title":"<code>workflow</code>","text":"<p>               Bases: <code>FileTypeFormat</code></p> Source code in <code>genie_registry/workflow.py</code> <pre><code>class workflow(FileTypeFormat):\n    _fileType = \"md\"\n\n    _process_kwargs = [\"databaseSynId\"]\n\n    def _validateFilename(self, filePath):\n        assert os.path.basename(filePath[0]).startswith(\n            self.center\n        ) and os.path.basename(filePath[0]).endswith(\".md\")\n\n    def process_steps(self, filePath, databaseSynId):\n        self.syn.store(synapseclient.File(filePath, parent=databaseSynId))\n        return filePath\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/","title":"create_case_lists","text":""},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists","title":"<code>genie.create_case_lists</code>","text":"<p>Creates case lists per cancer type</p>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists-attributes","title":"Attributes","text":""},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.CASE_LIST_TEXT_TEMPLATE","title":"<code>CASE_LIST_TEXT_TEMPLATE = 'cancer_study_identifier: {study_id}\\nstable_id: {stable_id}\\ncase_list_name: {case_list_name}\\ncase_list_description: {case_list_description}\\ncase_list_ids: {case_list_ids}'</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists-functions","title":"Functions","text":""},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.create_case_lists_map","title":"<code>create_case_lists_map(clinical_file_name)</code>","text":"<p>Creates the case list dictionary</p> PARAMETER DESCRIPTION <code>clinical_file_name</code> <p>clinical file path</p> <p> </p> RETURNS DESCRIPTION <code>dict</code> <p>key = cancer_type   value = list of sample ids</p> <code>dict</code> <p>key = seq_assay_id   value = list of sample ids</p> <code>list</code> <p>Clinical samples</p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def create_case_lists_map(clinical_file_name):\n    \"\"\"\n    Creates the case list dictionary\n\n    Args:\n        clinical_file_name: clinical file path\n\n    Returns:\n        dict: key = cancer_type\n              value = list of sample ids\n        dict: key = seq_assay_id\n              value = list of sample ids\n        list: Clinical samples\n    \"\"\"\n    with open(clinical_file_name, \"rU\") as clinical_file:\n        seq_assay_map = defaultdict(list)\n        clinical_file_map = defaultdict(list)\n        clin_samples = []\n        reader = csv.DictReader(clinical_file, dialect=\"excel-tab\")\n        for row in reader:\n            clinical_file_map[row[\"CANCER_TYPE\"]].append(row[\"SAMPLE_ID\"])\n            seq_assay_map[row[\"SEQ_ASSAY_ID\"]].append(row[\"SAMPLE_ID\"])\n            clin_samples.append(row[\"SAMPLE_ID\"])\n    return clinical_file_map, seq_assay_map, clin_samples\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists._write_single_oncotree_case_list","title":"<code>_write_single_oncotree_case_list(cancer_type, ids, study_id, output_directory)</code>","text":"<p>Writes one oncotree case list. Python verisons below 3.6 will sort the dictionary keys which causes tests to fail</p> PARAMETER DESCRIPTION <code>cancer_type</code> <p>Oncotree code cancer type</p> <p> </p> <code>ids</code> <p>GENIE sample ids</p> <p> </p> <code>study_id</code> <p>cBioPortal study id</p> <p> </p> <code>output_directory</code> <p>case list output directory</p> <p> </p> RETURNS DESCRIPTION <p>case list file path</p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def _write_single_oncotree_case_list(cancer_type, ids, study_id, output_directory):\n    \"\"\"\n    Writes one oncotree case list. Python verisons below\n    3.6 will sort the dictionary keys which causes tests to fail\n\n    Args:\n        cancer_type: Oncotree code cancer type\n        ids: GENIE sample ids\n        study_id: cBioPortal study id\n        output_directory: case list output directory\n\n    Returns:\n        case list file path\n    \"\"\"\n    cancer_type = \"NA\" if cancer_type == \"\" else cancer_type\n    cancer_type_no_spaces = (\n        cancer_type.replace(\" \", \"_\").replace(\",\", \"\").replace(\"/\", \"_\")\n    )\n    cancer_type_no_spaces = (\n        \"no_oncotree_code\" if cancer_type_no_spaces == \"NA\" else cancer_type_no_spaces\n    )\n    case_list_text = CASE_LIST_TEXT_TEMPLATE.format(\n        study_id=study_id,\n        stable_id=study_id + \"_\" + cancer_type_no_spaces,\n        case_list_name=\"Tumor Type: \" + cancer_type,\n        case_list_description=\"All tumors with cancer type \" + cancer_type,\n        case_list_ids=\"\\t\".join(ids),\n    )\n    case_list_path = os.path.abspath(\n        os.path.join(output_directory, \"cases_\" + cancer_type_no_spaces + \".txt\")\n    )\n    with open(case_list_path, \"w\") as case_list_file:\n        case_list_file.write(case_list_text)\n    return case_list_path\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.write_case_list_files","title":"<code>write_case_list_files(clinical_file_map, output_directory, study_id)</code>","text":"<p>Writes the cancer_type case list file to case_lists directory</p> PARAMETER DESCRIPTION <code>clinical_file_map</code> <p>cancer type to sample id mapping from                create_case_lists_map</p> <p> </p> <code>output_directory</code> <p>Directory to write case lists</p> <p> </p> <code>study_id</code> <p>cBioPortal study id</p> <p> </p> RETURNS DESCRIPTION <code>list</code> <p>oncotree code case list files</p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def write_case_list_files(clinical_file_map, output_directory, study_id):\n    \"\"\"\n    Writes the cancer_type case list file to case_lists directory\n\n    Args:\n        clinical_file_map: cancer type to sample id mapping from\n                           create_case_lists_map\n        output_directory: Directory to write case lists\n        study_id: cBioPortal study id\n\n    Returns:\n        list: oncotree code case list files\n    \"\"\"\n    case_list_files = []\n    for cancer_type, ids in clinical_file_map.items():\n        case_list_path = _write_single_oncotree_case_list(\n            cancer_type, ids, study_id, output_directory\n        )\n        case_list_files.append(case_list_path)\n    return case_list_files\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.create_sequenced_samples","title":"<code>create_sequenced_samples(seq_assay_map, assay_info_file_name)</code>","text":"<p>Gets samples sequenced</p> PARAMETER DESCRIPTION <code>seq_assay_map</code> <p>dictionary containing lists of samples per seq_assay_id</p> <p> </p> <code>assay_info_file_name</code> <p>Assay information name</p> <p> </p> RETURNS DESCRIPTION <p>lists of cna and sv samples</p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def create_sequenced_samples(seq_assay_map, assay_info_file_name):\n    \"\"\"\n    Gets samples sequenced\n\n    Args:\n        seq_assay_map: dictionary containing lists of samples per seq_assay_id\n        assay_info_file_name: Assay information name\n\n    Returns:\n        lists of cna and sv samples\n    \"\"\"\n    with open(assay_info_file_name, \"r\") as assay_info_file:\n        reader = csv.DictReader(assay_info_file, dialect=\"excel-tab\")\n        cna_samples = []\n        # TODO: Remove when depreciating fusion files\n        fusion_samples = []\n        sv_samples = []\n        for row in reader:\n            if \"cna\" in row[\"alteration_types\"]:\n                cna_samples.extend(seq_assay_map[row[\"SEQ_ASSAY_ID\"]])\n            if \"structural_variants\" in row[\"alteration_types\"]:\n                fusion_samples.extend(seq_assay_map[row[\"SEQ_ASSAY_ID\"]])\n                sv_samples.extend(seq_assay_map[row[\"SEQ_ASSAY_ID\"]])\n    return cna_samples, fusion_samples, sv_samples\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.write_case_list_sequenced","title":"<code>write_case_list_sequenced(clinical_samples, output_directory, study_id)</code>","text":"<p>Writes the genie sequenced and all samples. Since all samples are sequenced, _all and _sequenced are equal</p> PARAMETER DESCRIPTION <code>clinical_samples</code> <p>List of clinical samples</p> <p> </p> <code>output_directory</code> <p>Directory to write case lists</p> <p> </p> <code>study_id</code> <p>cBioPortal study id</p> <p> </p> RETURNS DESCRIPTION <code>list</code> <p>case list sequenced and all</p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def write_case_list_sequenced(clinical_samples, output_directory, study_id):\n    \"\"\"\n    Writes the genie sequenced and all samples. Since all samples\n    are sequenced, _all and _sequenced are equal\n\n    Args:\n        clinical_samples: List of clinical samples\n        output_directory: Directory to write case lists\n        study_id: cBioPortal study id\n\n    Returns:\n        list: case list sequenced and all\n    \"\"\"\n    caselist_files = []\n    case_list_ids = \"\\t\".join(clinical_samples)\n    case_sequenced_path = os.path.abspath(\n        os.path.join(output_directory, \"cases_sequenced.txt\")\n    )\n    with open(case_sequenced_path, \"w\") as case_list_sequenced_file:\n        case_list_file_text = CASE_LIST_TEXT_TEMPLATE.format(\n            study_id=study_id,\n            stable_id=study_id + \"_sequenced\",\n            case_list_name=\"Sequenced Tumors\",\n            case_list_description=\"All sequenced samples\",\n            case_list_ids=case_list_ids,\n        )\n        case_list_sequenced_file.write(case_list_file_text)\n    cases_all_path = os.path.abspath(os.path.join(output_directory, \"cases_all.txt\"))\n    with open(cases_all_path, \"w\") as case_list_all_file:\n        case_list_file_text = CASE_LIST_TEXT_TEMPLATE.format(\n            study_id=study_id,\n            stable_id=study_id + \"_all\",\n            case_list_name=\"All samples\",\n            case_list_description=\"All samples\",\n            case_list_ids=case_list_ids,\n        )\n        case_list_all_file.write(case_list_file_text)\n    caselist_files.extend([case_sequenced_path, cases_all_path])\n    return caselist_files\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.write_case_list_cna","title":"<code>write_case_list_cna(cna_samples, output_directory, study_id)</code>","text":"<p>Writes the cna sequenced samples</p> PARAMETER DESCRIPTION <code>cna_samples</code> <p>List of cna samples</p> <p> </p> <code>output_directory</code> <p>Directory to write case lists</p> <p> </p> <code>study_id</code> <p>cBioPortal study id</p> <p> </p> RETURNS DESCRIPTION <p>cna caselist path</p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def write_case_list_cna(cna_samples, output_directory, study_id):\n    \"\"\"\n    Writes the cna sequenced samples\n\n    Args:\n        cna_samples: List of cna samples\n        output_directory: Directory to write case lists\n        study_id: cBioPortal study id\n\n    Returns:\n        cna caselist path\n    \"\"\"\n    case_list_ids = \"\\t\".join(cna_samples)\n    cna_caselist_path = os.path.abspath(os.path.join(output_directory, \"cases_cna.txt\"))\n    with open(cna_caselist_path, \"w\") as case_list_file:\n        case_list_file_text = CASE_LIST_TEXT_TEMPLATE.format(\n            study_id=study_id,\n            stable_id=study_id + \"_cna\",\n            case_list_name=\"Samples with CNA\",\n            case_list_description=\"Samples with CNA\",\n            case_list_ids=case_list_ids,\n        )\n        case_list_file.write(case_list_file_text)\n    return cna_caselist_path\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.write_case_list_sv","title":"<code>write_case_list_sv(samples, output_directory, study_id)</code>","text":"<p>Writes the structural variant (sv) sequenced samples</p> PARAMETER DESCRIPTION <code>samples</code> <p>List of sv samples</p> <p> TYPE: <code>list</code> </p> <code>output_directory (str</code> <p>Directory to write case lists</p> <p> </p> <code>study_id</code> <p>cBioPortal study id</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>sv caselist path</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def write_case_list_sv(samples: list, output_directory: str, study_id: str) -&gt; str:\n    \"\"\"Writes the structural variant (sv) sequenced samples\n\n    Args:\n        samples (list): List of sv samples\n        output_directory (str: Directory to write case lists\n        study_id (str): cBioPortal study id\n\n    Returns:\n        str: sv caselist path\n    \"\"\"\n    case_list_ids = \"\\t\".join(samples)\n    caselist_path = os.path.abspath(os.path.join(output_directory, \"cases_sv.txt\"))\n    with open(caselist_path, \"w\") as case_list_file:\n        case_list_file_text = CASE_LIST_TEXT_TEMPLATE.format(\n            study_id=study_id,\n            stable_id=study_id + \"_sv\",\n            case_list_name=\"Samples with Structural Variants\",\n            case_list_description=\"Samples with Structural Variants\",\n            case_list_ids=case_list_ids,\n        )\n        case_list_file.write(case_list_file_text)\n    return caselist_path\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.write_case_list_cnaseq","title":"<code>write_case_list_cnaseq(cna_samples, output_directory, study_id)</code>","text":"<p>writes both cna and mutation samples (Just _cna file for now)</p> PARAMETER DESCRIPTION <code>cna_samples</code> <p>List of cna samples</p> <p> </p> <code>output_directory</code> <p>Directory to write case lists</p> <p> </p> <code>study_id</code> <p>cBioPortal study id</p> <p> </p> RETURNS DESCRIPTION <p>cnaseq path</p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def write_case_list_cnaseq(cna_samples, output_directory, study_id):\n    \"\"\"\n    writes both cna and mutation samples (Just _cna file for now)\n\n    Args:\n        cna_samples: List of cna samples\n        output_directory: Directory to write case lists\n        study_id: cBioPortal study id\n\n    Returns:\n        cnaseq path\n    \"\"\"\n    case_list_ids = \"\\t\".join(cna_samples)\n    cnaseq_caselist_path = os.path.abspath(\n        os.path.join(output_directory, \"cases_cnaseq.txt\")\n    )\n    with open(cnaseq_caselist_path, \"w\") as case_list_file:\n        case_list_file_text = CASE_LIST_TEXT_TEMPLATE.format(\n            study_id=study_id,\n            stable_id=study_id + \"_cnaseq\",\n            case_list_name=\"Samples with CNA and mutation\",\n            case_list_description=\"Samples with CNA and mutation\",\n            case_list_ids=case_list_ids,\n        )\n        case_list_file.write(case_list_file_text)\n    return cnaseq_caselist_path\n</code></pre>"},{"location":"reference/helper_modules/create_case_lists/#genie.create_case_lists.main","title":"<code>main(clinical_file_name, assay_info_file_name, output_directory, study_id)</code>","text":"<p>Gets clinical file and gene matrix file and processes it to obtain case list files</p> PARAMETER DESCRIPTION <code>clinical_file_name</code> <p>Clinical file path</p> <p> </p> <code>assay_info_file_name</code> <p>Assay information name</p> <p> </p> <code>output_directory</code> <p>Output directory of case list files</p> <p> </p> <code>study_id</code> <p>cBioPortal study id</p> <p> </p> Source code in <code>genie/create_case_lists.py</code> <pre><code>def main(clinical_file_name, assay_info_file_name, output_directory, study_id):\n    \"\"\"Gets clinical file and gene matrix file and processes it\n    to obtain case list files\n\n    Args:\n        clinical_file_name: Clinical file path\n        assay_info_file_name: Assay information name\n        output_directory: Output directory of case list files\n        study_id: cBioPortal study id\n    \"\"\"\n    case_lists_map, seq_assay_map, clin_samples = create_case_lists_map(\n        clinical_file_name\n    )\n    write_case_list_files(case_lists_map, output_directory, study_id)\n    # create_sequenced_samples used to get the samples, but since the removal\n    # of WES samples, we can no longer rely on the gene matrix file to grab\n    # all sequenced samples, must use assay information file\n    cna_samples, fusion_samples, sv_samples = create_sequenced_samples(\n        seq_assay_map, assay_info_file_name\n    )\n    write_case_list_sequenced(clin_samples, output_directory, study_id)\n    write_case_list_cna(cna_samples, output_directory, study_id)\n    write_case_list_cnaseq(cna_samples, output_directory, study_id)\n    write_case_list_sv(sv_samples, output_directory, study_id)\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/","title":"dashboard_table_updater","text":""},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater","title":"<code>genie.dashboard_table_updater</code>","text":"<p>Updates dashboard tables</p>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater-attributes","title":"Attributes","text":""},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater-functions","title":"Functions","text":""},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.get_center_data_completion","title":"<code>get_center_data_completion(center, df)</code>","text":"<p>Gets center data completion.  Calulates the percentile of how complete a clinical data element is: Number of not blank/Unknown/NA divded by total number of patients or samples</p> PARAMETER DESCRIPTION <code>center</code> <p>GENIE center</p> <p> </p> <code>df</code> <p>sample or patient dataframe</p> <p> </p> RETURNS DESCRIPTION <code>Dataframe</code> <p>Center data</p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def get_center_data_completion(center, df):\n    \"\"\"\n    Gets center data completion.  Calulates the percentile of\n    how complete a clinical data element is:\n    Number of not blank/Unknown/NA divded by\n    total number of patients or samples\n\n    Args:\n        center: GENIE center\n        df: sample or patient dataframe\n\n    Returns:\n        Dataframe: Center data\n    \"\"\"\n    centerdf = df[df[\"CENTER\"] == center]\n    total = len(centerdf)\n    center_data = pd.DataFrame()\n    skip_cols = [\n        \"CENTER\",\n        \"PATIENT_ID\",\n        \"SAMPLE_ID\",\n        \"SAMPLE_TYPE_DETAILED\",\n        \"SECONDARY_RACE\",\n        \"TERTIARY_RACE\",\n    ]\n    for col in centerdf:\n        if col not in skip_cols:\n            not_missing = [\n                not pd.isnull(value) and value != \"Not Collected\"\n                for value in centerdf[col]\n            ]\n            completeness = float(sum(not_missing)) / int(total)\n            returned = pd.DataFrame([[col, center, total, completeness]])\n            center_data = pd.concat([center_data, returned])\n    return center_data\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_samples_in_release_table","title":"<code>update_samples_in_release_table(syn, file_mapping, release, samples_in_release_synid)</code>","text":"<p>Updates the sample in release table This tracks the samples of each release.  1 means it exists, and 0 means it doesn't. For releases without a column in the sample in release table, a new release column will be created for that release.</p> <p>New samples will be displayed on top of old samples (pre-existing samples already in the sample in release table) in the release column.</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>file_mapping</code> <p>file mapping generated from file mapping function</p> <p> TYPE: <code>dict</code> </p> <code>release</code> <p>GENIE release number (ie. 5.3-consortium)</p> <p> TYPE: <code>str</code> </p> <code>samples_in_release_synid</code> <p>Synapse Id of 'samples in release' Table</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_samples_in_release_table(\n    syn: synapseclient.Synapse,\n    file_mapping: dict,\n    release: str,\n    samples_in_release_synid: str,\n) -&gt; None:\n    \"\"\"\n    Updates the sample in release table\n    This tracks the samples of each release.  1 means it exists, and 0\n    means it doesn't. For releases without a column in the sample\n    in release table, a new release column will be created for\n    that release.\n\n    New samples will be displayed on top of old samples (pre-existing samples\n    already in the sample in release table) in the release column.\n\n    Args:\n        syn (synapseclient.Synapse): synapse object\n        file_mapping (dict): file mapping generated from file mapping function\n        release (str):  GENIE release number (ie. 5.3-consortium)\n        samples_in_release_synid (str): Synapse Id of 'samples in release' Table\n    \"\"\"\n    clinical_ent = syn.get(file_mapping[\"sample\"], followLink=True)\n    clinicaldf = pd.read_csv(clinical_ent.path, sep=\"\\t\", comment=\"#\")\n    cols = [i[\"name\"] for i in list(syn.getTableColumns(samples_in_release_synid))]\n\n    if release not in cols:\n        schema = syn.get(samples_in_release_synid)\n        syn_col = synapseclient.Column(\n            name=release, columnType=\"INTEGER\", defaultValue=0\n        )\n        new_column = syn.store(syn_col)\n        schema.addColumn(new_column)\n        # no need to return schema variable because it isn't used\n        syn.store(schema)\n    # Columns of samples in release\n    samples_per_releasedf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f'SELECT SAMPLE_ID, \"{release}\" FROM {samples_in_release_synid}',\n    )\n    new_samples = clinicaldf[[\"SAMPLE_ID\"]][\n        ~clinicaldf.SAMPLE_ID.isin(samples_per_releasedf.SAMPLE_ID)\n    ]\n\n    new_samples[release] = 1\n    old_samples = clinicaldf[[\"SAMPLE_ID\"]][\n        clinicaldf.SAMPLE_ID.isin(samples_per_releasedf.SAMPLE_ID)\n    ]\n\n    old_samples[release] = 1\n    samples_in_releasedf = pd.concat([new_samples, old_samples])\n    load._update_table(\n        syn,\n        samples_per_releasedf,\n        samples_in_releasedf,\n        samples_in_release_synid,\n        [\"SAMPLE_ID\"],\n    )\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_cumulative_sample_table","title":"<code>update_cumulative_sample_table(syn, file_mapping, release, cumulative_sample_count_synid)</code>","text":"<p>Consortium release sample count table update function This gets the cumulative sample count of each file type in each release</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>file_mapping</code> <p>file mapping generated from file mapping function</p> <p> </p> <code>release</code> <p>GENIE release number (ie. 5.3-consortium)</p> <p> </p> <code>cumulative_sample_count_synid</code> <p>Synapse Id of 'Cumulative sample count' Table</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_cumulative_sample_table(\n    syn, file_mapping, release, cumulative_sample_count_synid\n):\n    \"\"\"\n    Consortium release sample count table update function\n    This gets the cumulative sample count of each file type in each release\n\n    Args:\n        syn: synapse object\n        file_mapping: file mapping generated from file mapping function\n        release:  GENIE release number (ie. 5.3-consortium)\n        cumulative_sample_count_synid: Synapse Id of\n            'Cumulative sample count' Table\n    \"\"\"\n    sample_count_per_rounddf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT * FROM {cumulative_sample_count_synid} where Release = '{release}'\",\n    )\n    clinical_ent = syn.get(file_mapping[\"sample\"], followLink=True)\n    clinicaldf = pd.read_csv(clinical_ent.path, sep=\"\\t\", comment=\"#\")\n    clinicaldf.columns = [i.upper() for i in clinicaldf.columns]\n    if clinicaldf.get(\"CENTER\") is None:\n        clinicaldf[\"CENTER\"] = [sample.split(\"-\")[1] for sample in clinicaldf.SAMPLE_ID]\n    clinical_counts = clinicaldf[\"CENTER\"].value_counts()\n    clinical_counts[\"Total\"] = sum(clinical_counts)\n    clinical_counts.name = \"Clinical\"\n\n    sv_ent = syn.get(file_mapping[\"sv\"], followLink=True)\n    sv_df = pd.read_csv(sv_ent.path, sep=\"\\t\", comment=\"#\")\n    sv_df.columns = [col.upper() for col in sv_df.columns]\n    sv_counts = sv_df[\"CENTER\"][~sv_df[\"SAMPLE_ID\"].duplicated()].value_counts()\n    sv_counts[\"Total\"] = sum(sv_counts)\n\n    cna_ent = syn.get(file_mapping[\"cna\"], followLink=True)\n    cnadf = pd.read_csv(cna_ent.path, sep=\"\\t\", comment=\"#\")\n    cna_counts = pd.Series([i.split(\"-\")[1] for i in cnadf.columns[1:]]).value_counts()\n    cna_counts[\"Total\"] = sum(cna_counts)\n\n    seg_ent = syn.get(file_mapping[\"seg\"], followLink=True)\n    segdf = pd.read_csv(seg_ent.path, sep=\"\\t\", comment=\"#\")\n    segdf.columns = [i.upper() for i in segdf.columns]\n\n    segdf[\"CENTER\"] = [i.split(\"-\")[1] for i in segdf[\"ID\"]]\n    seg_counts = segdf[\"CENTER\"][~segdf[\"ID\"].duplicated()].value_counts()\n    seg_counts[\"Total\"] = sum(seg_counts)\n\n    total_counts = pd.DataFrame(clinical_counts)\n    total_counts[\"Fusions\"] = sv_counts\n    total_counts[\"CNV\"] = cna_counts\n    total_counts[\"Mutation\"] = clinical_counts\n    total_counts[\"SEG\"] = seg_counts\n    total_counts = total_counts.fillna(0)\n    total_counts = total_counts.applymap(int)\n    total_counts[\"Center\"] = total_counts.index\n    total_counts[\"Release\"] = release\n    load._update_table(\n        syn,\n        sample_count_per_rounddf,\n        total_counts,\n        cumulative_sample_count_synid,\n        [\"Center\", \"Release\"],\n        to_delete=True,\n    )\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_release_numbers","title":"<code>update_release_numbers(syn, database_mappingdf, release=None)</code>","text":"<p>Updates all release dashboard numbers or specific release number</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> <code>release</code> <p>GENIE release (ie. 5.3-consortium).  Defaults to None</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_release_numbers(syn, database_mappingdf, release=None):\n    \"\"\"\n    Updates all release dashboard numbers or specific release number\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n        release: GENIE release (ie. 5.3-consortium).  Defaults to None\n    \"\"\"\n    # Update release table with current release or all releases\n    samples_in_release_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"samplesInRelease\"\n    ].values[0]\n    cumulative_sample_count_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"cumulativeSampleCount\"\n    ].values[0]\n\n    release_folder_fileview_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"releaseFolder\"\n    ].values[0]\n    # Add this query so that the fileview is indexed to include the\n    # new release folder\n    syn.tableQuery(f\"select * from {release_folder_fileview_synid} limit 1\")\n    query_str = (\n        f\"select id,name from {release_folder_fileview_synid} where \"\n        \"name not like 'Release%' and name &lt;&gt; 'case_lists' and \"\n        \"name not like '%.0.%'\"\n    )\n    release_folderdf = extract.get_syntabledf(syn=syn, query_string=query_str)\n\n    for rel_synid, rel_name in zip(release_folderdf.id, release_folderdf.name):\n        file_mapping = extract.get_file_mapping(syn, rel_synid)\n        # If release is specified, only process on that,\n        # otherwise process for all\n        if release is None or release == rel_name:\n            update_samples_in_release_table(\n                syn, file_mapping, rel_name, samples_in_release_synid\n            )\n            update_cumulative_sample_table(\n                syn, file_mapping, rel_name, cumulative_sample_count_synid\n            )\n        else:\n            pass\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_database_numbers","title":"<code>update_database_numbers(syn, database_mappingdf)</code>","text":"<p>Updates database cumulative numbers (Only called when not staging)</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_database_numbers(syn, database_mappingdf):\n    \"\"\"\n    Updates database cumulative numbers (Only called when not staging)\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n    \"\"\"\n    cumulative_sample_count_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"cumulativeSampleCount\"\n    ].values[0]\n    # Database\n    database_countdf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT * FROM {cumulative_sample_count_synid} where Release = 'Database'\",\n    )\n    clinicaldf = extract.get_syntabledf(\n        syn=syn, query_string=\"select CENTER from syn7517674\"\n    )\n    clinincal_counts = clinicaldf[\"CENTER\"].value_counts()\n    clinincal_counts[\"Total\"] = sum(clinincal_counts)\n    clinincal_counts.name = \"Clinical\"\n    sv_df = extract.get_syntabledf(syn=syn, query_string=\"select * from syn30891574\")\n    sv_counts = sv_df[\"CENTER\"][~sv_df[\"SAMPLE_ID\"].duplicated()].value_counts()\n    sv_counts[\"Total\"] = sum(sv_counts)\n\n    center_flat_files = syn.getChildren(\"syn12278118\")\n    cna_file_paths = [\n        syn.get(file[\"id\"]).path\n        for file in center_flat_files\n        if file[\"name\"].startswith(\"data_CNA\")\n    ]\n    cna_numbers = {}\n    for cna_file in cna_file_paths:\n        center = os.path.basename(cna_file).replace(\".txt\", \"\").split(\"_\")[2]\n        with open(cna_file, \"r\") as cna:\n            header = cna.readline()\n            samples = header.split(\"\\t\")\n            # Minus one because of Hugo_Symbol\n            cna_numbers[center] = len(samples) - 1\n    cna_counts = pd.Series(cna_numbers)\n    cna_counts[\"Total\"] = sum(cna_counts)\n    segdf = extract.get_syntabledf(syn=syn, query_string=\"select * from syn7893341\")\n    seg_counts = segdf[\"CENTER\"][~segdf[\"ID\"].duplicated()].value_counts()\n    seg_counts[\"Total\"] = sum(seg_counts)\n\n    db_counts = pd.DataFrame(clinincal_counts)\n    db_counts[\"Fusions\"] = sv_counts\n    db_counts[\"CNV\"] = cna_counts\n    db_counts[\"Mutation\"] = clinincal_counts\n    db_counts[\"SEG\"] = seg_counts\n    db_counts = db_counts.fillna(0)\n    db_counts = db_counts.applymap(int)\n    db_counts[\"Center\"] = db_counts.index\n    db_counts[\"Release\"] = \"Database\"\n    load._update_table(\n        syn,\n        database_countdf,\n        db_counts,\n        cumulative_sample_count_synid,\n        [\"Center\", \"Release\"],\n    )\n    today = datetime.date.today()\n    if today.month in [1, 4, 8, 12]:\n        db_count_tracker = db_counts[[\"Clinical\", \"Center\", \"Release\"]]\n        db_count_tracker.rename(\n            columns={\"Clinical\": \"sample_count\", \"Center\": \"center\", \"Release\": \"date\"},\n            inplace=True,\n        )\n        db_count_tracker[\"date\"] = today.strftime(\"%b-%Y\")\n        # Hard coded syn id\n        syn.store(synapseclient.Table(\"syn18404852\", db_count_tracker))\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_oncotree_code_tables","title":"<code>update_oncotree_code_tables(syn, database_mappingdf)</code>","text":"<p>Updates database statistics of oncotree codes and primary onocotree codes</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_oncotree_code_tables(syn, database_mappingdf):\n    \"\"\"\n    Updates database statistics of oncotree codes\n    and primary onocotree codes\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n    \"\"\"\n    oncotree_distribution_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"oncotree\"\n    ].values[0]\n    clinicaldf = extract.get_syntabledf(\n        syn=syn, query_string=\"select * from syn7517674\"\n    )\n\n    # DISTRIBUTION OF ONCOTREE CODE TABLE UPDATE\n    oncotree_code_distributiondf = pd.DataFrame(\n        columns=set(clinicaldf[\"CENTER\"]), index=set(clinicaldf[\"ONCOTREE_CODE\"])\n    )\n    for center in oncotree_code_distributiondf.columns:\n        onc_counts = clinicaldf[\"ONCOTREE_CODE\"][\n            clinicaldf[\"CENTER\"] == center\n        ].value_counts()\n        oncotree_code_distributiondf[center] = onc_counts\n    oncotree_code_distributiondf = oncotree_code_distributiondf.fillna(0)\n    oncotree_code_distributiondf = oncotree_code_distributiondf.applymap(int)\n    oncotree_code_distributiondf[\"Total\"] = oncotree_code_distributiondf.apply(\n        sum, axis=1\n    )\n    oncotree_code_distributiondf[\"Oncotree_Code\"] = oncotree_code_distributiondf.index\n\n    center_string = \",\".join(clinicaldf[\"CENTER\"].unique())\n    oncotree_distribution_dbdf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT Oncotree_Code,{center_string},Total  FROM {oncotree_distribution_synid}\",\n    )\n    load._update_table(\n        syn,\n        oncotree_distribution_dbdf,\n        oncotree_code_distributiondf,\n        oncotree_distribution_synid,\n        [\"Oncotree_Code\"],\n        to_delete=True,\n    )\n\n    # DISTRIBUTION OF PRIMARY CODE TABLE UPDATE\n    oncotree_link_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"oncotreeLink\"\n    ].values[0]\n    primary_code_synId = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"primaryCode\"\n    ].values[0]\n\n    # Can also use most up to date oncotree code,\n    # because these tables are updated from the database\n    oncotree_link_ent = syn.get(oncotree_link_synid)\n    oncotree_link = oncotree_link_ent.externalURL\n    oncotree_mapping = process_functions.get_oncotree_code_mappings(oncotree_link)\n\n    clinicaldf[\"PRIMARY_CODES\"] = [\n        (\n            oncotree_mapping[i.upper()][\"ONCOTREE_PRIMARY_NODE\"]\n            if i.upper() in oncotree_mapping.keys()\n            else \"DEPRECATED_CODE\"\n        )\n        for i in clinicaldf.ONCOTREE_CODE\n    ]\n\n    # ### DISTRIBUTION OF PRIMARY ONCOTREE CODE TABLE UPDATE\n    primary_code_distributiondf = pd.DataFrame(\n        columns=set(clinicaldf[\"CENTER\"]), index=set(clinicaldf[\"PRIMARY_CODES\"])\n    )\n\n    for center in primary_code_distributiondf.columns:\n        onc_counts = clinicaldf[\"PRIMARY_CODES\"][\n            clinicaldf[\"CENTER\"] == center\n        ].value_counts()\n        primary_code_distributiondf[center] = onc_counts\n    primary_code_distributiondf = primary_code_distributiondf.fillna(0)\n    primary_code_distributiondf = primary_code_distributiondf.applymap(int)\n    primary_code_distributiondf[\"Total\"] = primary_code_distributiondf.apply(\n        sum, axis=1\n    )\n    primary_code_distributiondf[\"Oncotree_Code\"] = primary_code_distributiondf.index\n    primary_code_dist_dbdf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT Oncotree_Code,{center_string},Total  FROM {primary_code_synId}\",\n    )\n    load._update_table(\n        syn,\n        primary_code_dist_dbdf,\n        primary_code_distributiondf,\n        primary_code_synId,\n        [\"Oncotree_Code\"],\n        to_delete=True,\n    )\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_sample_difference_table","title":"<code>update_sample_difference_table(syn, database_mappingdf)</code>","text":"<p>Updates sample difference table between consortium releases</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_sample_difference_table(syn, database_mappingdf):\n    \"\"\"\n    Updates sample difference table between\n    consortium releases\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n    \"\"\"\n    cumulative_sample_count_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"cumulativeSampleCount\"\n    ].values[0]\n\n    sample_diff_count_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"sampleDiffCount\"\n    ].values[0]\n\n    # UPDATE DIFF TABLE\n    sample_count_per_rounddf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT * FROM {cumulative_sample_count_synid} where Center &lt;&gt; 'Total' and Release &lt;&gt; 'Database'\",\n    )\n    releases = list(sample_count_per_rounddf[\"Release\"].unique())\n    # sort the releases and remove public releases\n    releases.sort()\n    consortium_releases = [\n        release\n        for release in releases\n        if \"public\" not in release and \".0.\" not in release\n    ]\n\n    diff_between_releasesdf = sample_count_per_rounddf[\n        sample_count_per_rounddf[\"Release\"] == consortium_releases[0]\n    ]\n\n    for index, release_name in enumerate(consortium_releases[1:]):\n        prior_release = sample_count_per_rounddf[\n            sample_count_per_rounddf[\"Release\"] == consortium_releases[index]\n        ]\n\n        current_release = sample_count_per_rounddf[\n            sample_count_per_rounddf[\"Release\"] == release_name\n        ]\n\n        prior_release.index = prior_release[\"Center\"]\n        current_release.index = current_release[\"Center\"]\n\n        del prior_release[\"Center\"]\n        del prior_release[\"Release\"]\n        del current_release[\"Center\"]\n        del current_release[\"Release\"]\n        # Append new rows of centers that are new and\n        # just added to the releases\n        new_centers = current_release.index[\n            ~current_release.index.isin(prior_release.index)\n        ]\n\n        if not new_centers.empty:\n            prior_release = pd.concat([prior_release, pd.DataFrame(index=new_centers)])\n            prior_release = prior_release.fillna(0)\n        difference = current_release - prior_release\n        difference[\"Center\"] = difference.index\n        difference[\"Release\"] = release_name\n        diff_between_releasesdf = pd.concat([diff_between_releasesdf, difference])\n    difftable_dbdf = extract.get_syntabledf(\n        syn=syn, query_string=f\"SELECT * FROM {sample_diff_count_synid}\"\n    )\n    difftable_dbdf = difftable_dbdf.fillna(0)\n    new_values = (\n        diff_between_releasesdf[[\"Clinical\", \"Mutation\", \"CNV\", \"SEG\", \"Fusions\"]]\n        .fillna(0)\n        .applymap(int)\n    )\n\n    diff_between_releasesdf[\n        [\"Clinical\", \"Mutation\", \"CNV\", \"SEG\", \"Fusions\"]\n    ] = new_values\n\n    load._update_table(\n        syn,\n        difftable_dbdf,\n        diff_between_releasesdf,\n        sample_diff_count_synid,\n        [\"Center\", \"Release\"],\n        to_delete=True,\n    )\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_data_completeness_table","title":"<code>update_data_completeness_table(syn, database_mappingdf)</code>","text":"<p>Updates the data completeness of the database</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_data_completeness_table(syn, database_mappingdf):\n    \"\"\"\n    Updates the data completeness of the database\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n    \"\"\"\n    data_completion_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"dataCompletion\"\n    ].values[0]\n\n    sampledf = extract.get_syntabledf(syn=syn, query_string=\"select * from syn7517674\")\n    patientdf = extract.get_syntabledf(syn=syn, query_string=\"select * from syn7517669\")\n    data_completenessdf = pd.DataFrame()\n    center_infos = sampledf.CENTER.drop_duplicates().apply(\n        lambda center: get_center_data_completion(center, sampledf)\n    )\n    for center_info in center_infos:\n        data_completenessdf = pd.concat([data_completenessdf, center_info])\n\n    center_infos = patientdf.CENTER.drop_duplicates().apply(\n        lambda center: get_center_data_completion(center, patientdf)\n    )\n    for center_info in center_infos:\n        data_completenessdf = pd.concat([data_completenessdf, center_info])\n    data_completeness_dbdf = extract.get_syntabledf(\n        syn=syn, query_string=f\"select * from {data_completion_synid}\"\n    )\n    data_completenessdf.columns = data_completeness_dbdf.columns\n    load._update_table(\n        syn,\n        data_completeness_dbdf,\n        data_completenessdf,\n        data_completion_synid,\n        [\"FIELD\", \"CENTER\"],\n        to_delete=True,\n    )\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_wiki","title":"<code>update_wiki(syn, database_mappingdf)</code>","text":"<p>Updates the GENIE project dashboard wiki timestamp</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_wiki(syn, database_mappingdf):\n    \"\"\"\n    Updates the GENIE project dashboard wiki timestamp\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n\n    \"\"\"\n    # Updates to query and date dashboard was updated\n    cumulative_sample_count_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"cumulativeSampleCount\"\n    ].values[0]\n\n    primary_code_synId = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"primaryCode\"\n    ].values[0]\n    centersdf = extract.get_syntabledf(\n        syn=syn, query_string=\"select distinct(CENTER) as CENTER from syn7517674\"\n    )\n    now = datetime.datetime.now()\n    markdown = [\n        \"_Updated {month}/{day}/{year}_\\n\\n\".format(\n            month=now.month, day=now.day, year=now.year\n        ),\n        \"##Count of Clinical Samples\\n\",\n        \"${synapsetable?query=SELECT Center%2C Clinical%2C Release FROM \"\n        + cumulative_sample_count_synid\n        + \"}\\n\\n\",\n        \"\\n\\n##Primary Oncotree Codes\\n\\n\",\n        \"${synapsetable?query=SELECT Oncotree%5FCode%2C \"\n        + \"%2C \".join(centersdf[\"CENTER\"].unique())\n        + \"%2C Total FROM \"\n        + primary_code_synId\n        + \" ORDER BY Total DESC&amp;limit=15}\\n\\n\",\n    ]\n\n    wikipage = syn.getWiki(\"syn3380222\", 235803)\n    wikipage.markdown = \"\".join(markdown)\n    syn.store(wikipage)\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.string_to_unix_epoch_time_milliseconds","title":"<code>string_to_unix_epoch_time_milliseconds(string_time)</code>","text":"<p>Takes dates in this format: 2018-10-25T20:16:07.959Z and turns it into unix epoch time in milliseconds</p> PARAMETER DESCRIPTION <code>string_time</code> <p>string in this format: 2018-10-25T20:16:07.959Z</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>int</code> <p>unix epoch time in milliseconds</p> <p> TYPE: <code>int</code> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def string_to_unix_epoch_time_milliseconds(string_time: str) -&gt; int:\n    \"\"\"\n    Takes dates in this format: 2018-10-25T20:16:07.959Z\n    and turns it into unix epoch time in milliseconds\n\n    Args:\n        string_time: string in this format: 2018-10-25T20:16:07.959Z\n\n    Returns:\n        int: unix epoch time in milliseconds\n    \"\"\"\n    datetime_obj = datetime.datetime.strptime(\n        string_time.split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\"\n    )\n    return process_functions.to_unix_epoch_time_utc(datetime_obj)\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.update_data_release_file_table","title":"<code>update_data_release_file_table(syn, database_mappingdf)</code>","text":"<p>Updates data release file table</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def update_data_release_file_table(syn, database_mappingdf):\n    \"\"\"\n    Updates data release file table\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n    \"\"\"\n    release_folder_fileview_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"releaseFolder\"\n    ].values[0]\n    # Add this query so that the fileview is indexed to include the\n    # new release folder\n    # syn.tableQuery(f\"select * from {release_folder_fileview_synid} limit 1\")\n    query_str = (\n        f\"select id,name from {release_folder_fileview_synid} where \"\n        \"name not like 'Release%' and name &lt;&gt; 'case_lists' \"\n        \"and name not like '0.%'\"\n    )\n    release_folderdf = extract.get_syntabledf(syn=syn, query_string=query_str)\n\n    data_release_table_synid = \"syn16804261\"\n    data_release_tabledf = extract.get_syntabledf(\n        syn=syn, query_string=f\"select * from {data_release_table_synid}\"\n    )\n\n    not_in_release_tabledf = release_folderdf[\n        ~release_folderdf.name.isin(data_release_tabledf.release)\n    ]\n\n    for synid, name in zip(not_in_release_tabledf.id, not_in_release_tabledf.name):\n        release_files = syn.getChildren(synid)\n\n        append_rows = [\n            [\n                release_file[\"name\"],\n                release_file[\"id\"],\n                name,\n                string_to_unix_epoch_time_milliseconds(release_file[\"modifiedOn\"]),\n                synid,\n            ]\n            for release_file in release_files\n            if release_file[\"name\"] != \"case_lists\"\n        ]\n\n        syn.store(synapseclient.Table(data_release_table_synid, append_rows))\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.check_column_decreases","title":"<code>check_column_decreases(currentdf, olderdf)</code>","text":"<p>Checks entity decreases</p> PARAMETER DESCRIPTION <code>current_ent</code> <p>Current entity dataframe</p> <p> </p> <code>old_ent</code> <p>Older entity dataframe</p> <p> </p> RETURNS DESCRIPTION <p>Differences in values</p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def check_column_decreases(currentdf, olderdf):\n    \"\"\"\n    Checks entity decreases\n\n    Args:\n        current_ent: Current entity dataframe\n        old_ent: Older entity dataframe\n\n    Returns:\n        Differences in values\n    \"\"\"\n    diff_map = dict()\n    for col in currentdf:\n        new_counts = currentdf[col].value_counts()\n        if olderdf.get(col) is not None:\n            old_counts = olderdf[col].value_counts()\n            # Make sure any values that exist in the new get added\n            # to the old to show the decrease\n            new_keys = pd.Series(\n                index=new_counts.keys()[~new_counts.keys().isin(old_counts.keys())]\n            )\n            old_counts = old_counts.add(new_keys, fill_value=0)\n            old_counts.fillna(0, inplace=True)\n            # Make sure any values that don't exist in the old get added\n            # to show the decrease\n            new_keys = pd.Series(\n                index=old_counts.keys()[~old_counts.keys().isin(new_counts.keys())]\n            )\n            new_counts = new_counts.add(new_keys, fill_value=0)\n            new_counts.fillna(0, inplace=True)\n            if any(new_counts - old_counts &lt; 0):\n                logger.info(\"\\tDECREASE IN COLUMN: %s\" % col)\n                # diff = new_counts[new_counts - old_counts &lt; 0]\n                diffs = new_counts - old_counts\n                diffstext = diffs[diffs &lt; 0].to_csv().replace(\"\\n\", \"; \")\n                logger.info(\"\\t\" + diffstext)\n                diff_map[col] = True\n            else:\n                diff_map[col] = False\n    return diff_map\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.print_clinical_values_difference_table","title":"<code>print_clinical_values_difference_table(syn, database_mappingdf)</code>","text":"<p>Checks for a decrease in values in the clinical file from last consortium release to most recent consortium release</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def print_clinical_values_difference_table(syn, database_mappingdf):\n    \"\"\"\n    Checks for a decrease in values in the clinical file\n    from last consortium release to most recent consortium release\n\n    Args:\n        syn: synapse object\n        database_mappingdf: mapping between synapse ids and database\n    \"\"\"\n    release_folder_fileview_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"releaseFolder\"\n    ].values[0]\n\n    clinical_key_decrease_synid = database_mappingdf[\"Id\"][\n        database_mappingdf[\"Database\"] == \"clinicalKeyDecrease\"\n    ].values[0]\n\n    query_str = (\n        f\"select id,name from {release_folder_fileview_synid} where \"\n        \"name not like 'Release%' and name &lt;&gt; 'case_lists' \"\n        \"and name not like '%.0.%' and name not like '%-public' \"\n        \"and name &lt;&gt; 'potential_artifacts'\"\n    )\n    release_folderdf = extract.get_syntabledf(syn=syn, query_string=query_str)\n    # Set release number as a numerical value since string \"10\" &lt; \"9\"\n    # Also can't set by created on date, because sometimes\n    # there are patch releases\n    release_folderdf[\"num_release\"] = [\n        float(name.replace(\".0\", \"\").replace(\"-consortium\", \"\"))\n        for name in release_folderdf[\"name\"]\n    ]\n    release_folderdf.sort_values(\"num_release\", ascending=False, inplace=True)\n    current_release = release_folderdf[\"id\"][0]\n    older_release = release_folderdf[\"id\"][1]\n\n    current_release_files = syn.getChildren(current_release)\n    current_clinical_synids = {\n        file[\"name\"]: file[\"id\"]\n        for file in current_release_files\n        if file[\"name\"] in [\"data_clinical_sample.txt\", \"data_clinical_patient.txt\"]\n    }\n\n    older_release_files = syn.getChildren(older_release)\n\n    older_clinical_synids = {\n        file[\"name\"]: file[\"id\"]\n        for file in older_release_files\n        if file[\"name\"] in [\"data_clinical_sample.txt\", \"data_clinical_patient.txt\"]\n    }\n\n    current_sample_ent = syn.get(\n        current_clinical_synids[\"data_clinical_sample.txt\"], followLink=True\n    )\n\n    older_sample_ent = syn.get(\n        older_clinical_synids[\"data_clinical_sample.txt\"], followLink=True\n    )\n    current_sampledf = pd.read_csv(current_sample_ent.path, sep=\"\\t\", comment=\"#\")\n\n    current_sampledf[\"CENTER\"] = [\n        patient.split(\"-\")[1] for patient in current_sampledf[\"PATIENT_ID\"]\n    ]\n\n    older_sampledf = pd.read_csv(older_sample_ent.path, sep=\"\\t\", comment=\"#\")\n    older_sampledf[\"CENTER\"] = [\n        patient.split(\"-\")[1] for patient in older_sampledf[\"PATIENT_ID\"]\n    ]\n    # Rather than take the CENTER, must take the SAMPLE_ID to compare\n    current_sampledf = current_sampledf[\n        current_sampledf[\"SAMPLE_ID\"].isin(older_sampledf[\"SAMPLE_ID\"].unique())\n    ]\n\n    logger.info(\"SAMPLE CLINICAL VALUE DECREASES\")\n    center_decrease_mapping = dict()\n    for center in older_sampledf[\"CENTER\"].unique():\n        current_center_sampledf = current_sampledf[current_sampledf[\"CENTER\"] == center]\n\n        older_center_sampledf = older_sampledf[older_sampledf[\"CENTER\"] == center]\n\n        logger.info(center)\n\n        decrease_map = check_column_decreases(\n            current_center_sampledf, older_center_sampledf\n        )\n        center_decrease_mapping[center] = decrease_map\n\n    current_patient_ent = syn.get(\n        current_clinical_synids[\"data_clinical_patient.txt\"], followLink=True\n    )\n\n    older_patient_ent = syn.get(\n        older_clinical_synids[\"data_clinical_patient.txt\"], followLink=True\n    )\n\n    current_patientdf = pd.read_csv(current_patient_ent.path, sep=\"\\t\", comment=\"#\")\n\n    older_patientdf = pd.read_csv(older_patient_ent.path, sep=\"\\t\", comment=\"#\")\n    # Rather than take the CENTER, must take the PATIENT_ID to compare\n    current_patientdf = current_patientdf[\n        current_patientdf[\"PATIENT_ID\"].isin(older_patientdf[\"PATIENT_ID\"].unique())\n    ]\n\n    logger.info(\"PATIENT CLINICAL VALUE DECREASES\")\n    for center in older_patientdf[\"CENTER\"].unique():\n        current_center_patientdf = current_patientdf[\n            current_patientdf[\"CENTER\"] == center\n        ]\n\n        older_center_patientdf = older_patientdf[older_patientdf[\"CENTER\"] == center]\n\n        logger.info(center)\n        patient_decrease_map = check_column_decreases(\n            current_center_patientdf, older_center_patientdf\n        )\n\n        center_decrease_mapping[center].update(patient_decrease_map)\n\n    center_decrease_mapping = pd.DataFrame(center_decrease_mapping)\n    center_decrease_mapping = center_decrease_mapping.transpose()\n    center_decrease_mapping[\"CENTER\"] = center_decrease_mapping.index\n\n    clinical_key_decreasedbdf = extract.get_syntabledf(\n        syn=syn, query_string=f\"select * from {clinical_key_decrease_synid}\"\n    )\n    load._update_table(\n        syn,\n        clinical_key_decreasedbdf,\n        center_decrease_mapping,\n        clinical_key_decrease_synid,\n        [\"CENTER\"],\n        to_delete=True,\n    )\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.run_dashboard","title":"<code>run_dashboard(syn, database_mappingdf, release, staging=False, public=False)</code>","text":"<p>Runs the dashboard scripts</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>database_mappingdf</code> <p>mapping between synapse ids and database</p> <p> TYPE: <code>DataFrame</code> </p> <code>release</code> <p>GENIE release (ie. 5.3-consortium)</p> <p> TYPE: <code>str</code> </p> <code>public</code> <p>whether to run for public release or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>staging</code> <p>whether to run in staging mode or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def run_dashboard(\n    syn: synapseclient.Synapse,\n    database_mappingdf: pd.DataFrame,\n    release: str,\n    staging: bool = False,\n    public: bool = False,\n) -&gt; None:\n    \"\"\"\n    Runs the dashboard scripts\n\n    Args:\n        syn (synapseclient.Synapse): synapse object\n        database_mappingdf (pd.DataFrame): mapping between synapse ids and database\n        release (str): GENIE release (ie. 5.3-consortium)\n        public (bool): whether to run for public release or not\n        staging (bool): whether to run in staging mode or not\n    \"\"\"\n    update_release_numbers(syn, database_mappingdf, release=release)\n\n    if not staging:\n        update_data_release_file_table(syn, database_mappingdf)\n        if not public:\n            print_clinical_values_difference_table(syn, database_mappingdf)\n            update_sample_difference_table(syn, database_mappingdf)\n            update_data_completeness_table(syn, database_mappingdf)\n            update_database_numbers(syn, database_mappingdf)\n            update_oncotree_code_tables(syn, database_mappingdf)\n            update_wiki(syn, database_mappingdf)\n</code></pre>"},{"location":"reference/helper_modules/dashboard_table_updater/#genie.dashboard_table_updater.main","title":"<code>main()</code>","text":"Source code in <code>genie/dashboard_table_updater.py</code> <pre><code>def main():\n    parser = argparse.ArgumentParser(description=\"Update dashboard tables\")\n\n    parser.add_argument(\n        \"--release\", help=\"GENIE release number (ie. 5.3-consortium)\", default=None\n    )\n\n    parser.add_argument(\n        \"--staging\", action=\"store_true\", help=\"Using staging directory files\"\n    )\n\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Synapse debugging flag\")\n\n    parser.add_argument(\n        \"--public\", action=\"store_true\", help=\"Set true if releasing public release\"\n    )\n\n    args = parser.parse_args()\n    syn = process_functions.synapse_login(debug=args.debug)\n    if args.staging:\n        # Database to Synapse Id mapping Table\n        # TODO: use project ids instead of table ids\n        database_mapping_synid = \"syn12094210\"\n    else:\n        database_mapping_synid = \"syn10967259\"\n\n    # TODO: get genie config here... that would be a lot easier\n    # extract.get_genie_config(syn=syn, project_id=)\n    database_mapping = syn.tableQuery(\"select * from %s\" % database_mapping_synid)\n    database_mappingdf = database_mapping.asDataFrame()\n\n    run_dashboard(\n        syn, database_mappingdf, args.release, staging=args.staging, public=args.public\n    )\n</code></pre>"},{"location":"reference/helper_modules/extract/","title":"extract","text":""},{"location":"reference/helper_modules/extract/#genie.extract","title":"<code>genie.extract</code>","text":"<p>This module contains all the functions that extract data from Synapse</p>"},{"location":"reference/helper_modules/extract/#genie.extract-attributes","title":"Attributes","text":""},{"location":"reference/helper_modules/extract/#genie.extract.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/extract/#genie.extract.stdout_handler","title":"<code>stdout_handler = logging.StreamHandler(stream=(sys.stdout))</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/extract/#genie.extract-functions","title":"Functions","text":""},{"location":"reference/helper_modules/extract/#genie.extract.get_center_input_files","title":"<code>get_center_input_files(syn, synid, center, process='main', downloadFile=True)</code>","text":"<p>Walks through each center's input directory to get a list of tuples of center files</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>synid</code> <p>Synapse Id of a folder</p> <p> TYPE: <code>str</code> </p> <code>center</code> <p>GENIE center name</p> <p> TYPE: <code>str</code> </p> <code>process</code> <p>Process type include \"main\", \"mutation\".                      Defaults to \"main\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'main'</code> </p> <code>downloadFile</code> <p>Downloads the file. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list</code> <p>List of Synapse entities</p> <p> TYPE: <code>List[List[Entity]]</code> </p> Source code in <code>genie/extract.py</code> <pre><code>def get_center_input_files(\n    syn: synapseclient.Synapse,\n    synid: str,\n    center: str,\n    process: str = \"main\",\n    downloadFile: bool = True,\n) -&gt; List[List[synapseclient.Entity]]:\n    \"\"\"Walks through each center's input directory\n    to get a list of tuples of center files\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        synid (str): Synapse Id of a folder\n        center (str): GENIE center name\n        process (str, optional): Process type include \"main\", \"mutation\".\n                                 Defaults to \"main\".\n        downloadFile (bool, optional): Downloads the file. Defaults to True.\n\n    Returns:\n        list: List of Synapse entities\n    \"\"\"\n    logger.info(\"GETTING {center} INPUT FILES\".format(center=center))\n    clinical_pair_name = [\n        \"data_clinical_supp_sample_{center}.txt\".format(center=center),\n        \"data_clinical_supp_patient_{center}.txt\".format(center=center),\n    ]\n\n    center_files = synapseutils.walk(syn, synid)\n    clinicalpair_entities = []\n    prepared_center_file_list = []\n\n    for _, _, entities in center_files:\n        for name, ent_synid in entities:\n            # This is to remove vcfs from being validated during main\n            # processing. Often there are too many vcf files, and it is\n            # not necessary for them to be run everytime.\n            if name.endswith(\".vcf\") and process != \"mutation\":\n                continue\n\n            logger.info(f\"GETTING FILE {name} ({ent_synid})\")\n            ent = syn.get(ent_synid, downloadFile=downloadFile)\n\n            # HACK: Clinical file can come as two files.\n            # The two files need to be merged together which is\n            # why there is this format\n            if name in clinical_pair_name:\n                clinicalpair_entities.append(ent)\n                continue\n\n            prepared_center_file_list.append([ent])\n\n    if clinicalpair_entities:\n        prepared_center_file_list.append(clinicalpair_entities)\n\n    return prepared_center_file_list\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract._map_name_to_filetype","title":"<code>_map_name_to_filetype(name)</code>","text":"<p>Maps file name to filetype</p> PARAMETER DESCRIPTION <code>name</code> <p>File name</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>filetype</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/extract.py</code> <pre><code>def _map_name_to_filetype(name: str) -&gt; str:\n    \"\"\"Maps file name to filetype\n\n    Args:\n        name (str): File name\n\n    Returns:\n        str: filetype\n    \"\"\"\n    # By default, set the filetype to be the filename\n    filetype = name\n    if not name.startswith(\"meta\"):\n        if name.startswith(\"data_clinical_sample\"):\n            filetype = \"sample\"\n        elif name.endswith(\"fusions.txt\"):\n            filetype = \"fusion\"\n        elif name.endswith(\"CNA.txt\"):\n            filetype = \"cna\"\n        elif name.endswith(\".seg\"):\n            filetype = \"seg\"\n        elif name == \"data_sv.txt\":\n            filetype = \"sv\"\n    return filetype\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract.get_file_mapping","title":"<code>get_file_mapping(syn, synid)</code>","text":"<p>Get mapping between Synapse entity name and Synapse ids of all entities in a folder</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>synid</code> <p>Synapse Id of folder</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>mapping between Synapse Entity name and Id</p> <p> TYPE: <code>dict</code> </p> Source code in <code>genie/extract.py</code> <pre><code>def get_file_mapping(syn: synapseclient.Synapse, synid: str) -&gt; dict:\n    \"\"\"Get mapping between Synapse entity name and Synapse ids\n    of all entities in a folder\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        synid (str): Synapse Id of folder\n\n    Returns:\n        dict: mapping between Synapse Entity name and Id\n    \"\"\"\n    files = syn.getChildren(synid)\n    file_mapping = {\n        _map_name_to_filetype(name=metadata[\"name\"]): metadata[\"id\"]\n        for metadata in files\n    }\n    return file_mapping\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract.get_public_to_consortium_synid_mapping","title":"<code>get_public_to_consortium_synid_mapping(syn, release_synid)</code>","text":"<p>Gets the mapping between potential public release names and the consortium release folder</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>release_synid</code> <p>Release folder fileview</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Mapping between potential public release and consortium   release synapse id</p> <p> TYPE: <code>dict</code> </p> Source code in <code>genie/extract.py</code> <pre><code>def get_public_to_consortium_synid_mapping(\n    syn: synapseclient.Synapse, release_synid: str\n) -&gt; dict:\n    \"\"\"\n    Gets the mapping between potential public release names and\n    the consortium release folder\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        release_synid (str): Release folder fileview\n\n    Returns:\n        dict: Mapping between potential public release and consortium\n              release synapse id\n    \"\"\"\n    # This dict contains the mapping between public release name and\n    # consortium release folder\n    public_to_consortium_map = dict()\n    # release_files = synapseutils.walk(syn, releaseSynId)\n    # TODO: fix the database to mapping table\n    consortium_release_folders = syn.tableQuery(\n        f\"SELECT name, id FROM {release_synid} WHERE \"\n        \"name NOT LIKE 'Release %' \"\n        \"and name NOT LIKE '%-public' \"\n        \"and name NOT IN ('case_lists', 'potential_artifacts')\"\n        \"ORDER BY name\"\n    )\n    consortium_release_folders_df = consortium_release_folders.asDataFrame()\n    # Get major release version\n    consortium_release_folders_df[\"major_release\"] = [\n        release.split(\".\")[0] for release in consortium_release_folders_df[\"name\"]\n    ]\n    # only keep the latest consortium release for the public release\n    consortium_release_folders_df.drop_duplicates(\n        \"major_release\", keep=\"last\", inplace=True\n    )\n\n    for _, release_info in consortium_release_folders_df.iterrows():\n        major_release = release_info[\"major_release\"]\n        # add support for potential patch releases\n        for num in [0, 1, 2, 3]:\n            # This has to exist because the the first three GENIE releases\n            # used semantic versioning\n            if release_info[\"major_release\"] in [\"0\", \"1\", \"2\"]:\n                public_release_name = f\"{int(major_release) + 1}.{num}.0\"\n                public_to_consortium_map[public_release_name] = release_info[\"id\"]\n            else:\n                public_release_name = f\"{major_release}.{num}-public\"\n                public_to_consortium_map[public_release_name] = release_info[\"id\"]\n    return public_to_consortium_map\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract.get_syntabledf","title":"<code>get_syntabledf(syn, query_string)</code>","text":"<p>Get dataframe from Synapse Table query</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>query_string</code> <p>Table query</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: Query results in a dataframe</p> Source code in <code>genie/extract.py</code> <pre><code>def get_syntabledf(syn: synapseclient.Synapse, query_string: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Get dataframe from Synapse Table query\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        query_string (str): Table query\n\n    Returns:\n        pd.DataFrame: Query results in a dataframe\n    \"\"\"\n    table = syn.tableQuery(query_string)\n    tabledf = table.asDataFrame()\n    return tabledf\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract._get_synid_database_mappingdf","title":"<code>_get_synid_database_mappingdf(syn, project_id)</code>","text":"<p>Get database to synapse id mapping dataframe</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>project_id</code> <p>Synapse Project ID with a 'dbMapping' annotation.</p> <p> </p> RETURNS DESCRIPTION <p>database to synapse id mapping dataframe</p> Source code in <code>genie/extract.py</code> <pre><code>def _get_synid_database_mappingdf(syn, project_id):\n    \"\"\"\n    Get database to synapse id mapping dataframe\n\n    Args:\n        syn: Synapse object\n        project_id: Synapse Project ID with a 'dbMapping' annotation.\n\n    Returns:\n        database to synapse id mapping dataframe\n    \"\"\"\n\n    project = syn.get(project_id)\n    database_mapping_synid = project.annotations[\"dbMapping\"][0]\n    database_map_query = f\"SELECT * FROM {database_mapping_synid}\"\n    mappingdf = get_syntabledf(syn, database_map_query)\n    return mappingdf\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract.getDatabaseSynId","title":"<code>getDatabaseSynId(syn, tableName, project_id=None, databaseToSynIdMappingDf=None)</code>","text":"<p>Get database synapse id from database to synapse id mapping table</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>project_id</code> <p>Synapse Project ID with a database mapping table.</p> <p> DEFAULT: <code>None</code> </p> <code>tableName</code> <p>Name of synapse table</p> <p> </p> <code>databaseToSynIdMappingDf</code> <p>Avoid calling rest call to download table                       if the mapping table is already downloaded</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Synapse id of wanted database</p> Source code in <code>genie/extract.py</code> <pre><code>def getDatabaseSynId(syn, tableName, project_id=None, databaseToSynIdMappingDf=None):\n    \"\"\"\n    Get database synapse id from database to synapse id mapping table\n\n    Args:\n        syn: Synapse object\n        project_id: Synapse Project ID with a database mapping table.\n        tableName: Name of synapse table\n        databaseToSynIdMappingDf: Avoid calling rest call to download table\n                                  if the mapping table is already downloaded\n\n    Returns:\n        str:  Synapse id of wanted database\n    \"\"\"\n    if databaseToSynIdMappingDf is None:\n        databaseToSynIdMappingDf = _get_synid_database_mappingdf(\n            syn, project_id=project_id\n        )\n\n    synId = process_functions.lookup_dataframe_value(\n        databaseToSynIdMappingDf, \"Id\", f'Database == \"{tableName}\"'\n    )\n    return synId\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract._get_database_mapping_config","title":"<code>_get_database_mapping_config(syn, synid)</code>","text":"<p>Gets Synapse database to Table mapping in dict</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>synid</code> <p>Synapse id of database mapping table</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>{'databasename': 'synid'}</p> <p> TYPE: <code>dict</code> </p> Source code in <code>genie/extract.py</code> <pre><code>def _get_database_mapping_config(syn: synapseclient.Synapse, synid: str) -&gt; dict:\n    \"\"\"Gets Synapse database to Table mapping in dict\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        synid (str): Synapse id of database mapping table\n\n    Returns:\n        dict: {'databasename': 'synid'}\n    \"\"\"\n    configdf = get_syntabledf(syn=syn, query_string=f\"SELECT * FROM {synid}\")\n    configdf.index = configdf[\"Database\"]\n    config_dict = configdf.to_dict()\n    return config_dict[\"Id\"]\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract.get_genie_config","title":"<code>get_genie_config(syn, project_id)</code>","text":"<p>Get configurations needed for the GENIE codebase</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>project_id</code> <p>Synapse project id</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>GENIE table type/name to Synapse Id</p> <p> TYPE: <code>dict</code> </p> Source code in <code>genie/extract.py</code> <pre><code>def get_genie_config(\n    syn: synapseclient.Synapse,\n    project_id: str,\n) -&gt; dict:\n    \"\"\"Get configurations needed for the GENIE codebase\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        project_id (str): Synapse project id\n\n    Returns:\n        dict: GENIE table type/name to Synapse Id\n    \"\"\"\n    # Get the Synapse Project where data is stored\n    # Should have annotations to find the table lookup\n    project = syn.get(project_id)\n\n    # Get project GENIE configurations\n    database_to_synid_mapping_synid = project.annotations.get(\"dbMapping\", \"\")\n    genie_config = _get_database_mapping_config(\n        syn=syn, synid=database_to_synid_mapping_synid[0]\n    )\n    # Fill in GENIE center configurations\n    center_mapping_id = genie_config[\"centerMapping\"]\n    center_mapping_df = get_syntabledf(\n        syn=syn, query_string=f\"SELECT * FROM {center_mapping_id} where release is true\"\n    )\n    center_mapping_df.index = center_mapping_df.center\n    # Add center configurations including input/staging synapse ids\n    genie_config[\"center_config\"] = center_mapping_df.to_dict(\"index\")\n    return genie_config\n</code></pre>"},{"location":"reference/helper_modules/extract/#genie.extract._get_oncotreelink","title":"<code>_get_oncotreelink(syn, genie_config, oncotree_link=None)</code>","text":"<p>Gets oncotree link unless a link is specified by the user</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>genie_config</code> <p>database name to synid mapping</p> <p> TYPE: <code>dict</code> </p> <code>oncotree_link</code> <p>link to oncotree. Default is None</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>oncotree link</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/extract.py</code> <pre><code>def _get_oncotreelink(\n    syn: synapseclient.Synapse, genie_config: dict, oncotree_link: Optional[str] = None\n) -&gt; str:\n    \"\"\"\n    Gets oncotree link unless a link is specified by the user\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        genie_config (dict): database name to synid mapping\n        oncotree_link (str): link to oncotree. Default is None\n\n    Returns:\n        str: oncotree link\n    \"\"\"\n    if oncotree_link is None:\n        onco_link_ent = syn.get(genie_config[\"oncotreeLink\"])\n        oncotree_link = onco_link_ent.externalURL\n    return oncotree_link\n</code></pre>"},{"location":"reference/helper_modules/load/","title":"load","text":""},{"location":"reference/helper_modules/load/#genie.load","title":"<code>genie.load</code>","text":"<p>This module contains all the functions that stores data to Synapse</p>"},{"location":"reference/helper_modules/load/#genie.load-attributes","title":"Attributes","text":""},{"location":"reference/helper_modules/load/#genie.load.__version__","title":"<code>__version__ = '17.0.0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/load/#genie.load.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/load/#genie.load-functions","title":"Functions","text":""},{"location":"reference/helper_modules/load/#genie.load.store_file","title":"<code>store_file(syn, filepath, parentid, name=None, annotations=None, used=None, version_comment=None)</code>","text":"<p>Stores file into Synapse</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>filepath</code> <p>Path to file</p> <p> TYPE: <code>str</code> </p> <code>parentid</code> <p>Synapse Id of a folder or project</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of entity. Defaults to basename of your file path.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>annotations</code> <p>Synapse annotations to the File Entity. Defaults to None.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>None</code> </p> <code>used</code> <p>Entities used to generate file. Defaults to None.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>version_comment</code> <p>File Entity version comment. Defaults to None.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>File</code> <p>synapseclient.File: Synapse File entity</p> Source code in <code>genie/load.py</code> <pre><code>def store_file(\n    syn: synapseclient.Synapse,\n    filepath: str,\n    parentid: str,\n    name: Optional[str] = None,\n    annotations: Optional[Dict] = None,\n    used: Optional[Union[List[str], str]] = None,\n    version_comment: Optional[str] = None,\n) -&gt; synapseclient.File:\n    \"\"\"Stores file into Synapse\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        filepath (str): Path to file\n        parentid (str): Synapse Id of a folder or project\n        name (str, optional): Name of entity. Defaults to basename of your file path.\n        annotations (Dict, optional): Synapse annotations to the File Entity. Defaults to None.\n        used (List[str], optional): Entities used to generate file. Defaults to None.\n        version_comment (str, optional): File Entity version comment. Defaults to None.\n\n    Returns:\n        synapseclient.File: Synapse File entity\n    \"\"\"\n    file_ent = synapseclient.File(\n        filepath, parentId=parentid, versionComment=version_comment\n    )\n    if name is not None:\n        file_ent.name = name\n    if annotations is not None:\n        file_ent.annotations = annotations\n    file_ent = syn.store(\n        file_ent,\n        used=used,\n        executed=f\"https://github.com/Sage-Bionetworks/Genie/tree/v{__version__}\",\n    )\n    return file_ent\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load.store_files","title":"<code>store_files(syn, filepaths, parentid)</code>","text":"<p>Stores a list of files</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>filepaths</code> <p>List of filepaths</p> <p> TYPE: <code>List[str]</code> </p> <code>parentid</code> <p>Synapse Id of a folder or project</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[File]</code> <p>List[synapseclient.File]: List of Synaps File entities</p> Source code in <code>genie/load.py</code> <pre><code>def store_files(\n    syn: synapseclient.Synapse, filepaths: List[str], parentid: str\n) -&gt; List[synapseclient.File]:\n    \"\"\"Stores a list of files\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        filepaths (List[str]): List of filepaths\n        parentid (str): Synapse Id of a folder or project\n\n    Returns:\n        List[synapseclient.File]: List of Synaps File entities\n    \"\"\"\n    file_entities = [\n        store_file(syn=syn, filepath=path, parentid=parentid) for path in filepaths\n    ]\n    return file_entities\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load.store_table","title":"<code>store_table(syn, filepath, tableid)</code>","text":"<p>Stores a tsv to a Synapse Table.  This can append, update, and delete rows in a Synapse table depending on how the tsv file is formatted.</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>filepath</code> <p>Path to a TSV</p> <p> TYPE: <code>str</code> </p> <code>tableid</code> <p>Synapse Id of a Synapse Table</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/load.py</code> <pre><code>def store_table(syn: synapseclient.Synapse, filepath: str, tableid: str):\n    \"\"\"Stores a tsv to a Synapse Table.  This can append, update, and delete\n    rows in a Synapse table depending on how the tsv file is formatted.\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        filepath (str): Path to a TSV\n        tableid (str): Synapse Id of a Synapse Table\n\n    \"\"\"\n    try:\n        update_table = synapseclient.Table(tableid, filepath, separator=\"\\t\")\n        syn.store(update_table)\n    except SynapseTimeoutError:\n        # This error occurs because of waiting for table to index.\n        # Don't worry about this error\n        pass\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load.update_process_trackingdf","title":"<code>update_process_trackingdf(syn, process_trackerdb_synid, center, process_type, start=True)</code>","text":"<p>Updates the processing tracking database</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>process_trackerdb_synid</code> <p>Synapse Id of Process Tracking Table</p> <p> TYPE: <code>str</code> </p> <code>center</code> <p>GENIE center (ie. SAGE)</p> <p> TYPE: <code>str</code> </p> <code>process_type</code> <p>processing type (dbToStage or public)</p> <p> TYPE: <code>str</code> </p> <code>start</code> <p>Start or end of processing.  Default is True for start</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>genie/load.py</code> <pre><code>def update_process_trackingdf(\n    syn: synapseclient.Synapse,\n    process_trackerdb_synid: str,\n    center: str,\n    process_type: str,\n    start: bool = True,\n):\n    \"\"\"\n    Updates the processing tracking database\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        process_trackerdb_synid: Synapse Id of Process Tracking Table\n        center: GENIE center (ie. SAGE)\n        process_type: processing type (dbToStage or public)\n        start: Start or end of processing.  Default is True for start\n    \"\"\"\n    logger.info(\"UPDATE PROCESS TRACKING TABLE\")\n    column = \"timeStartProcessing\" if start else \"timeEndProcessing\"\n    query_str = (\n        f\"SELECT {column} FROM {process_trackerdb_synid} where center = '{center}' \"\n        f\"and processingType = '{process_type}'\"\n    )\n    process_trackerdf = extract.get_syntabledf(syn=syn, query_string=query_str)\n    process_trackerdf[column].iloc[0] = str(int(time.time() * 1000))\n    syn.store(synapseclient.Table(process_trackerdb_synid, process_trackerdf))\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load.update_table","title":"<code>update_table(syn, databaseSynId, newData, filterBy, filterByColumn='CENTER', col=None, toDelete=False)</code>","text":"<p>Update Synapse table given a new dataframe</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>databaseSynId</code> <p>Synapse Id of Synapse Table</p> <p> TYPE: <code>str</code> </p> <code>newData</code> <p>New data in a dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>filterBy</code> <p>Value to filter new data by</p> <p> TYPE: <code>str</code> </p> <code>filterByColumn</code> <p>Column to filter values by. Defaults to \"CENTER\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'CENTER'</code> </p> <code>col</code> <p>List of columns to ingest. Defaults to None.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>toDelete</code> <p>Delete rows given the primary key. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>genie/load.py</code> <pre><code>def update_table(\n    syn: synapseclient.Synapse,\n    databaseSynId: str,\n    newData: pd.DataFrame,\n    filterBy: str,\n    filterByColumn: str = \"CENTER\",\n    col: Optional[List[str]] = None,\n    toDelete: bool = False,\n):\n    \"\"\"Update Synapse table given a new dataframe\n\n    Args:\n        syn (Synapse): Synapse connection\n        databaseSynId (str): Synapse Id of Synapse Table\n        newData (pd.DataFrame): New data in a dataframe\n        filterBy (str): Value to filter new data by\n        filterByColumn (str, optional): Column to filter values by. Defaults to \"CENTER\".\n        col (List[str], optional): List of columns to ingest. Defaults to None.\n        toDelete (bool, optional): Delete rows given the primary key. Defaults to False.\n    \"\"\"\n    databaseEnt = syn.get(databaseSynId)\n    database = syn.tableQuery(\n        f\"SELECT * FROM {databaseSynId} where {filterByColumn} ='{filterBy}'\"\n    )\n    database = database.asDataFrame()\n    db_cols = set(database.columns)\n    if col is not None:\n        new_data_cols = set(col)\n        # Make sure columns from file exists in database columns\n        use_cols = db_cols.intersection(new_data_cols)\n        # No need to fail, because there is bound to be at least one\n        # column that will exist in the database\n        database = database[list(use_cols)]\n    else:\n        newData = newData[database.columns]\n    _update_table(\n        syn=syn,\n        database=database,\n        new_dataset=newData,\n        database_synid=databaseSynId,\n        primary_key_cols=databaseEnt.primaryKey,\n        to_delete=toDelete,\n    )\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load._update_table","title":"<code>_update_table(syn, database, new_dataset, database_synid, primary_key_cols, to_delete=False)</code>","text":"<p>A helper function to compare new dataset with existing data, and store any changes that need to be made to the database</p> Source code in <code>genie/load.py</code> <pre><code>def _update_table(\n    syn: synapseclient.Synapse,\n    database: pd.DataFrame,\n    new_dataset: pd.DataFrame,\n    database_synid: str,\n    primary_key_cols: List[str],\n    to_delete: bool = False,\n):\n    \"\"\"\n    A helper function to compare new dataset with existing data,\n    and store any changes that need to be made to the database\n    \"\"\"\n    changes = check_database_changes(database, new_dataset, primary_key_cols, to_delete)\n    store_database(\n        syn,\n        database_synid,\n        changes[\"col_order\"],\n        changes[\"allupdates\"],\n        changes[\"to_delete_rows\"],\n    )\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load._get_col_order","title":"<code>_get_col_order(orig_database_cols)</code>","text":"<p>Get column order</p> PARAMETER DESCRIPTION <code>orig_database_cols</code> <p>A list of column names of the original database</p> <p> TYPE: <code>Index</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>The list of re-ordered column names</p> Source code in <code>genie/load.py</code> <pre><code>def _get_col_order(orig_database_cols: pd.Index) -&gt; List[str]:\n    \"\"\"\n    Get column order\n\n    Args:\n        orig_database_cols (pd.Index): A list of column names of the original database\n\n    Returns:\n        The list of re-ordered column names\n    \"\"\"\n    col_order = [\"ROW_ID\", \"ROW_VERSION\"]\n    col_order.extend(orig_database_cols.tolist())\n    return col_order\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load._reorder_new_dataset","title":"<code>_reorder_new_dataset(orig_database_cols, new_dataset)</code>","text":"<p>Reorder new dataset based on the original database</p> PARAMETER DESCRIPTION <code>orig_database_cols</code> <p>A list of column names of the original database</p> <p> TYPE: <code>Index</code> </p> <code>new_dataset</code> <p>New Data</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The re-ordered new dataset</p> Source code in <code>genie/load.py</code> <pre><code>def _reorder_new_dataset(\n    orig_database_cols: pd.Index, new_dataset: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Reorder new dataset based on the original database\n\n    Args:\n        orig_database_cols (pd.Index): A list of column names of the original database\n        new_dataset(pd.DataFrame): New Data\n\n    Returns:\n        The re-ordered new dataset\n    \"\"\"\n    # Columns must be in the same order as the original data\n    new_dataset = new_dataset[orig_database_cols]\n    return new_dataset\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load._generate_primary_key","title":"<code>_generate_primary_key(dataset, primary_key_cols, primary_key)</code>","text":"<p>Generate primary key column a dataframe</p> PARAMETER DESCRIPTION <code>dataset</code> <p>A dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>primary_key_cols</code> <p>Column(s) that make up the primary key</p> <p> TYPE: <code>List[str]</code> </p> <code>primary_key</code> <p>The column name of the primary_key</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: The dataframe with primary_key column added</p> Source code in <code>genie/load.py</code> <pre><code>def _generate_primary_key(\n    dataset: pd.DataFrame, primary_key_cols: List[str], primary_key: str\n) -&gt; pd.DataFrame:\n    \"\"\"Generate primary key column a dataframe\n\n    Args:\n        dataset (pd.DataFrame): A dataframe\n        primary_key_cols (List[str]): Column(s) that make up the primary key\n        primary_key (str): The column name of the primary_key\n\n    Returns:\n        pd.DataFrame: The dataframe with primary_key column added\n    \"\"\"\n    # replace NAs with emtpy string\n    dataset = dataset.fillna(\"\")\n    # generate primary key column for original database\n    dataset[primary_key_cols] = dataset[primary_key_cols].applymap(str)\n    if dataset.empty:\n        dataset[primary_key] = \"\"\n    else:\n        dataset[primary_key] = dataset[primary_key_cols].apply(\n            lambda x: \" \".join(x), axis=1\n        )\n    return dataset\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load.check_database_changes","title":"<code>check_database_changes(database, new_dataset, primary_key_cols, to_delete=False)</code>","text":"<p>Check changes that need to be made, i.e. append/update/delete rows to the database based on its comparison with new data</p> PARAMETER DESCRIPTION <code>database</code> <p>Original Data</p> <p> TYPE: <code>DataFrame</code> </p> <code>new_dataset</code> <p>New Data</p> <p> TYPE: <code>DataFrame</code> </p> <code>primary_key_cols</code> <p>Column(s) that make up the primary key</p> <p> TYPE: <code>list</code> </p> <code>to_delete</code> <p>Delete rows. Defaults to False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>genie/load.py</code> <pre><code>def check_database_changes(\n    database: pd.DataFrame,\n    new_dataset: pd.DataFrame,\n    primary_key_cols: List[str],\n    to_delete: bool = False,\n) -&gt; Dict[pd.DataFrame, List[str]]:\n    \"\"\"\n    Check changes that need to be made, i.e. append/update/delete rows to the database\n    based on its comparison with new data\n\n    Args:\n        database (pd.DataFrame): Original Data\n        new_dataset (pd.DataFrame): New Data\n        primary_key_cols (list): Column(s) that make up the primary key\n        to_delete (bool, optional): Delete rows. Defaults to False\n    \"\"\"\n    # get a list of column names of the original database\n    orig_database_cols = database.columns\n    # get the final column order\n    col_order = _get_col_order(orig_database_cols)\n    # reorder new_dataset\n    new_dataset = _reorder_new_dataset(orig_database_cols, new_dataset)\n    # set the primary_key name\n    primary_key = \"UNIQUE_KEY\"\n    # generate primary_key column for dataset comparison\n    ori_data = _generate_primary_key(database, primary_key_cols, primary_key)\n    new_data = _generate_primary_key(new_dataset, primary_key_cols, primary_key)\n    # output dictionary\n    changes = {\"col_order\": col_order, \"allupdates\": None, \"to_delete_rows\": None}\n    # get rows to be appened or updated\n    allupdates = pd.DataFrame(columns=col_order)\n    to_append_rows = process_functions._append_rows(new_data, ori_data, primary_key)\n    to_update_rows = process_functions._update_rows(new_data, ori_data, primary_key)\n    allupdates = pd.concat([allupdates, to_append_rows, to_update_rows], sort=False)\n    changes[\"allupdates\"] = allupdates\n    # get rows to be deleted\n    if to_delete:\n        to_delete_rows = process_functions._delete_rows(new_data, ori_data, primary_key)\n    else:\n        to_delete_rows = pd.DataFrame()\n    changes[\"to_delete_rows\"] = to_delete_rows\n    return changes\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load.store_database","title":"<code>store_database(syn, database_synid, col_order, all_updates, to_delete_rows)</code>","text":"<p>Store changes to the database</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>database_synid</code> <p>Synapse Id of the Synapse table</p> <p> TYPE: <code>str</code> </p> <code>col_order</code> <p>The ordered column names to be saved</p> <p> TYPE: <code>List[str]</code> </p> <code>all_updates</code> <p>rows to be appended and/or updated</p> <p> TYPE: <code>DataFrame</code> </p> <code>to_deleted_rows</code> <p>rows to be deleted</p> <p> TYPE: <code>DataFrame</code> </p> Source code in <code>genie/load.py</code> <pre><code>def store_database(\n    syn: synapseclient.Synapse,\n    database_synid: str,\n    col_order: List[str],\n    all_updates: pd.DataFrame,\n    to_delete_rows: pd.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Store changes to the database\n\n    Args:\n        syn (synapseclient.Synapse): Synapse object\n        database_synid (str): Synapse Id of the Synapse table\n        col_order (List[str]): The ordered column names to be saved\n        all_updates (pd.DataFrame): rows to be appended and/or updated\n        to_deleted_rows (pd.DataFrame): rows to be deleted\n    \"\"\"\n    storedatabase = False\n    update_all_file = tempfile.NamedTemporaryFile(\n        dir=process_functions.SCRIPT_DIR, delete=False\n    )\n    with open(update_all_file.name, \"w\") as updatefile:\n        # Must write out the headers in case there are no appends or updates\n        updatefile.write(\",\".join(col_order) + \"\\n\")\n        if not all_updates.empty:\n            \"\"\"\n            This is done because of pandas typing.\n            An integer column with one NA/blank value\n            will be cast as a double.\n            \"\"\"\n            updatefile.write(\n                all_updates[col_order]\n                .to_csv(index=False, header=None)\n                .replace(\".0,\", \",\")\n                .replace(\".0\\n\", \"\\n\")\n            )\n            storedatabase = True\n        if not to_delete_rows.empty:\n            updatefile.write(\n                to_delete_rows.to_csv(index=False, header=None)\n                .replace(\".0,\", \",\")\n                .replace(\".0\\n\", \"\\n\")\n            )\n            storedatabase = True\n    if storedatabase:\n        syn.store(synapseclient.Table(database_synid, update_all_file.name))\n    # Delete the update file\n    os.unlink(update_all_file.name)\n</code></pre>"},{"location":"reference/helper_modules/load/#genie.load._copyRecursive","title":"<code>_copyRecursive(syn, entity, destinationId, mapping=None, skipCopyAnnotations=False, **kwargs)</code>","text":"<p>NOTE: This is a copy of the function found here: https://github.com/Sage-Bionetworks/synapsePythonClient/blob/develop/synapseutils/copy_functions.py#L409 This was copied because there is a restriction that doesn't allow for copying entities with access requirements</p> <p>Recursively copies synapse entites, but does not copy the wikis</p> PARAMETER DESCRIPTION <code>syn</code> <p>A Synapse object with user's login</p> <p> TYPE: <code>Synapse</code> </p> <code>entity</code> <p>A synapse entity ID</p> <p> TYPE: <code>str</code> </p> <code>destinationId</code> <p>Synapse ID of a folder/project that the copied entity is being copied to</p> <p> TYPE: <code>str</code> </p> <code>mapping</code> <p>A mapping of the old entities to the new entities</p> <p> TYPE: <code>Dict[str, str]</code> DEFAULT: <code>None</code> </p> <code>skipCopyAnnotations</code> <p>Skips copying the annotations                     Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>a mapping between the original and copied entity: {'syn1234':'syn33455'}</p> Source code in <code>genie/load.py</code> <pre><code>def _copyRecursive(\n    syn: synapseclient.Synapse,\n    entity: str,\n    destinationId: str,\n    mapping: Dict[str, str] = None,\n    skipCopyAnnotations: bool = False,\n    **kwargs,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    NOTE: This is a copy of the function found here: https://github.com/Sage-Bionetworks/synapsePythonClient/blob/develop/synapseutils/copy_functions.py#L409\n    This was copied because there is a restriction that doesn't allow for copying entities with access requirements\n\n    Recursively copies synapse entites, but does not copy the wikis\n\n    Arguments:\n        syn: A Synapse object with user's login\n        entity: A synapse entity ID\n        destinationId: Synapse ID of a folder/project that the copied entity is being copied to\n        mapping: A mapping of the old entities to the new entities\n        skipCopyAnnotations: Skips copying the annotations\n                                Default is False\n\n    Returns:\n        a mapping between the original and copied entity: {'syn1234':'syn33455'}\n    \"\"\"\n\n    version = kwargs.get(\"version\", None)\n    setProvenance = kwargs.get(\"setProvenance\", \"traceback\")\n    excludeTypes = kwargs.get(\"excludeTypes\", [])\n    updateExisting = kwargs.get(\"updateExisting\", False)\n    if mapping is None:\n        mapping = dict()\n    # Check that passed in excludeTypes is file, table, and link\n    if not isinstance(excludeTypes, list):\n        raise ValueError(\"Excluded types must be a list\")\n    elif not all([i in [\"file\", \"link\", \"table\"] for i in excludeTypes]):\n        raise ValueError(\n            \"Excluded types can only be a list of these values: file, table, and link\"\n        )\n\n    ent = syn.get(entity, downloadFile=False)\n    if ent.id == destinationId:\n        raise ValueError(\"destinationId cannot be the same as entity id\")\n\n    if (isinstance(ent, Project) or isinstance(ent, Folder)) and version is not None:\n        raise ValueError(\"Cannot specify version when copying a project of folder\")\n\n    if not isinstance(ent, (Project, Folder, File, Link, Schema, Entity)):\n        raise ValueError(\"Not able to copy this type of file\")\n\n    permissions = syn.restGET(\"/entity/{}/permissions\".format(ent.id))\n    # Don't copy entities without DOWNLOAD permissions\n    if not permissions[\"canDownload\"]:\n        syn.logger.warning(\n            \"%s not copied - this file lacks download permission\" % ent.id\n        )\n        return mapping\n\n    # HACK: These lines of code were removed to allow for data with access requirements to be copied\n    # https://github.com/Sage-Bionetworks/synapsePythonClient/blob/2909fa778e814f62f6fe6ce2d951ce58c0080a4e/synapseutils/copy_functions.py#L464-L470\n\n    copiedId = None\n\n    if isinstance(ent, Project):\n        project = syn.get(destinationId)\n        if not isinstance(project, Project):\n            raise ValueError(\n                \"You must give a destinationId of a new project to copy projects\"\n            )\n        copiedId = destinationId\n        # Projects include Docker repos, and Docker repos cannot be copied\n        # with the Synapse rest API. Entity views currently also aren't\n        # supported\n        entities = syn.getChildren(\n            entity, includeTypes=[\"folder\", \"file\", \"table\", \"link\"]\n        )\n        for i in entities:\n            mapping = _copyRecursive(\n                syn,\n                i[\"id\"],\n                destinationId,\n                mapping=mapping,\n                skipCopyAnnotations=skipCopyAnnotations,\n                **kwargs,\n            )\n\n        if not skipCopyAnnotations:\n            project.annotations = ent.annotations\n            syn.store(project)\n    elif isinstance(ent, Folder):\n        copiedId = synu.copy_functions._copyFolder(\n            syn,\n            ent.id,\n            destinationId,\n            mapping=mapping,\n            skipCopyAnnotations=skipCopyAnnotations,\n            **kwargs,\n        )\n    elif isinstance(ent, File) and \"file\" not in excludeTypes:\n        copiedId = synu.copy_functions._copyFile(\n            syn,\n            ent.id,\n            destinationId,\n            version=version,\n            updateExisting=updateExisting,\n            setProvenance=setProvenance,\n            skipCopyAnnotations=skipCopyAnnotations,\n        )\n    elif isinstance(ent, Link) and \"link\" not in excludeTypes:\n        copiedId = synu.copy_functions._copyLink(\n            syn, ent.id, destinationId, updateExisting=updateExisting\n        )\n    elif isinstance(ent, Schema) and \"table\" not in excludeTypes:\n        copiedId = synu.copy_functions._copyTable(\n            syn, ent.id, destinationId, updateExisting=updateExisting\n        )\n    # This is currently done because copyLink returns None sometimes\n    if copiedId is not None:\n        mapping[ent.id] = copiedId\n        syn.logger.info(\"Copied %s to %s\" % (ent.id, copiedId))\n    else:\n        syn.logger.info(\"%s not copied\" % ent.id)\n    return mapping\n</code></pre>"},{"location":"reference/helper_modules/process_functions/","title":"process_functions","text":""},{"location":"reference/helper_modules/process_functions/#genie.process_functions","title":"<code>genie.process_functions</code>","text":"<p>Processing functions that are used in the GENIE pipeline</p>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions-attributes","title":"Attributes","text":""},{"location":"reference/helper_modules/process_functions/#genie.process_functions.__version__","title":"<code>__version__ = '17.0.0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/process_functions/#genie.process_functions.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/process_functions/#genie.process_functions.SCRIPT_DIR","title":"<code>SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/process_functions/#genie.process_functions-functions","title":"Functions","text":""},{"location":"reference/helper_modules/process_functions/#genie.process_functions.to_unix_epoch_time_utc","title":"<code>to_unix_epoch_time_utc(dt)</code>","text":"<p>Wrapper for Synapse to_unix_epoch_time that forces UTC tzinfo</p> PARAMETER DESCRIPTION <code>dt</code> <p>input datetime time object</p> <p> TYPE: <code>Union[date, datetime, str]</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Converted UTC datetime object to UNIX time</p> <p> TYPE: <code>int</code> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def to_unix_epoch_time_utc(dt: Union[datetime.date, datetime.datetime, str]) -&gt; int:\n    \"\"\"Wrapper for Synapse to_unix_epoch_time that forces UTC tzinfo\n\n    Args:\n        dt (Union[datetime.date, datetime.datetime, str]): input datetime time object\n\n    Returns:\n        int: Converted UTC datetime object to UNIX time\n    \"\"\"\n    return to_unix_epoch_time(dt.replace(tzinfo=datetime.timezone.utc))\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.get_clinical_dataframe","title":"<code>get_clinical_dataframe(filePathList)</code>","text":"<p>Gets the clinical file(s) and reads them in as a dataframe</p> PARAMETER DESCRIPTION <code>filePathList</code> <p>List of clinical files</p> <p> TYPE: <code>list</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>when PATIENT_ID column doesn't exist</p> <code>ValueError</code> <p>When PATIENT_IDs in sample file doesn't exist in patient file</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: clinical file as a dataframe</p> Source code in <code>genie/process_functions.py</code> <pre><code>def get_clinical_dataframe(filePathList: list) -&gt; pd.DataFrame:\n    \"\"\"Gets the clinical file(s) and reads them in as a\n    dataframe\n\n    Args:\n        filePathList (list): List of clinical files\n\n    Raises:\n        ValueError: when PATIENT_ID column doesn't exist\n        ValueError: When PATIENT_IDs in sample file doesn't exist in patient file\n\n    Returns:\n        pd.DataFrame: clinical file as a dataframe\n    \"\"\"\n    clinicaldf = pd.read_csv(filePathList[0], sep=\"\\t\", comment=\"#\")\n    clinicaldf.columns = [col.upper() for col in clinicaldf.columns]\n\n    if len(filePathList) &gt; 1:\n        other_clinicaldf = pd.read_csv(filePathList[1], sep=\"\\t\", comment=\"#\")\n        other_clinicaldf.columns = [col.upper() for col in other_clinicaldf.columns]\n\n        try:\n            clinicaldf = clinicaldf.merge(other_clinicaldf, on=\"PATIENT_ID\")\n        except Exception:\n            raise ValueError(\n                (\n                    \"If submitting separate patient and sample files, \"\n                    \"they both must have the PATIENT_ID column\"\n                )\n            )\n        # Must figure out which is sample and which is patient\n        if \"sample\" in filePathList[0]:\n            sample = clinicaldf\n            patient = other_clinicaldf\n        else:\n            sample = other_clinicaldf\n            patient = clinicaldf\n\n        if not all(sample[\"PATIENT_ID\"].isin(patient[\"PATIENT_ID\"])):\n            raise ValueError(\n                (\n                    \"Patient Clinical File: All samples must have associated \"\n                    \"patient information\"\n                )\n            )\n\n    return clinicaldf\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.get_assay_dataframe","title":"<code>get_assay_dataframe(filepath_list)</code>","text":"<p>Reads in assay_information.yaml file     and outputs it as a dataframe</p> PARAMETER DESCRIPTION <code>filepath_list</code> <p>list of files</p> <p> TYPE: <code>list</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>thrown if read error with file</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: dataframe version of assay info file</p> Source code in <code>genie/process_functions.py</code> <pre><code>def get_assay_dataframe(filepath_list: list) -&gt; pd.DataFrame:\n    \"\"\"Reads in assay_information.yaml file\n        and outputs it as a dataframe\n\n    Args:\n        filepath_list (list): list of files\n\n    Raises:\n        ValueError: thrown if read error with file\n\n    Returns:\n        pd.DataFrame: dataframe version of assay info file\n    \"\"\"\n    filepath = filepath_list[0]\n    try:\n        with open(filepath, \"r\") as yamlfile:\n            # https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation\n            # Must add this because yaml load deprecation\n            assay_info_dict = yaml.safe_load(yamlfile)\n    except Exception:\n        raise ValueError(\n            \"assay_information.yaml: Can't read in your file. \"\n            \"Please make sure the file is a correctly formatted yaml\"\n        )\n    # assay_info_df = pd.DataFrame(panel_info_dict)\n    # assay_info_df = assay_info_df.transpose()\n    # assay_info_df['SEQ_ASSAY_ID'] = assay_info_df.index\n    # assay_info_df.reset_index(drop=True, inplace=True)\n    assay_infodf = pd.DataFrame(assay_info_dict)\n    assay_info_transposeddf = assay_infodf.transpose()\n\n    all_panel_info = pd.DataFrame()\n    for assay in assay_info_dict:\n        assay_specific_info = assay_info_dict[assay][\"assay_specific_info\"]\n        assay_specific_infodf = pd.DataFrame(assay_specific_info)\n\n        intial_seq_id_infodf = assay_info_transposeddf.loc[[assay]]\n\n        # make sure to create a skeleton for the number of seq assay ids\n        # in the seq pipeline\n        seq_assay_id_infodf = pd.concat(\n            [intial_seq_id_infodf] * len(assay_specific_info)\n        )\n        seq_assay_id_infodf.reset_index(drop=True, inplace=True)\n        assay_finaldf = pd.concat([assay_specific_infodf, seq_assay_id_infodf], axis=1)\n        del assay_finaldf[\"assay_specific_info\"]\n        # Transform values containing lists to string concatenated values\n        columns_containing_lists = [\n            \"variant_classifications\",\n            \"alteration_types\",\n            \"preservation_technique\",\n            \"coverage\",\n        ]\n\n        for col in columns_containing_lists:\n            if assay_finaldf.get(col) is not None:\n                assay_finaldf[col] = [\";\".join(row) for row in assay_finaldf[col]]\n        assay_finaldf[\"SEQ_PIPELINE_ID\"] = assay\n        all_panel_info = pd.concat([all_panel_info, assay_finaldf])\n    return all_panel_info\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.retry_get_url","title":"<code>retry_get_url(url)</code>","text":"<p>Implement retry logic when getting urls. Timesout at 3 seconds, retries 5 times.</p> PARAMETER DESCRIPTION <code>url</code> <p>Http or https url</p> <p> </p> RETURNS DESCRIPTION <p>requests.get()</p> Source code in <code>genie/process_functions.py</code> <pre><code>def retry_get_url(url):\n    \"\"\"\n    Implement retry logic when getting urls.\n    Timesout at 3 seconds, retries 5 times.\n\n    Args:\n        url:  Http or https url\n\n    Returns:\n        requests.get()\n    \"\"\"\n    s = requests.Session()\n    retries = Retry(total=5, backoff_factor=1)\n    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n    response = s.get(url, timeout=3)\n    return response\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.checkUrl","title":"<code>checkUrl(url)</code>","text":"<p>Check if URL link is live</p> PARAMETER DESCRIPTION <code>url</code> <p>web URL</p> <p> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def checkUrl(url):\n    \"\"\"\n    Check if URL link is live\n\n    Args:\n        url: web URL\n    \"\"\"\n    temp = retry_get_url(url)\n    assert temp.status_code == 200, \"%s site is down\" % url\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.checkColExist","title":"<code>checkColExist(DF, key)</code>","text":"<p>This function checks if the column(s) exist(s) in a dataframe</p> PARAMETER DESCRIPTION <code>DF</code> <p>pandas dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>key</code> <p>Expected column header name(s)</p> <p> TYPE: <code>Union[str, int, list]</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if column(s) exist(s)</p> <p> TYPE: <code>bool</code> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def checkColExist(DF: pd.DataFrame, key: Union[str, int, list]) -&gt; bool:\n    \"\"\"\n    This function checks if the column(s) exist(s) in a dataframe\n\n    Args:\n        DF: pandas dataframe\n        key: Expected column header name(s)\n\n    Returns:\n        bool:  True if column(s) exist(s)\n    \"\"\"\n    result = False if DF.get(key) is None else True\n    return result\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.validate_genie_identifier","title":"<code>validate_genie_identifier(identifiers, center, filename, col)</code>","text":"<p>Validate GENIE sample and patient ids.</p> PARAMETER DESCRIPTION <code>identifiers</code> <p>Array of GENIE identifiers</p> <p> TYPE: <code>Series</code> </p> <code>center</code> <p>GENIE center name</p> <p> TYPE: <code>str</code> </p> <code>filename</code> <p>name of file</p> <p> TYPE: <code>str</code> </p> <code>col</code> <p>Column with identifiers</p> <p> TYPE: <code>str</code> </p> return <p>str: Errors</p> Source code in <code>genie/process_functions.py</code> <pre><code>def validate_genie_identifier(\n    identifiers: pd.Series, center: str, filename: str, col: str\n) -&gt; str:\n    \"\"\"Validate GENIE sample and patient ids.\n\n    Args:\n        identifiers (pd.Series): Array of GENIE identifiers\n        center (str): GENIE center name\n        filename (str): name of file\n        col (str): Column with identifiers\n\n    return:\n        str: Errors\n    \"\"\"\n    total_error = \"\"\n    if not all(identifiers.str.startswith(f\"GENIE-{center}\")):\n        total_error = total_error + (\n            f\"{filename}: {col} must start with GENIE-{center}\\n\"\n        )\n    if any(identifiers.str.len() &gt;= 50):\n        total_error = total_error + (\n            f\"{filename}: {col} must have less than 50 characters.\\n\"\n        )\n    return total_error\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.lookup_dataframe_value","title":"<code>lookup_dataframe_value(df, col, query)</code>","text":"<p>Look up dataframe value given query and column</p> PARAMETER DESCRIPTION <code>df</code> <p>dataframe</p> <p> </p> <code>col</code> <p>column with value to return</p> <p> </p> <code>query</code> <p>Query for specific column</p> <p> </p> RETURNS DESCRIPTION <p>value</p> Source code in <code>genie/process_functions.py</code> <pre><code>def lookup_dataframe_value(df, col, query):\n    \"\"\"\n    Look up dataframe value given query and column\n\n    Args:\n        df: dataframe\n        col: column with value to return\n        query: Query for specific column\n\n    Returns:\n        value\n    \"\"\"\n    query = df.query(query)\n    query_val = query[col].iloc[0]\n    return query_val\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.rmFiles","title":"<code>rmFiles(folderPath, recursive=True)</code>","text":"<p>Convenience function to remove all files in dir</p> PARAMETER DESCRIPTION <code>folderPath</code> <p>Path to folder</p> <p> </p> <code>recursive</code> <p>Removes all files recursively</p> <p> DEFAULT: <code>True</code> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def rmFiles(folderPath, recursive=True):\n    \"\"\"\n    Convenience function to remove all files in dir\n\n    Args:\n        folderPath: Path to folder\n        recursive:  Removes all files recursively\n    \"\"\"\n    for dirPath, dirNames, filePaths in os.walk(folderPath):\n        for filePath in filePaths:\n            os.unlink(os.path.join(dirPath, filePath))\n        if not recursive:\n            break\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.removeStringFloat","title":"<code>removeStringFloat(string)</code>","text":"<p>remove string float in tsv file</p> PARAMETER DESCRIPTION <code>string</code> <p>tsv file in string format</p> <p> </p> Return <p>string: string with float removed</p> Source code in <code>genie/process_functions.py</code> <pre><code>def removeStringFloat(string):\n    \"\"\"\n    remove string float in tsv file\n\n    Args:\n        string: tsv file in string format\n\n    Return:\n        string: string with float removed\n    \"\"\"\n    string = string.replace(\".0\\t\", \"\\t\")\n    string = string.replace(\".0\\n\", \"\\n\")\n    return string\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.removePandasDfFloat","title":"<code>removePandasDfFloat(df, header=True)</code>","text":"<p>Remove decimal for integers due to pandas</p> PARAMETER DESCRIPTION <code>df</code> <p>Pandas dataframe</p> <p> </p> Return <p>str: tsv in text</p> Source code in <code>genie/process_functions.py</code> <pre><code>def removePandasDfFloat(df, header=True):\n    \"\"\"\n    Remove decimal for integers due to pandas\n\n    Args:\n        df:  Pandas dataframe\n\n    Return:\n        str: tsv in text\n    \"\"\"\n    if header:\n        text = df.to_csv(sep=\"\\t\", index=False)\n    else:\n        text = df.to_csv(sep=\"\\t\", index=False, header=None)\n\n    text = removeStringFloat(text)\n    return text\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.removeFloat","title":"<code>removeFloat(df)</code>","text":"<p>Need to remove this function as it calls another function</p> Source code in <code>genie/process_functions.py</code> <pre><code>def removeFloat(df):\n    \"\"\"\n    Need to remove this function\n    as it calls another function\n    \"\"\"\n    # text = df.to_csv(sep=\"\\t\",index=False)\n    # text = text.replace(\".0\\t\",\"\\t\")\n    # text = text.replace(\".0\\n\",\"\\n\")\n    text = removePandasDfFloat(df)\n    return text\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.checkGenieId","title":"<code>checkGenieId(ID, center)</code>","text":"<p>Checks if GENIE ID is labelled correctly and reformats the GENIE ID</p> PARAMETER DESCRIPTION <code>ID</code> <p>string</p> <p> </p> <code>center</code> <p>GENIE center</p> <p> </p> Return <p>str: Formatted GENIE ID string</p> Source code in <code>genie/process_functions.py</code> <pre><code>def checkGenieId(ID, center):\n    \"\"\"\n    Checks if GENIE ID is labelled correctly\n    and reformats the GENIE ID\n\n    Args:\n        ID: string\n        center: GENIE center\n\n    Return:\n        str: Formatted GENIE ID string\n    \"\"\"\n    if str(ID).startswith(\"%s-\" % center):\n        return \"GENIE-%s\" % str(ID)\n    elif not str(ID).startswith(\"GENIE-%s-\" % center):\n        return \"GENIE-%s-%s\" % (center, str(ID))\n    else:\n        return str(ID)\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.seqDateFilter","title":"<code>seqDateFilter(clinicalDf, processingDate, days)</code>","text":"<p>SEQ_DATE filter SEQ_DATE - Clinical data (6 and 12 as parameters) Jan-2017 , given processing date (today) -&gt;     staging release (processing date - Jan-2017 &lt; 6 months) July-2016 , given processing date (today) -&gt;     consortium release (processing date - July-2016 between     6 months - 12 months)</p> Source code in <code>genie/process_functions.py</code> <pre><code>def seqDateFilter(clinicalDf, processingDate, days):\n    \"\"\"\n    SEQ_DATE filter\n    SEQ_DATE - Clinical data (6 and 12 as parameters)\n    Jan-2017 , given processing date (today) -&gt;\n        staging release (processing date - Jan-2017 &lt; 6 months)\n    July-2016 , given processing date (today) -&gt;\n        consortium release (processing date - July-2016 between\n        6 months - 12 months)\n\n    \"\"\"\n    copyClinicalDf = clinicalDf.copy()\n    # copyClinicalDf['SEQ_DATE'][copyClinicalDf['SEQ_DATE'].astype(str) == '999'] = \"Jan-1988\"\n    # copyClinicalDf['SEQ_DATE'][copyClinicalDf['SEQ_DATE'].astype(str) == '999.0'] = \"Jan-1988\"\n    if not isinstance(processingDate, datetime.datetime):\n        processingDate = datetime.datetime.strptime(processingDate, \"%b-%Y\")\n    # Remove this null statement after clinical files have been re-validated\n    # copyClinicalDf['SEQ_DATE'][copyClinicalDf['SEQ_DATE'].isnull()] = \"Jan-1900\"\n    copyClinicalDf[\"SEQ_DATE\"][copyClinicalDf[\"SEQ_DATE\"] == \"Release\"] = \"Jan-1900\"\n    # clinicalDf['SEQ_DATE'][clinicalDf['SEQ_DATE'] == '999'] = \"Jan-1988\"\n    dates = copyClinicalDf[\"SEQ_DATE\"].apply(\n        lambda date: datetime.datetime.strptime(date, \"%b-%Y\")\n    )\n    keep = processingDate - dates &gt; datetime.timedelta(days)\n    keepSamples = copyClinicalDf[\"SAMPLE_ID\"][~keep]\n    # copyClinicalDf.SEQ_DATE[keep].unique()\n    return keepSamples\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.addClinicalHeaders","title":"<code>addClinicalHeaders(clinicalDf, mapping, patientCols, sampleCols, samplePath, patientPath)</code>","text":"<p>Add clinical file headers</p> PARAMETER DESCRIPTION <code>clinicalDf</code> <p>clinical dataframe</p> <p> </p> <code>mapping</code> <p>mapping dataframe, maps clinical columns to      labels and descriptions</p> <p> </p> <code>patientCols</code> <p>list of patient columns</p> <p> </p> <code>sampleCols</code> <p>list of sample columns</p> <p> </p> <code>samplePath</code> <p>clinical sample path</p> <p> </p> <code>patientPath</code> <p>clinical patient path</p> <p> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def addClinicalHeaders(\n    clinicalDf, mapping, patientCols, sampleCols, samplePath, patientPath\n):\n    \"\"\"\n    Add clinical file headers\n\n    Args:\n        clinicalDf: clinical dataframe\n        mapping: mapping dataframe, maps clinical columns to\n                 labels and descriptions\n        patientCols: list of patient columns\n        sampleCols: list of sample columns\n        samplePath: clinical sample path\n        patientPath: clinical patient path\n    \"\"\"\n    patientLabels = [\n        str(mapping[\"labels\"][mapping[\"cbio\"] == i].values[0]) for i in patientCols\n    ]\n    sampleLabels = [\n        str(mapping[\"labels\"][mapping[\"cbio\"] == i].values[0]) for i in sampleCols\n    ]\n    patientDesc = [\n        str(mapping[\"description\"][mapping[\"cbio\"] == i].values[0]) for i in patientCols\n    ]\n    sampleDesc = [\n        str(mapping[\"description\"][mapping[\"cbio\"] == i].values[0]) for i in sampleCols\n    ]\n    patientType = [\n        str(mapping[\"colType\"][mapping[\"cbio\"] == i].values[0]) for i in patientCols\n    ]\n    sampleType = [\n        str(mapping[\"colType\"][mapping[\"cbio\"] == i].values[0]) for i in sampleCols\n    ]\n\n    with open(patientPath, \"w+\") as patientFile:\n        patientFile.write(\"#%s\\n\" % \"\\t\".join(patientLabels))\n        patientFile.write(\"#%s\\n\" % \"\\t\".join(patientDesc))\n        patientFile.write(\"#%s\\n\" % \"\\t\".join(patientType))\n        patientFile.write(\"#%s\\n\" % \"\\t\".join([\"1\"] * len(patientLabels)))\n        text = removeFloat(clinicalDf[patientCols].drop_duplicates(\"PATIENT_ID\"))\n        patientFile.write(text)\n    with open(samplePath, \"w+\") as sampleFile:\n        sampleFile.write(\"#%s\\n\" % \"\\t\".join(sampleLabels))\n        sampleFile.write(\"#%s\\n\" % \"\\t\".join(sampleDesc))\n        sampleFile.write(\"#%s\\n\" % \"\\t\".join(sampleType))\n        sampleFile.write(\"#%s\\n\" % \"\\t\".join([\"1\"] * len(sampleLabels)))\n        text = removeFloat(clinicalDf[sampleCols].drop_duplicates(\"SAMPLE_ID\"))\n        sampleFile.write(text)\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._check_valid_df","title":"<code>_check_valid_df(df, col)</code>","text":"<p>Checking if variable is a pandas dataframe and column specified exist</p> PARAMETER DESCRIPTION <code>df</code> <p>Pandas dataframe</p> <p> </p> <code>col</code> <p>Column name</p> <p> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def _check_valid_df(df, col):\n    \"\"\"\n    Checking if variable is a pandas dataframe and column specified exist\n\n    Args:\n        df: Pandas dataframe\n        col: Column name\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Must pass in pandas dataframe\")\n    if df.get(col) is None:\n        raise ValueError(\"'{}' column must exist in dataframe\".format(col))\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._get_left_diff_df","title":"<code>_get_left_diff_df(left, right, checkby)</code>","text":"<p>Subset the dataframe based on 'checkby' by taking values in the left df that arent in the right df</p> PARAMETER DESCRIPTION <code>left</code> <p>Dataframe</p> <p> </p> <code>right</code> <p>Dataframe</p> <p> </p> <code>checkby</code> <p>Column of values to compare</p> <p> </p> Return <p>Dataframe: Subset of dataframe from left that don't exist in the right</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _get_left_diff_df(left, right, checkby):\n    \"\"\"\n    Subset the dataframe based on 'checkby' by taking values in the left df\n    that arent in the right df\n\n    Args:\n        left: Dataframe\n        right: Dataframe\n        checkby: Column of values to compare\n\n    Return:\n        Dataframe: Subset of dataframe from left that don't exist in the right\n    \"\"\"\n    _check_valid_df(left, checkby)\n    _check_valid_df(right, checkby)\n    diffdf = left[~left[checkby].isin(right[checkby])]\n    return diffdf\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._get_left_union_df","title":"<code>_get_left_union_df(left, right, checkby)</code>","text":"<p>Subset the dataframe based on 'checkby' by taking the union of values in the left df with the right df</p> PARAMETER DESCRIPTION <code>left</code> <p>Dataframe</p> <p> </p> <code>right</code> <p>Dataframe</p> <p> </p> <code>checkby</code> <p>Column of values to compare</p> <p> </p> Return <p>Dataframe: Subset of dataframe from left that also exist in the right</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _get_left_union_df(left, right, checkby):\n    \"\"\"\n    Subset the dataframe based on 'checkby' by taking the union of\n    values in the left df with the right df\n\n    Args:\n        left: Dataframe\n        right: Dataframe\n        checkby: Column of values to compare\n\n    Return:\n        Dataframe: Subset of dataframe from left that also exist in the right\n    \"\"\"\n    _check_valid_df(left, checkby)\n    _check_valid_df(right, checkby)\n    uniondf = left[left[checkby].isin(right[checkby])]\n    return uniondf\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._append_rows","title":"<code>_append_rows(new_datasetdf, databasedf, checkby)</code>","text":"<p>Compares the dataset from the database and determines which rows to append from the dataset</p> PARAMETER DESCRIPTION <code>new_datasetdf</code> <p>Input data dataframe</p> <p> </p> <code>databasedf</code> <p>Existing data dataframe</p> <p> </p> <code>checkby</code> <p>Column of values to compare</p> <p> </p> Return <p>Dataframe: Dataframe of rows to append</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _append_rows(new_datasetdf, databasedf, checkby):\n    \"\"\"\n    Compares the dataset from the database and determines which rows to\n    append from the dataset\n\n    Args:\n        new_datasetdf: Input data dataframe\n        databasedf: Existing data dataframe\n        checkby: Column of values to compare\n\n    Return:\n        Dataframe: Dataframe of rows to append\n    \"\"\"\n    databasedf.fillna(\"\", inplace=True)\n    new_datasetdf.fillna(\"\", inplace=True)\n\n    appenddf = _get_left_diff_df(new_datasetdf, databasedf, checkby)\n    if not appenddf.empty:\n        logger.info(\"Adding Rows\")\n    else:\n        logger.info(\"No new rows\")\n    del appenddf[checkby]\n    appenddf.reset_index(drop=True, inplace=True)\n    return appenddf\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._delete_rows","title":"<code>_delete_rows(new_datasetdf, databasedf, checkby)</code>","text":"<p>Compares the dataset from the database and determines which rows to delete from the dataset</p> PARAMETER DESCRIPTION <code>new_datasetdf</code> <p>Input data dataframe</p> <p> </p> <code>databasedf</code> <p>Existing data dataframe</p> <p> </p> <code>checkby</code> <p>Column of values to compare</p> <p> </p> Return <p>Dataframe: Dataframe of rows to delete</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _delete_rows(new_datasetdf, databasedf, checkby):\n    \"\"\"\n    Compares the dataset from the database and determines which rows to\n    delete from the dataset\n\n    Args:\n        new_datasetdf: Input data dataframe\n        databasedf: Existing data dataframe\n        checkby: Column of values to compare\n\n    Return:\n        Dataframe: Dataframe of rows to delete\n    \"\"\"\n\n    databasedf.fillna(\"\", inplace=True)\n    new_datasetdf.fillna(\"\", inplace=True)\n    # If the new dataset is empty, delete everything in the database\n    deletedf = _get_left_diff_df(databasedf, new_datasetdf, checkby)\n    if not deletedf.empty:\n        logger.info(\"Deleting Rows\")\n        delete_rowid_version = pd.DataFrame(\n            [[rowid.split(\"_\")[0], rowid.split(\"_\")[1]] for rowid in deletedf.index]\n        )\n        delete_rowid_version.reset_index(drop=True, inplace=True)\n    else:\n        delete_rowid_version = pd.DataFrame()\n        logger.info(\"No deleted rows\")\n\n    # del deletedf[checkby]\n    return delete_rowid_version\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._create_update_rowsdf","title":"<code>_create_update_rowsdf(updating_databasedf, updatesetdf, rowids, differentrows)</code>","text":"<p>Create the update dataset dataframe</p> PARAMETER DESCRIPTION <code>updating_databasedf</code> <p>Update database dataframe</p> <p> </p> <code>updatesetdf</code> <p>Update dataset dataframe</p> <p> </p> <code>rowids</code> <p>rowids of the database (Synapse ROW_ID, ROW_VERSION)</p> <p> </p> <code>differentrows</code> <p>vector of booleans for rows that need to be updated            True for update, False for not</p> <p> </p> RETURNS DESCRIPTION <code>dataframe</code> <p>Update dataframe</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _create_update_rowsdf(updating_databasedf, updatesetdf, rowids, differentrows):\n    \"\"\"\n    Create the update dataset dataframe\n\n    Args:\n        updating_databasedf: Update database dataframe\n        updatesetdf:  Update dataset dataframe\n        rowids: rowids of the database (Synapse ROW_ID, ROW_VERSION)\n        differentrows: vector of booleans for rows that need to be updated\n                       True for update, False for not\n\n    Returns:\n        dataframe: Update dataframe\n    \"\"\"\n    if sum(differentrows) &gt; 0:\n        updating_databasedf.loc[differentrows] = updatesetdf.loc[differentrows]\n        toupdatedf = updating_databasedf.loc[differentrows]\n        logger.info(\"Updating rows\")\n        rowid_version = pd.DataFrame(\n            [\n                [rowid.split(\"_\")[0], rowid.split(\"_\")[1]]\n                for rowid, row in zip(rowids, differentrows)\n                if row\n            ]\n        )\n        toupdatedf[\"ROW_ID\"] = rowid_version[0].values\n        toupdatedf[\"ROW_VERSION\"] = rowid_version[1].values\n        toupdatedf.reset_index(drop=True, inplace=True)\n    else:\n        toupdatedf = pd.DataFrame()\n        logger.info(\"No updated rows\")\n    return toupdatedf\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._update_rows","title":"<code>_update_rows(new_datasetdf, databasedf, checkby)</code>","text":"<p>Compares the dataset from the database and determines which rows to update from the dataset</p> PARAMETER DESCRIPTION <code>new_datasetdf</code> <p>Input data dataframe</p> <p> </p> <code>databasedf</code> <p>Existing data dataframe</p> <p> </p> <code>checkby</code> <p>Column of values to compare</p> <p> </p> Return <p>Dataframe: Dataframe of rows to update</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _update_rows(new_datasetdf, databasedf, checkby):\n    \"\"\"\n    Compares the dataset from the database and determines which rows to\n    update from the dataset\n\n    Args:\n        new_datasetdf: Input data dataframe\n        databasedf: Existing data dataframe\n        checkby: Column of values to compare\n\n    Return:\n        Dataframe: Dataframe of rows to update\n    \"\"\"\n    # initial_database = databasedf.copy()\n    databasedf.fillna(\"\", inplace=True)\n    new_datasetdf.fillna(\"\", inplace=True)\n    updatesetdf = _get_left_union_df(new_datasetdf, databasedf, checkby)\n    updating_databasedf = _get_left_union_df(databasedf, new_datasetdf, checkby)\n\n    # If you input the exact same dataframe theres nothing to update\n    # must save row version and ids for later\n    rowids = updating_databasedf.index.values\n    # Set index values to be 'checkby' values\n    updatesetdf.index = updatesetdf[checkby]\n    updating_databasedf.index = updating_databasedf[checkby]\n    del updatesetdf[checkby]\n    del updating_databasedf[checkby]\n\n    # Remove duplicated index values\n    updatesetdf = updatesetdf[~updatesetdf.index.duplicated()]\n    # Reorder dataset index\n    updatesetdf = updatesetdf.loc[updating_databasedf.index]\n    # Index comparison\n    differences = updatesetdf != updating_databasedf\n    differentrows = differences.apply(sum, axis=1) &gt; 0\n\n    toupdatedf = _create_update_rowsdf(\n        updating_databasedf, updatesetdf, rowids, differentrows\n    )\n\n    return toupdatedf\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.checkInt","title":"<code>checkInt(element)</code>","text":"<p>Check if an item can become an integer</p> PARAMETER DESCRIPTION <code>element</code> <p>Any variable and type</p> <p> </p> RETURNS DESCRIPTION <code>boolean</code> <p>True/False</p> Source code in <code>genie/process_functions.py</code> <pre><code>def checkInt(element):\n    \"\"\"\n    Check if an item can become an integer\n\n    Args:\n        element: Any variable and type\n\n    Returns:\n        boolean: True/False\n    \"\"\"\n    try:\n        element = float(element)\n        return element.is_integer()\n    except (ValueError, TypeError):\n        return False\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.check_col_and_values","title":"<code>check_col_and_values(df, col, possible_values, filename, na_allowed=False, required=False, sep=None)</code>","text":"<p>This function checks if the column exists then checks if the values in the column have the correct values</p> PARAMETER DESCRIPTION <code>df</code> <p>Input dataframe</p> <p> </p> <code>col</code> <p>Expected column name</p> <p> </p> <code>possible_values</code> <p>list of possible values</p> <p> </p> <code>filename</code> <p>Name of file</p> <p> </p> <code>required</code> <p>If the column is required.  Default is False</p> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>warning, error</p> Source code in <code>genie/process_functions.py</code> <pre><code>def check_col_and_values(\n    df, col, possible_values, filename, na_allowed=False, required=False, sep=None\n):\n    \"\"\"\n    This function checks if the column exists then checks if the values in the\n    column have the correct values\n\n    Args:\n        df: Input dataframe\n        col: Expected column name\n        possible_values: list of possible values\n        filename: Name of file\n        required: If the column is required.  Default is False\n\n    Returns:\n        tuple: warning, error\n    \"\"\"\n    warning = \"\"\n    error = \"\"\n    have_column = checkColExist(df, col)\n    if not have_column:\n        if required:\n            error = \"{filename}: Must have {col} column.\\n\".format(\n                filename=filename, col=col\n            )\n        else:\n            warning = (\n                \"{filename}: Doesn't have {col} column. \"\n                \"This column will be added\\n\".format(filename=filename, col=col)\n            )\n    else:\n        if na_allowed:\n            check_values = df[col].dropna()\n        else:\n            check_values = df[col]\n        if sep:\n            final = []\n            for value in check_values:\n                final.extend(value.split(sep))\n            check_values = pd.Series(final)\n        if not check_values.isin(possible_values).all():\n            error = \"{filename}: Please double check your {col} column.  This column must only be these values: {possible_vals}\\n\".format(\n                filename=filename,\n                col=col,\n                possible_vals=\", \".join(\n                    [\n                        # This is done because of pandas typing.\n                        # An integer column with one NA/blank value\n                        # will be cast as a double.\n                        str(value).replace(\".0\", \"\")\n                        for value in possible_values\n                    ]\n                ),\n            )\n    return (warning, error)\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.extract_oncotree_code_mappings_from_oncotree_json","title":"<code>extract_oncotree_code_mappings_from_oncotree_json(oncotree_json, primary, secondary)</code>","text":"Source code in <code>genie/process_functions.py</code> <pre><code>def extract_oncotree_code_mappings_from_oncotree_json(\n    oncotree_json, primary, secondary\n):\n    oncotree_code_to_info = {}\n    data = oncotree_json[\"children\"]\n    for node in data:\n        # if not node['code']:\n        #     sys.stderr.write('Encountered oncotree node without '\n        #                       'oncotree code : ' + node + '\\n')\n        #     continue\n        if data[node][\"level\"] == 1:\n            primary = node\n            secondary = \"\"\n        elif data[node][\"level\"] == 2:\n            secondary = node\n        cancer_type = data[node][\"mainType\"]\n        cancer_type_detailed = data[node][\"name\"]\n        if not cancer_type_detailed:\n            cancer_type_detailed = \"\"\n        oncotree_code_to_info[node.upper()] = {\n            \"CANCER_TYPE\": cancer_type,\n            \"CANCER_TYPE_DETAILED\": cancer_type_detailed,\n            \"ONCOTREE_PRIMARY_NODE\": primary,\n            \"ONCOTREE_SECONDARY_NODE\": secondary,\n        }\n\n        if len(data[node][\"children\"]) &gt; 0:\n            recurseDict = extract_oncotree_code_mappings_from_oncotree_json(\n                data[node], primary, secondary\n            )\n            oncotree_code_to_info.update(recurseDict)\n    return oncotree_code_to_info\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.get_oncotree_code_mappings","title":"<code>get_oncotree_code_mappings(oncotree_tumortype_api_endpoint_url)</code>","text":"<p>CREATE ONCOTREE DICTIONARY MAPPING TO PRIMARY, SECONDARY, CANCER TYPE, AND CANCER DESCRIPTION</p> Source code in <code>genie/process_functions.py</code> <pre><code>def get_oncotree_code_mappings(oncotree_tumortype_api_endpoint_url):\n    \"\"\"\n    CREATE ONCOTREE DICTIONARY MAPPING TO PRIMARY, SECONDARY,\n    CANCER TYPE, AND CANCER DESCRIPTION\n    \"\"\"\n    # oncotree_raw_response = urlopen(oncotree_tumortype_api_endpoint_url).text\n    # with requests.get(oncotree_tumortype_api_endpoint_url) as oncotreeUrl:\n    oncotreeUrl = retry_get_url(oncotree_tumortype_api_endpoint_url)\n    oncotree_raw_response = oncotreeUrl.text\n    oncotree_response = json.loads(oncotree_raw_response)\n    oncotree_response = oncotree_response[\"TISSUE\"]\n    return extract_oncotree_code_mappings_from_oncotree_json(oncotree_response, \"\", \"\")\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.getCODE","title":"<code>getCODE(mapping, key, useDescription=False)</code>","text":"Source code in <code>genie/process_functions.py</code> <pre><code>def getCODE(mapping, key, useDescription=False):\n    if useDescription:\n        value = mapping[\"DESCRIPTION\"][mapping[\"CODE\"] == key].values\n    else:\n        value = mapping[\"CBIO_LABEL\"][mapping[\"CODE\"] == key].values\n    if len(value) &gt; 0:\n        return value[0]\n    else:\n        return \"\"\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.getPrimary","title":"<code>getPrimary(code, oncotreeDict, primary)</code>","text":"Source code in <code>genie/process_functions.py</code> <pre><code>def getPrimary(code, oncotreeDict, primary):\n    if code != \"\":\n        for level in oncotreeDict:\n            if sum(oncotreeDict[level] == code) &gt; 0:\n                toAdd = primary[oncotreeDict[level] == code].values[0]\n                break\n            else:\n                toAdd = code\n    else:\n        toAdd = \"NOT_ANNOTATED\"\n    return toAdd\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.synapse_login","title":"<code>synapse_login(debug=False)</code>","text":"<p>Logs into Synapse if credentials are saved. If not saved, then user is prompted username and auth token.</p> PARAMETER DESCRIPTION <code>debug</code> <p>Synapse debug feature. Defaults to False</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Synapse</code> <p>Synapseclient object</p> Source code in <code>genie/process_functions.py</code> <pre><code>def synapse_login(debug: Optional[bool] = False) -&gt; Synapse:\n    \"\"\"\n    Logs into Synapse if credentials are saved.\n    If not saved, then user is prompted username and auth token.\n\n    Args:\n        debug: Synapse debug feature. Defaults to False\n\n    Returns:\n        Synapseclient object\n    \"\"\"\n    # If debug is True, then silent should be False\n    silent = False if debug else False\n    syn = synapseclient.Synapse(\n        debug=debug, silent=silent, user_agent=f\"aacrgenie/{__version__}\"\n    )\n    try:\n        syn.login()\n    except Exception:\n        raise ValueError(\n            \"Please view https://help.synapse.org/docs/Client-Configuration.1985446156.html\"\n            \"to configure authentication to the client.  Configure a ~/.synapseConfig\"\n            \"or set the SYNAPSE_AUTH_TOKEN environmental variable.\"\n        )\n    return syn\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.get_gdc_data_dictionary","title":"<code>get_gdc_data_dictionary(filetype)</code>","text":"<p>Use the GDC API to get the values allowed for columns of different filetypes (ie. disease_type in the case file)</p> PARAMETER DESCRIPTION <code>filetype</code> <p>GDC file type (ie. case, read_group)</p> <p> </p> Return <p>json:  Dictionary of allowed columns for the filetype and        allowed values for those columns</p> Source code in <code>genie/process_functions.py</code> <pre><code>def get_gdc_data_dictionary(filetype):\n    \"\"\"\n    Use the GDC API to get the values allowed for columns of\n    different filetypes (ie. disease_type in the case file)\n\n    Args:\n        filetype: GDC file type (ie. case, read_group)\n\n    Return:\n        json:  Dictionary of allowed columns for the filetype and\n               allowed values for those columns\n    \"\"\"\n    gdc_dict = retry_get_url(\n        \"https://api.gdc.cancer.gov/v0/submission/_dictionary/{filetype}\".format(\n            filetype=filetype\n        )\n    )\n    gdc_response = json.loads(gdc_dict.text)\n    return gdc_response\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._create_schema","title":"<code>_create_schema(syn, table_name, parentid, columns=None, annotations=None)</code>","text":"<p>Creates Table Schema</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>table_name</code> <p>Name of table</p> <p> </p> <code>parentid</code> <p>Project synapse id</p> <p> </p> <code>columns</code> <p>Columns of Table</p> <p> DEFAULT: <code>None</code> </p> <code>annotations</code> <p>Dictionary of annotations to add</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>Schema</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _create_schema(syn, table_name, parentid, columns=None, annotations=None):\n    \"\"\"Creates Table Schema\n\n    Args:\n        syn: Synapse object\n        table_name: Name of table\n        parentid: Project synapse id\n        columns: Columns of Table\n        annotations: Dictionary of annotations to add\n\n    Returns:\n        Schema\n    \"\"\"\n    schema = synapseclient.Schema(\n        name=table_name, columns=columns, parent=parentid, annotations=annotations\n    )\n    new_schema = syn.store(schema)\n    return new_schema\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._update_database_mapping","title":"<code>_update_database_mapping(syn, database_synid_mappingdf, database_mapping_synid, fileformat, new_tableid)</code>","text":"<p>Updates database to synapse id mapping table</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>database_synid_mappingdf</code> <p>Database to synapse id mapping dataframe</p> <p> </p> <code>database_mapping_synid</code> <p>Database to synapse id table id</p> <p> </p> <code>fileformat</code> <p>File format updated</p> <p> </p> <code>new_tableid</code> <p>New file format table id</p> <p> </p> RETURNS DESCRIPTION <p>Updated Table object</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _update_database_mapping(\n    syn, database_synid_mappingdf, database_mapping_synid, fileformat, new_tableid\n):\n    \"\"\"Updates database to synapse id mapping table\n\n    Args:\n        syn: Synapse object\n        database_synid_mappingdf: Database to synapse id mapping dataframe\n        database_mapping_synid: Database to synapse id table id\n        fileformat: File format updated\n        new_tableid: New file format table id\n\n    Returns:\n        Updated Table object\n    \"\"\"\n    fileformat_ind = database_synid_mappingdf[\"Database\"] == fileformat\n    # Store in the new database synid\n    database_synid_mappingdf[\"Id\"][fileformat_ind] = new_tableid\n    # Only update the one row\n    to_update_row = database_synid_mappingdf[fileformat_ind]\n\n    syn.store(synapseclient.Table(database_mapping_synid, to_update_row))\n    return database_synid_mappingdf\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions._move_entity","title":"<code>_move_entity(syn, ent, parentid, name=None)</code>","text":"<p>Moves an entity (works like linux mv)</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>ent</code> <p>Synapse Entity</p> <p> </p> <code>parentid</code> <p>Synapse Project id</p> <p> </p> <code>name</code> <p>New Entity name if a new name is desired</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>Moved Entity</p> Source code in <code>genie/process_functions.py</code> <pre><code>def _move_entity(syn, ent, parentid, name=None):\n    \"\"\"Moves an entity (works like linux mv)\n\n    Args:\n        syn: Synapse object\n        ent: Synapse Entity\n        parentid: Synapse Project id\n        name: New Entity name if a new name is desired\n\n    Returns:\n        Moved Entity\n    \"\"\"\n    ent.parentId = parentid\n    if name is not None:\n        ent.name = name\n    moved_ent = syn.store(ent)\n    return moved_ent\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.get_dbmapping","title":"<code>get_dbmapping(syn, projectid)</code>","text":"<p>Gets database mapping information</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>projectid</code> <p>Project id where new data lives</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>{'synid': database mapping syn id, 'df': database mapping pd.DataFrame}</p> Source code in <code>genie/process_functions.py</code> <pre><code>def get_dbmapping(syn: Synapse, projectid: str) -&gt; dict:\n    \"\"\"Gets database mapping information\n\n    Args:\n        syn: Synapse connection\n        projectid: Project id where new data lives\n\n    Returns:\n        {'synid': database mapping syn id,\n         'df': database mapping pd.DataFrame}\n\n    \"\"\"\n    project_ent = syn.get(projectid)\n    dbmapping_synid = project_ent.annotations.get(\"dbMapping\", \"\")[0]\n    database_mapping = syn.tableQuery(f\"select * from {dbmapping_synid}\")\n    database_mappingdf = database_mapping.asDataFrame()\n    return {\"synid\": dbmapping_synid, \"df\": database_mappingdf}\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.create_new_fileformat_table","title":"<code>create_new_fileformat_table(syn, file_format, newdb_name, projectid, archive_projectid)</code>","text":"<p>Creates new database table based on old database table and archives old database table</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>file_format</code> <p>File format to update</p> <p> TYPE: <code>str</code> </p> <code>newdb_name</code> <p>Name of new database table</p> <p> TYPE: <code>str</code> </p> <code>projectid</code> <p>Project id where new database should live</p> <p> TYPE: <code>str</code> </p> <code>archive_projectid</code> <p>Project id where old database should be moved</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>{\"newdb_ent\": New database synapseclient.Table, \"newdb_mappingdf\": new databse pd.DataFrame, \"moved_ent\": old database synpaseclient.Table}</p> Source code in <code>genie/process_functions.py</code> <pre><code>def create_new_fileformat_table(\n    syn: Synapse,\n    file_format: str,\n    newdb_name: str,\n    projectid: str,\n    archive_projectid: str,\n) -&gt; dict:\n    \"\"\"Creates new database table based on old database table and archives\n    old database table\n\n    Args:\n        syn: Synapse object\n        file_format: File format to update\n        newdb_name: Name of new database table\n        projectid: Project id where new database should live\n        archive_projectid: Project id where old database should be moved\n\n    Returns:\n        {\"newdb_ent\": New database synapseclient.Table,\n         \"newdb_mappingdf\": new databse pd.DataFrame,\n         \"moved_ent\": old database synpaseclient.Table}\n    \"\"\"\n    db_info = get_dbmapping(syn, projectid)\n    database_mappingdf = db_info[\"df\"]\n    dbmapping_synid = db_info[\"synid\"]\n\n    olddb_synid = extract.getDatabaseSynId(\n        syn, file_format, databaseToSynIdMappingDf=database_mappingdf\n    )\n    olddb_ent = syn.get(olddb_synid)\n    olddb_columns = list(syn.getTableColumns(olddb_synid))\n\n    newdb_ent = _create_schema(\n        syn,\n        table_name=newdb_name,\n        columns=olddb_columns,\n        parentid=projectid,\n        annotations=olddb_ent.annotations,\n    )\n\n    newdb_mappingdf = _update_database_mapping(\n        syn, database_mappingdf, dbmapping_synid, file_format, newdb_ent.id\n    )\n    # Automatically rename the archived entity with ARCHIVED\n    # This will attempt to resolve any issues if the table already exists at\n    # location\n    new_table_name = f\"ARCHIVED {time.time()}-{olddb_ent.name}\"\n    moved_ent = _move_entity(syn, olddb_ent, archive_projectid, name=new_table_name)\n    return {\n        \"newdb_ent\": newdb_ent,\n        \"newdb_mappingdf\": newdb_mappingdf,\n        \"moved_ent\": moved_ent,\n    }\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.create_missing_columns","title":"<code>create_missing_columns(dataset, schema)</code>","text":"<p>Creates and fills missing columns with the relevant NA value for the     given data type. Note that special handling had to occur for     allowing NAs in integer based columns in pandas by converting     the integer column into the Int64 (pandas nullable integer data type)</p> PARAMETER DESCRIPTION <code>dataset</code> <p>input dataset to fill missing columns for</p> <p> TYPE: <code>DataFrame</code> </p> <code>schema</code> <p>the expected schema {column_name(str): data_type(str)} for the input dataset</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pd.Series: updated dataset</p> Source code in <code>genie/process_functions.py</code> <pre><code>def create_missing_columns(dataset: pd.DataFrame, schema: dict) -&gt; pd.Series:\n    \"\"\"Creates and fills missing columns with the relevant NA value for the\n        given data type. Note that special handling had to occur for\n        allowing NAs in integer based columns in pandas by converting\n        the integer column into the Int64 (pandas nullable integer data type)\n\n    Args:\n        dataset (pd.DataFrame): input dataset to fill missing columns for\n        schema (dict): the expected schema {column_name(str): data_type(str)}\n            for the input dataset\n\n    Returns:\n        pd.Series: updated dataset\n    \"\"\"\n    missing_values = {\n        \"string\": \"\",\n        \"integer\": None,\n        \"float\": float(\"nan\"),\n        \"boolean\": None,\n    }\n    for column, data_type in schema.items():\n        if column not in dataset.columns:\n            dataset = dataset.assign(**{column: missing_values[data_type]})\n\n        # only way to preserve NAs for these specific dtype columns\n        if data_type == \"integer\":\n            dataset[column] = dataset[column].astype(\"Int64\")\n        elif data_type == \"boolean\":\n            dataset[column] = dataset[column].astype(pd.BooleanDtype())\n    return dataset[list(schema.keys())]\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.check_values_in_column","title":"<code>check_values_in_column(df, col, values)</code>","text":"<p>Check if a column in a dataframe contains specific values Args:     df (pd.DataFrame): The clinical dataframe     col (str): The column name     values (list): Expected values in the column Returns:     bool: True if the column contains the specified values</p> Source code in <code>genie/process_functions.py</code> <pre><code>def check_values_in_column(\n    df: pd.DataFrame, col: str, values: Union[str, list]\n) -&gt; bool:\n    \"\"\"Check if a column in a dataframe contains specific values\n    Args:\n        df (pd.DataFrame): The clinical dataframe\n        col (str): The column name\n        values (list): Expected values in the column\n    Returns:\n        bool: True if the column contains the specified values\n    \"\"\"\n    if not checkColExist(df, col):\n        logger.error(f\"Must have {col} column in the dataframe.\")\n    else:\n        # Ensure values is always a list for next step\n        if isinstance(values, str):\n            values = [values]\n        result = df[col].isin(values).any()\n        return result\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.get_row_indices_for_invalid_column_values","title":"<code>get_row_indices_for_invalid_column_values(df, col, possible_values, na_allowed=False, sep=None)</code>","text":"<p>This function checks the column values against possible_values and returns row indices of invalid rows.</p> PARAMETER DESCRIPTION <code>df</code> <p>Input dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>col</code> <p>The column to be checked</p> <p> TYPE: <code>str</code> </p> <code>possible_values</code> <p>The list of possible values</p> <p> TYPE: <code>list</code> </p> <code>na_allowed</code> <p>If NA is allowed. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sep</code> <p>The string separator. Defaults to None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Index</code> <p>pd.Index: The row indices of the rows with values that are not in possible_values.</p> Source code in <code>genie/process_functions.py</code> <pre><code>def get_row_indices_for_invalid_column_values(\n    df: pd.DataFrame,\n    col: str,\n    possible_values: list,\n    na_allowed: bool = False,\n    sep: Optional[str] = None,\n) -&gt; pd.Index:\n    \"\"\"This function checks the column values against possible_values and returns row indices of invalid rows.\n\n    Args:\n        df (pd.DataFrame): Input dataframe\n        col (str): The column to be checked\n        possible_values (list): The list of possible values\n        na_allowed (bool, optional): If NA is allowed. Defaults to False.\n        sep (Optional[str], optional): The string separator. Defaults to None.\n\n    Returns:\n        pd.Index: The row indices of the rows with values that are not in possible_values.\n    \"\"\"\n    if na_allowed:\n        # this is only useful for dropping NAs for individual values rather than value_list\n        check_values = df[col].dropna()\n    else:\n        check_values = df[col]\n    if sep:\n        # for columns contain lists of values\n        check_values = check_values.apply(\n            lambda x: all(substring in possible_values for substring in x.split(sep))\n        )\n    else:\n        check_values = check_values.apply(lambda x: x in possible_values)\n    return check_values[check_values == False].index\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.get_message_for_invalid_column_value","title":"<code>get_message_for_invalid_column_value(col, filename, invalid_indices, possible_values)</code>","text":"<p>This function returns the error and warning messages if the target column has rows with invalid values.</p> PARAMETER DESCRIPTION <code>col</code> <p>The column to be checked</p> <p> TYPE: <code>str</code> </p> <code>filename</code> <p>The file name</p> <p> TYPE: <code>str</code> </p> <code>invalid_indices</code> <p>The row indices of the rows with invalid values</p> <p> TYPE: <code>Index</code> </p> <code>possible_values</code> <p>The list of possible values</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>warning, error</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def get_message_for_invalid_column_value(\n    col: str, filename: str, invalid_indices: pd.Index, possible_values: list\n) -&gt; tuple:\n    \"\"\"This function returns the error and warning messages if the target column has rows with invalid values.\n\n    Args:\n        col (str): The column to be checked\n        filename (str): The file name\n        invalid_indices (pd.Index): The row indices of the rows with invalid values\n        possible_values (list): The list of possible values\n\n    Returns:\n        tuple: warning, error\n    \"\"\"\n    warning = \"\"\n    error = \"\"\n    # check the validity of values in the column\n    # concatenated possible values. This is done because of pandas typing. An integer column with one NA/blank value will be cast as a double.\n    possible_values = \", \".join(\n        [str(value).replace(\".0\", \"\") for value in possible_values]\n    )\n    if len(invalid_indices) &gt; 0:\n        error = (\n            f\"{filename}: Please double check your {col} column. Valid values are {possible_values}. \"\n            f\"You have {len(invalid_indices)} row(s) in your file where {col} column contains invalid values. \"\n            f\"The row(s) this occurs in are: {invalid_indices.tolist()}. Please correct.\\n\"\n        )\n    return (warning, error)\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.check_column_and_values_row_specific","title":"<code>check_column_and_values_row_specific(df, col, possible_values, filename, na_allowed=False, required=False, sep=None)</code>","text":"<p>This function checks if the column exists and checks if the values in the column have the valid values.    Currently, this function is only used in assay.py</p> PARAMETER DESCRIPTION <code>df</code> <p>Input dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>col</code> <p>The column to be checked</p> <p> TYPE: <code>str</code> </p> <code>possible_values</code> <p>The list of possible values</p> <p> TYPE: <code>list</code> </p> <code>filename</code> <p>The file name</p> <p> TYPE: <code>str</code> </p> <code>na_allowed</code> <p>If NA is allowed. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>required</code> <p>If the column is required. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sep</code> <p>The string separator. Defaults to None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>warning, error</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def check_column_and_values_row_specific(\n    df: pd.DataFrame,\n    col: str,\n    possible_values: list,\n    filename: str,\n    na_allowed: bool = False,\n    required: bool = False,\n    sep: Optional[str] = None,\n) -&gt; tuple:\n    \"\"\"This function checks if the column exists and checks if the values in the column have the valid values.\n       Currently, this function is only used in assay.py\n\n    Args:\n        df (pd.DataFrame): Input dataframe\n        col (str): The column to be checked\n        possible_values (list): The list of possible values\n        filename (str): The file name\n        na_allowed (bool, optional): If NA is allowed. Defaults to False.\n        required (bool, optional): If the column is required. Defaults to False.\n        sep (Optional[str], optional): The string separator. Defaults to None.\n\n    Returns:\n        tuple: warning, error\n    \"\"\"\n    warning = \"\"\n    error = \"\"\n    # check the existence of the column\n    have_column = checkColExist(df, col)\n    if not have_column:\n        if required:\n            error = \"{filename}: Must have {col} column.\\n\".format(\n                filename=filename, col=col\n            )\n        else:\n            warning = (\n                \"{filename}: Doesn't have {col} column. \"\n                \"This column will be added.\\n\".format(filename=filename, col=col)\n            )\n    else:\n        # get the row indices\n        invalid_indices = get_row_indices_for_invalid_column_values(\n            df, col, possible_values, na_allowed, sep\n        )\n        # generate validation message\n        warning, error = get_message_for_invalid_column_value(\n            col, filename, invalid_indices, possible_values\n        )\n\n    return (warning, error)\n</code></pre>"},{"location":"reference/helper_modules/process_functions/#genie.process_functions.add_columns_to_data_gene_matrix","title":"<code>add_columns_to_data_gene_matrix(data_gene_matrix, sample_list, column_name)</code>","text":"<p>Add  CNA and SV columns to data gene matrix</p> PARAMETER DESCRIPTION <code>data_gene_matrix</code> <p>data gene matrix</p> <p> TYPE: <code>DataFrame</code> </p> <code>sample_list</code> <p>The list of cna or sv samples</p> <p> TYPE: <code>list</code> </p> <code>column_name</code> <p>The column name to be added</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/process_functions.py</code> <pre><code>def add_columns_to_data_gene_matrix(\n    data_gene_matrix: pd.DataFrame, sample_list: list, column_name: str\n):\n    \"\"\"Add  CNA and SV columns to data gene matrix\n\n    Args:\n        data_gene_matrix (pd.DataFrame): data gene matrix\n        sample_list (list): The list of cna or sv samples\n        column_name (str): The column name to be added\n    \"\"\"\n    # extract the sample list\n    seqids = data_gene_matrix[\"mutations\"][\n        data_gene_matrix[\"SAMPLE_ID\"].isin(sample_list)\n    ].unique()\n\n    # add the column to the data gene matrix and set the value to non-CNA or non-SV rows to NA\n    data_gene_matrix[column_name] = data_gene_matrix[\"mutations\"]\n    data_gene_matrix[column_name][~data_gene_matrix[column_name].isin(seqids)] = \"NA\"\n\n    return data_gene_matrix\n</code></pre>"},{"location":"reference/helper_modules/transform/","title":"transform","text":""},{"location":"reference/helper_modules/transform/#genie.transform","title":"<code>genie.transform</code>","text":"<p>This module contains all the transformation functions used throughout the GENIE package</p>"},{"location":"reference/helper_modules/transform/#genie.transform-functions","title":"Functions","text":""},{"location":"reference/helper_modules/transform/#genie.transform._col_name_to_titlecase","title":"<code>_col_name_to_titlecase(string)</code>","text":"<p>Convert strings to titlecase. Supports strings separated by _.</p> PARAMETER DESCRIPTION <code>string</code> <p>A string</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string converted to title case</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/transform.py</code> <pre><code>def _col_name_to_titlecase(string: str) -&gt; str:\n    \"\"\"Convert strings to titlecase. Supports strings separated by _.\n\n    Args:\n        string (str): A string\n\n    Returns:\n        str: A string converted to title case\n\n    \"\"\"\n    # This is a mapping of strings that are abbreviations\n    abbrev_map = {\"Dna_\": \"DNA_\", \"Rna_\": \"RNA_\", \"Sv_\": \"SV_\", \"Ncbi_\": \"NCBI_\"}\n    # The reason I split the string by _ and utilize the capitalize function\n    # instead of using something like .title(), so outlined here:\n    # https://stackoverflow.com/questions/1549641/how-can-i-capitalize-the-first-letter-of-each-word-in-a-string\n    converted_str = \"_\".join([each.capitalize() for each in string.split(\"_\")])\n    for titlecase, abbrev in abbrev_map.items():\n        converted_str = converted_str.replace(titlecase, abbrev)\n    return converted_str\n</code></pre>"},{"location":"reference/helper_modules/transform/#genie.transform._convert_col_with_nas_to_str","title":"<code>_convert_col_with_nas_to_str(df, col)</code>","text":"<p>This converts a column into str while preserving NAs</p> Source code in <code>genie/transform.py</code> <pre><code>def _convert_col_with_nas_to_str(df: pd.DataFrame, col: str) -&gt; list:\n    \"\"\"This converts a column into str while preserving NAs\"\"\"\n    new_vals = [str(val) if pd.notna(val) else val for val in df[col]]\n    return new_vals\n</code></pre>"},{"location":"reference/helper_modules/transform/#genie.transform._convert_float_col_with_nas_to_int","title":"<code>_convert_float_col_with_nas_to_int(df, col)</code>","text":"<p>This converts int column that was turned into a float col because pandas does that with int values that have NAs back into an int col with NAs intact</p> Source code in <code>genie/transform.py</code> <pre><code>def _convert_float_col_with_nas_to_int(df: pd.DataFrame, col: str) -&gt; list:\n    \"\"\"This converts int column that was turned into a float col because\n    pandas does that with int values that have NAs back into an int col\n    with NAs intact\"\"\"\n    if is_float_dtype(df[col]) and df[col].isnull().values.any():\n        new_vals = df[col].astype(pd.Int64Dtype()).tolist()\n        return new_vals\n    else:\n        return df[col].tolist()\n</code></pre>"},{"location":"reference/helper_modules/transform/#genie.transform._convert_df_with_mixed_dtypes","title":"<code>_convert_df_with_mixed_dtypes(read_csv_params)</code>","text":"<p>This checks if a dataframe read in normally comes out with mixed data types (which happens when low_memory = True because read_csv parses in chunks and guesses dtypes by chunk) and converts a dataframe with mixed datatypes to one datatype.</p> PARAMETER DESCRIPTION <code>read_csv_params</code> <p>of input params and values to pandas's read_csv function. needs to include filepath to dataset to be read in</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame : The dataset read in</p> Source code in <code>genie/transform.py</code> <pre><code>def _convert_df_with_mixed_dtypes(read_csv_params: dict) -&gt; pd.DataFrame:\n    \"\"\"This checks if a dataframe read in normally comes out with mixed data types (which happens\n    when low_memory = True because read_csv parses in chunks and guesses dtypes by chunk) and\n    converts a dataframe with mixed datatypes to one datatype.\n\n    Args:\n        read_csv_params (dict): of input params and values to pandas's read_csv function.\n            needs to include filepath to dataset to be read in\n\n    Returns:\n        pd.DataFrame : The dataset read in\n    \"\"\"\n    warnings.simplefilter(\"error\", pd.errors.DtypeWarning)\n    try:\n        df = pd.read_csv(**read_csv_params, low_memory=True)\n    except pd.errors.DtypeWarning:\n        # setting engine to c as that is the only engine that works with low_memory=False\n        df = pd.read_csv(**read_csv_params, low_memory=False, engine=\"c\")\n    warnings.resetwarnings()\n    return df\n</code></pre>"},{"location":"reference/helper_modules/transform/#genie.transform._convert_values_to_na","title":"<code>_convert_values_to_na(input_df, values_to_replace, columns_to_convert)</code>","text":"<p>Converts given values to NA in an input dataset</p> PARAMETER DESCRIPTION <code>input_df</code> <p>input dataset</p> <p> TYPE: <code>DataFrame</code> </p> <code>values_to_replace</code> <p>string values to replace with na</p> <p> TYPE: <code>List[str]</code> </p> <code>columns_to_convert</code> <p>subset of columns to convert with na in</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: dataset with specified values replaced with NAs</p> Source code in <code>genie/transform.py</code> <pre><code>def _convert_values_to_na(\n    input_df: pd.DataFrame, values_to_replace: List[str], columns_to_convert: List[str]\n) -&gt; pd.DataFrame:\n    \"\"\"Converts given values to NA in an input dataset\n\n    Args:\n        input_df (pd.DataFrame): input dataset\n        values_to_replace (List[str]): string values to replace with na\n        columns_to_convert (List[str]): subset of columns to convert with na in\n\n    Returns:\n        pd.DataFrame: dataset with specified values replaced with NAs\n    \"\"\"\n    if not input_df.empty:\n        replace_mapping = {value: None for value in values_to_replace}\n        input_df[columns_to_convert] = input_df[columns_to_convert].replace(\n            replace_mapping\n        )\n    return input_df\n</code></pre>"},{"location":"reference/helper_modules/validate/","title":"Validate","text":""},{"location":"reference/helper_modules/validate/#genie.validate","title":"<code>genie.validate</code>","text":""},{"location":"reference/helper_modules/validate/#genie.validate-attributes","title":"Attributes","text":""},{"location":"reference/helper_modules/validate/#genie.validate.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/validate/#genie.validate.ACCEPTED_CHROMOSOMES","title":"<code>ACCEPTED_CHROMOSOMES = list(map(str, range(1, 23))) + ['X', 'Y', 'MT']</code>  <code>module-attribute</code>","text":""},{"location":"reference/helper_modules/validate/#genie.validate-classes","title":"Classes","text":""},{"location":"reference/helper_modules/validate/#genie.validate.ValidationHelper","title":"<code>ValidationHelper</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>genie/validate.py</code> <pre><code>class ValidationHelper(object):\n    # Used for the kwargs in validate_single_file\n    # Overload this per class\n    _validate_kwargs: List[str] = []\n\n    def __init__(\n        self,\n        syn: synapseclient.Synapse,\n        project_id: str,\n        center: str,\n        entitylist: List[synapseclient.File],\n        format_registry: Optional[Dict] = None,\n        file_type: Optional[str] = None,\n        genie_config: Optional[Dict] = None,\n        ancillary_files: Optional[list] = None,\n    ):\n        \"\"\"A validator helper class for a center's files.\n\n        Args:\n            syn: a synapseclient.Synapse object\n            project_id: Synapse Project ID where files are stored and configured.\n            center: The participating center name.\n            entitylist: a list of file paths.\n            format_registry: A dictionary mapping file format name to the\n                             format class.\n            file_type: Specify file type to skip filename validation\n            ancillary_files: all files downloaded for validation\n        \"\"\"\n        self._synapse_client = syn\n        self._project = syn.get(project_id)\n        self.entitylist = entitylist\n        self.center = center\n        self._format_registry = format_registry\n        self.file_type = self.determine_filetype() if file_type is None else file_type\n        self.genie_config = genie_config\n        self.ancillary_files = ancillary_files\n\n    def determine_filetype(self):\n        \"\"\"Gets the file type of the file by validating its filename\n\n        Args:\n            syn: Synapse object\n            filepathlist: list of filepaths to center files\n\n        Returns:\n            str: File type of input files.  None if no filetype found\n\n        \"\"\"\n        filetype = None\n        # Loop through file formats\n        for file_format in self._format_registry:\n            validator = self._format_registry[file_format](\n                self._synapse_client, self.center\n            )\n            try:\n                filenames = [entity.name for entity in self.entitylist]\n                filetype = validator.validateFilename(filenames)\n            except AssertionError:\n                continue\n            # If valid filename, return file type.\n            if filetype is not None:\n                break\n        return filetype\n\n    def validate_single_file(self, **kwargs):\n        \"\"\"Validate a submitted file unit.\n\n        Returns:\n            message: errors and warnings\n            valid: Boolean value of validation status\n        \"\"\"\n        if self.file_type not in self._format_registry:\n            allowed_filetypes = list(self._format_registry.keys())\n            error_message = (\n                f\"Your filename is incorrect! Please change your filename before you run the validator or specify --filetype if you are running the validator locally. \"\n                f\"If specifying filetype, options are: [{', '.join(allowed_filetypes)}]\\n\"\n            )\n            valid_result_cls = example_filetype_format.ValidationResults(\n                errors=error_message,\n                warnings=\"\",\n            )\n        else:\n            mykwargs = {}\n            for required_parameter in self._validate_kwargs:\n                assert required_parameter in kwargs.keys(), (\n                    \"%s not in parameter list\" % required_parameter\n                )\n                mykwargs[required_parameter] = kwargs[required_parameter]\n                mykwargs[\"project_id\"] = self._project.id\n\n            validator_cls = self._format_registry[self.file_type]\n            validator = validator_cls(\n                syn=self._synapse_client,\n                center=self.center,\n                genie_config=self.genie_config,\n                ancillary_files=self.ancillary_files,\n            )\n            filepathlist = [entity.path for entity in self.entitylist]\n            valid_result_cls = validator.validate(filePathList=filepathlist, **mykwargs)\n\n        # Complete error message\n        message = valid_result_cls.collect_errors_and_warnings()\n        return (valid_result_cls, message)\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.ValidationHelper-functions","title":"Functions","text":""},{"location":"reference/helper_modules/validate/#genie.validate.ValidationHelper.__init__","title":"<code>__init__(syn, project_id, center, entitylist, format_registry=None, file_type=None, genie_config=None, ancillary_files=None)</code>","text":"<p>A validator helper class for a center's files.</p> PARAMETER DESCRIPTION <code>syn</code> <p>a synapseclient.Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>project_id</code> <p>Synapse Project ID where files are stored and configured.</p> <p> TYPE: <code>str</code> </p> <code>center</code> <p>The participating center name.</p> <p> TYPE: <code>str</code> </p> <code>entitylist</code> <p>a list of file paths.</p> <p> TYPE: <code>List[File]</code> </p> <code>format_registry</code> <p>A dictionary mapping file format name to the              format class.</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> <code>file_type</code> <p>Specify file type to skip filename validation</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation</p> <p> TYPE: <code>Optional[list]</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/validate.py</code> <pre><code>def __init__(\n    self,\n    syn: synapseclient.Synapse,\n    project_id: str,\n    center: str,\n    entitylist: List[synapseclient.File],\n    format_registry: Optional[Dict] = None,\n    file_type: Optional[str] = None,\n    genie_config: Optional[Dict] = None,\n    ancillary_files: Optional[list] = None,\n):\n    \"\"\"A validator helper class for a center's files.\n\n    Args:\n        syn: a synapseclient.Synapse object\n        project_id: Synapse Project ID where files are stored and configured.\n        center: The participating center name.\n        entitylist: a list of file paths.\n        format_registry: A dictionary mapping file format name to the\n                         format class.\n        file_type: Specify file type to skip filename validation\n        ancillary_files: all files downloaded for validation\n    \"\"\"\n    self._synapse_client = syn\n    self._project = syn.get(project_id)\n    self.entitylist = entitylist\n    self.center = center\n    self._format_registry = format_registry\n    self.file_type = self.determine_filetype() if file_type is None else file_type\n    self.genie_config = genie_config\n    self.ancillary_files = ancillary_files\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.ValidationHelper.determine_filetype","title":"<code>determine_filetype()</code>","text":"<p>Gets the file type of the file by validating its filename</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>filepathlist</code> <p>list of filepaths to center files</p> <p> </p> RETURNS DESCRIPTION <code>str</code> <p>File type of input files.  None if no filetype found</p> Source code in <code>genie/validate.py</code> <pre><code>def determine_filetype(self):\n    \"\"\"Gets the file type of the file by validating its filename\n\n    Args:\n        syn: Synapse object\n        filepathlist: list of filepaths to center files\n\n    Returns:\n        str: File type of input files.  None if no filetype found\n\n    \"\"\"\n    filetype = None\n    # Loop through file formats\n    for file_format in self._format_registry:\n        validator = self._format_registry[file_format](\n            self._synapse_client, self.center\n        )\n        try:\n            filenames = [entity.name for entity in self.entitylist]\n            filetype = validator.validateFilename(filenames)\n        except AssertionError:\n            continue\n        # If valid filename, return file type.\n        if filetype is not None:\n            break\n    return filetype\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.ValidationHelper.validate_single_file","title":"<code>validate_single_file(**kwargs)</code>","text":"<p>Validate a submitted file unit.</p> RETURNS DESCRIPTION <code>message</code> <p>errors and warnings</p> <code>valid</code> <p>Boolean value of validation status</p> Source code in <code>genie/validate.py</code> <pre><code>def validate_single_file(self, **kwargs):\n    \"\"\"Validate a submitted file unit.\n\n    Returns:\n        message: errors and warnings\n        valid: Boolean value of validation status\n    \"\"\"\n    if self.file_type not in self._format_registry:\n        allowed_filetypes = list(self._format_registry.keys())\n        error_message = (\n            f\"Your filename is incorrect! Please change your filename before you run the validator or specify --filetype if you are running the validator locally. \"\n            f\"If specifying filetype, options are: [{', '.join(allowed_filetypes)}]\\n\"\n        )\n        valid_result_cls = example_filetype_format.ValidationResults(\n            errors=error_message,\n            warnings=\"\",\n        )\n    else:\n        mykwargs = {}\n        for required_parameter in self._validate_kwargs:\n            assert required_parameter in kwargs.keys(), (\n                \"%s not in parameter list\" % required_parameter\n            )\n            mykwargs[required_parameter] = kwargs[required_parameter]\n            mykwargs[\"project_id\"] = self._project.id\n\n        validator_cls = self._format_registry[self.file_type]\n        validator = validator_cls(\n            syn=self._synapse_client,\n            center=self.center,\n            genie_config=self.genie_config,\n            ancillary_files=self.ancillary_files,\n        )\n        filepathlist = [entity.path for entity in self.entitylist]\n        valid_result_cls = validator.validate(filePathList=filepathlist, **mykwargs)\n\n    # Complete error message\n    message = valid_result_cls.collect_errors_and_warnings()\n    return (valid_result_cls, message)\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.GenieValidationHelper","title":"<code>GenieValidationHelper</code>","text":"<p>               Bases: <code>ValidationHelper</code></p> <p>A validator helper class for AACR Project Genie.</p> Source code in <code>genie/validate.py</code> <pre><code>class GenieValidationHelper(ValidationHelper):\n    \"\"\"A validator helper class for AACR Project Genie.\"\"\"\n\n    _validate_kwargs = [\"nosymbol_check\"]\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate-functions","title":"Functions","text":""},{"location":"reference/helper_modules/validate/#genie.validate._check_parentid_permission_container","title":"<code>_check_parentid_permission_container(syn, parentid)</code>","text":"<p>Checks permission / container</p> Source code in <code>genie/validate.py</code> <pre><code>def _check_parentid_permission_container(syn, parentid):\n    \"\"\"Checks permission / container\"\"\"\n    if parentid is not None:\n        try:\n            syn_ent = syn.get(parentid, downloadFile=False)\n            # If not container, throw an assertion\n            assert synapseclient.entity.is_container(syn_ent)\n        except (SynapseHTTPError, AssertionError):\n            raise ValueError(\n                \"Provided Synapse id must be your input folder Synapse id \"\n                \"or a Synapse Id of a folder inside your input directory\"\n            )\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate._check_center_input","title":"<code>_check_center_input(center, center_list)</code>","text":"<p>Checks center input</p> PARAMETER DESCRIPTION <code>center</code> <p>Center name</p> <p> </p> <code>center_list</code> <p>List of allowed centers</p> <p> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If specify a center not part of the center list</p> Source code in <code>genie/validate.py</code> <pre><code>def _check_center_input(center, center_list):\n    \"\"\"Checks center input\n\n    Args:\n        center: Center name\n        center_list: List of allowed centers\n\n    Raises:\n        ValueError: If specify a center not part of the center list\n\n    \"\"\"\n    if center not in center_list:\n        raise ValueError(\n            \"Must specify one of these \" f\"centers: {', '.join(center_list)}\"\n        )\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate._validate_chromosome","title":"<code>_validate_chromosome(df, col, fileformat, allow_chr=True, allow_na=False)</code>","text":"<p>Validate chromosome values</p> PARAMETER DESCRIPTION <code>df</code> <p>Dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>col</code> <p>Column header for column containing chromosome values</p> <p> TYPE: <code>str</code> </p> <code>fileformat</code> <p>GENIE supported file format</p> <p> TYPE: <code>str</code> </p> <code>allow_chr</code> <p>whether the chr prefix is allowed in the values</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>allow_na</code> <p>whether NA/blanks are allowed in the values</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>errors and warnings</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>genie/validate.py</code> <pre><code>def _validate_chromosome(\n    df: pd.DataFrame,\n    col: str,\n    fileformat: str,\n    allow_chr: bool = True,\n    allow_na: bool = False,\n) -&gt; tuple:\n    \"\"\"Validate chromosome values\n\n    Args:\n        df (pd.DataFrame): Dataframe\n        col (str): Column header for column containing chromosome values\n        fileformat (str): GENIE supported file format\n        allow_chr (bool): whether the chr prefix is allowed in the values\n        allow_na (bool): whether NA/blanks are allowed in the values\n\n    Returns:\n        tuple: errors and warnings\n    \"\"\"\n    have_column = process_functions.checkColExist(df, col)\n    errors = \"\"\n    warnings = \"\"\n    if have_column:\n        nochr = [\"chr\" in i for i in df[col] if isinstance(i, str)]\n        if sum(nochr) &gt; 0:\n            if allow_chr:\n                warnings += f\"{fileformat}: Should not have the chr prefix in front of chromosomes.\\n\"\n            else:\n                errors += f\"{fileformat}: Should not have the chr prefix in front of chromosomes.\\n\"\n        # correct_chromosomes = [\n        #     str(chrom).replace(\"chr\", \"\") in accepted_chromosomes\n        #     for chrom in df[col]\n        # ]\n        # preserve NAs\n        df[col] = transform._convert_float_col_with_nas_to_int(df, col)\n        df[col] = transform._convert_col_with_nas_to_str(df, col)\n        df[col] = [val.replace(\"chr\", \"\") if pd.notna(val) else val for val in df[col]]\n        warning, error = process_functions.check_col_and_values(\n            df=df,\n            col=col,\n            possible_values=ACCEPTED_CHROMOSOMES,\n            filename=fileformat,\n            na_allowed=allow_na,\n        )\n        errors += error\n        warnings += warning\n    return (errors, warnings)\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate._perform_validate","title":"<code>_perform_validate(syn, args)</code>","text":"<p>This is the main entry point to the genie command line tool.</p> Source code in <code>genie/validate.py</code> <pre><code>def _perform_validate(syn, args):\n    \"\"\"This is the main entry point to the genie command line tool.\"\"\"\n\n    # Check parentid argparse\n    _check_parentid_permission_container(syn=syn, parentid=args.parentid)\n    genie_config = extract.get_genie_config(syn=syn, project_id=args.project_id)\n    # HACK: Modify oncotree link config\n    # TODO: Remove oncotree_link parameter from this function\n    genie_config[\"oncotreeLink\"] = extract._get_oncotreelink(\n        syn=syn, genie_config=genie_config, oncotree_link=args.oncotree_link\n    )\n    # Check center argparse\n    _check_center_input(args.center, list(genie_config[\"center_config\"].keys()))\n\n    format_registry = config.collect_format_types(args.format_registry_packages)\n    logger.debug(f\"Using {format_registry} file formats.\")\n    entity_list = [\n        synapseclient.File(name=filepath, path=filepath, parentId=None)\n        for filepath in args.filepath\n    ]\n\n    validator = GenieValidationHelper(\n        syn=syn,\n        project_id=args.project_id,\n        center=args.center,\n        entitylist=entity_list,\n        format_registry=format_registry,\n        file_type=args.filetype,\n        genie_config=genie_config,\n    )\n    mykwargs = dict(\n        nosymbol_check=args.nosymbol_check,\n        project_id=args.project_id,\n    )\n    valid, message = validator.validate_single_file(**mykwargs)\n\n    # Upload to synapse if parentid is specified and valid\n    if valid and args.parentid is not None:\n        logger.info(f\"Uploading files to {args.parentid}\")\n        load.store_files(syn=syn, filepaths=args.filepath, parentid=args.parentid)\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.parse_file_info_in_nested_list","title":"<code>parse_file_info_in_nested_list(nested_list, search_str, ignore_case=False, allow_underscore=False)</code>","text":"<p>TODO: To refactor/remove once clinical file structure gets updated to be non-nested</p> <p>Parses for a name and filepath in a nested list of Synapse entity objects</p> PARAMETER DESCRIPTION <code>nested_list</code> <p>description</p> <p> TYPE: <code>list[list]</code> </p> <code>search_str</code> <p>the substring to look for in the files</p> <p> TYPE: <code>str</code> </p> <code>ignore_case</code> <p>whether to perform case-insensitive comparison.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>allow_underscore</code> <p>whether to treat underscores as equivalent to dashes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>dict[dict]: files found, name(s) and filepath(s) of the file(s) found</p> Source code in <code>genie/validate.py</code> <pre><code>def parse_file_info_in_nested_list(\n    nested_list: List[List[synapseclient.Entity]],\n    search_str: str,\n    ignore_case: bool = False,\n    allow_underscore: bool = False,\n) -&gt; dict:\n    \"\"\"\n    TODO: To refactor/remove once clinical file structure gets updated to\n    be non-nested\n\n    Parses for a name and filepath in a nested list of Synapse entity objects\n\n    Args:\n        nested_list (list[list]): _description_\n        search_str (str): the substring to look for in the files\n        ignore_case (bool, optional): whether to perform case-insensitive comparison.\n        allow_underscore (bool, optional): whether to treat underscores as equivalent to dashes.\n\n    Returns:\n        dict[dict]: files found,\n            name(s) and filepath(s) of the file(s) found\n    \"\"\"\n    file_info = {}\n    all_files = {\n        file[\"name\"]: file[\"path\"]\n        for files in nested_list\n        for file in files\n        if standardize_string_for_validation(\n            input_string=file[\"name\"],\n            ignore_case=ignore_case,\n            allow_underscore=allow_underscore,\n        ).startswith(\n            standardize_string_for_validation(\n                input_string=search_str,\n                ignore_case=ignore_case,\n                allow_underscore=allow_underscore,\n            )\n        )\n    }\n\n    file_info[\"name\"] = \",\".join(all_files.keys())\n    file_info[\"path\"] = list(all_files.values())  # type: ignore[assignment]\n    return {\"files\": all_files, \"file_info\": file_info}\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.check_values_between_two_df","title":"<code>check_values_between_two_df(df1, df1_filename, df1_id_to_check, df2, df2_filename, df2_id_to_check, ignore_case=False, allow_underscore=False)</code>","text":"<p>Check that all the identifier(s) (ids) in one file (df1) exists in the other file (df1)</p> PARAMETER DESCRIPTION <code>df1</code> <p>file to use as base of check</p> <p> TYPE: <code>DataFrame</code> </p> <code>df1_filename</code> <p>filename of file to use as base of check</p> <p> TYPE: <code>str</code> </p> <code>df1_id_to_check</code> <p>name of column to check values for in df1</p> <p> TYPE: <code>str</code> </p> <code>df2</code> <p>file to cross-validate against</p> <p> TYPE: <code>DataFrame</code> </p> <code>df2_filename</code> <p>filename of file to cross-validate against</p> <p> TYPE: <code>str</code> </p> <code>df2_id_to_check</code> <p>name of column to check values for in df2</p> <p> TYPE: <code>str</code> </p> <code>ignore_case</code> <p>whether to perform case-insensitive comparison.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>allow_underscore</code> <p>whether to treat underscores as equivalent to dashes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings as a file from cross-validation.    Defaults to blank strings</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>genie/validate.py</code> <pre><code>def check_values_between_two_df(\n    df1: pd.DataFrame,\n    df1_filename: str,\n    df1_id_to_check: str,\n    df2: pd.DataFrame,\n    df2_filename: str,\n    df2_id_to_check: str,\n    ignore_case: bool = False,\n    allow_underscore: bool = False,\n) -&gt; tuple:\n    \"\"\"Check that all the identifier(s) (ids) in one\n    file (df1) exists in the other file (df1)\n\n    Args:\n        df1 (pd.DataFrame): file to use as base of check\n        df1_filename (str): filename of file to use as base of check\n        df1_id_to_check (str): name of column to check values for in df1\n        df2 (pd.DataFrame): file to cross-validate against\n        df2_filename (str): filename of file to cross-validate against\n        df2_id_to_check (str): name of column to check values for in df2\n        ignore_case (bool, optional): whether to perform case-insensitive comparison.\n        allow_underscore (bool, optional): whether to treat underscores as equivalent to dashes.\n\n    Returns:\n        tuple: The errors and warnings as a file from cross-validation.\n               Defaults to blank strings\n    \"\"\"\n    errors = \"\"\n    warnings = \"\"\n\n    # standardize case\n    df1.columns = [col.upper() for col in df1.columns]\n    df2.columns = [col.upper() for col in df2.columns]\n\n    # standardize string values\n    df1_values = [\n        standardize_string_for_validation(\n            input_string=val,\n            ignore_case=ignore_case,\n            allow_underscore=allow_underscore,\n        )\n        for val in df1[df1_id_to_check]\n    ]\n    df2_values = [\n        standardize_string_for_validation(\n            input_string=val,\n            ignore_case=ignore_case,\n            allow_underscore=allow_underscore,\n        )\n        for val in df2[df2_id_to_check]\n    ]\n\n    # check to see if df1 ids are present in df2\n    if not set(df1_values) &lt;= set(df2_values):\n        errors = (\n            f\"At least one {df1_id_to_check} in your {df1_filename} file \"\n            f\"does not exist as a {df2_id_to_check} in your {df2_filename} file. \"\n            \"Please update your file(s) to be consistent.\\n\"\n        )\n\n    return errors, warnings\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.check_variant_start_and_end_positions","title":"<code>check_variant_start_and_end_positions(input_df, start_pos_col, end_pos_col, filename)</code>","text":"<p>Checks that a variant's start position is less than its     end position</p> PARAMETER DESCRIPTION <code>input_df</code> <p>Input data to check positions</p> <p> TYPE: <code>DataFrame</code> </p> <code>start_pos_col</code> <p>Name of the start position column</p> <p> TYPE: <code>str</code> </p> <code>end_pos_col</code> <p>Name of the end position column</p> <p> TYPE: <code>str</code> </p> <code>filename</code> <p>Name of the file</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings from the position validation    Defaults to blank strings</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>genie/validate.py</code> <pre><code>def check_variant_start_and_end_positions(\n    input_df: pd.DataFrame, start_pos_col: str, end_pos_col: str, filename: str\n) -&gt; tuple:\n    \"\"\"Checks that a variant's start position is less than its\n        end position\n\n    Args:\n        input_df (pd.DataFrame): Input data to check positions\n        start_pos_col (str): Name of the start position column\n        end_pos_col (str): Name of the end position column\n        filename (str): Name of the file\n\n    Returns:\n        tuple: The errors and warnings from the position validation\n               Defaults to blank strings\n\n    \"\"\"\n    errors = \"\"\n    warnings = \"\"\n\n    if any(input_df[start_pos_col] &gt; input_df[end_pos_col]):\n        warnings = (\n            f\"{filename}: Your variants file has record(s) that have an end position \"\n            \"value less than the start position value. Please update your file to be consistent. \"\n            \"When we annotate using the genome-nexus-annotation-pipeline, the records with this \"\n            \"position discrepancy will be re-annotated with different end position values.\\n\"\n        )\n    return errors, warnings\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.standardize_string_for_validation","title":"<code>standardize_string_for_validation(input_string, ignore_case=False, allow_underscore=False)</code>","text":"<p>This standardizes a string to prep it for further validation purposes     e.g: string comparison</p> PARAMETER DESCRIPTION <code>input_string</code> <p>input string to standardize</p> <p> TYPE: <code>str</code> </p> <code>ignore_case</code> <p>Lowercases the string perform case-insensitive comparison. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>allow_underscore</code> <p>Treats underscores as equivalent to dashes. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>str</code> <p>standardized string</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/validate.py</code> <pre><code>def standardize_string_for_validation(\n    input_string: str, ignore_case: bool = False, allow_underscore: bool = False\n) -&gt; str:\n    \"\"\"This standardizes a string to prep it for further validation purposes\n        e.g: string comparison\n\n    Args:\n        input_string (str): input string to standardize\n        ignore_case (bool, optional): Lowercases the string perform case-insensitive comparison. Defaults to False.\n        allow_underscore (bool, optional): Treats underscores as equivalent to dashes. Defaults to False.\n\n    Returns:\n        str: standardized string\n    \"\"\"\n    if isinstance(input_string, str):\n        standardized_str = input_string\n        if ignore_case:\n            standardized_str = standardized_str.lower()\n        if allow_underscore:\n            standardized_str = standardized_str.replace(\"_\", \"-\")\n        return standardized_str\n    else:\n        return input_string\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.get_invalid_allele_rows","title":"<code>get_invalid_allele_rows(input_data, input_col, allowed_comb_alleles, allowed_ind_alleles, ignore_case=False, allow_na=False)</code>","text":"<p>Find invalid indices in a DataFrame column based on allowed allele values.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The DataFrame to search.</p> <p> TYPE: <code>DataFrame</code> </p> <code>input_col</code> <p>The name of the column to check.</p> <p> TYPE: <code>str</code> </p> <code>allowed_comb_alleles</code> <p>The list of allowed allele values (can appear in combinations or individually)</p> <p> TYPE: <code>list</code> </p> <code>allowed_ind_alleles</code> <p>The list of allowed allele values (can only appear individually)</p> <p> TYPE: <code>list</code> </p> <code>ignore_case</code> <p>whether to perform case-insensitive matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>allow_na</code> <p>whether to allow NAs to be an allowed allele value or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     pd.Index: A pandas index object indicating the row indices that     don't match the allowed alleles</p> Source code in <code>genie/validate.py</code> <pre><code>def get_invalid_allele_rows(\n    input_data: pd.DataFrame,\n    input_col: str,\n    allowed_comb_alleles: list,\n    allowed_ind_alleles: list,\n    ignore_case: bool = False,\n    allow_na: bool = False,\n) -&gt; pd.Index:\n    \"\"\"\n    Find invalid indices in a DataFrame column based on allowed allele values.\n\n    Args:\n        input_data (pd.DataFrame): The DataFrame to search.\n        input_col (str): The name of the column to check.\n        allowed_comb_alleles (list): The list of allowed allele values\n            (can appear in combinations or individually)\n        allowed_ind_alleles (list): The list of allowed allele values\n            (can only appear individually)\n        ignore_case (bool, optional): whether to perform case-insensitive matching\n        allow_na (bool, optional): whether to allow NAs to be an allowed allele\n            value or not.\n    Returns:\n        pd.Index: A pandas index object indicating the row indices that\n        don't match the allowed alleles\n    \"\"\"\n    search_str = \"\"\n    if allowed_comb_alleles:\n        search_str += f'^[{re.escape(\"\".join(allowed_comb_alleles))}]+$'\n\n    if allowed_ind_alleles:\n        search_str += f'|^[{re.escape(\"\".join(allowed_ind_alleles))}]+$'\n\n    if ignore_case:\n        flags = re.IGNORECASE\n    else:\n        flags = 0  # no flags\n\n    # special handling for all NA column\n    is_all_na = pd.isna(input_data[input_col]).all()\n    if is_all_na and allow_na:\n        invalid_indices = pd.Index([])\n    elif is_all_na and not allow_na:\n        invalid_indices = input_data.index\n    else:\n        # convert numeric cols to string while preserving NAs in order to use str.match\n        transformed_data = input_data.copy()\n        transformed_data[input_col] = transform._convert_col_with_nas_to_str(\n            transformed_data, input_col\n        )\n\n        matching_indices = transformed_data[input_col].str.match(\n            search_str, flags=flags, na=allow_na\n        )\n        invalid_indices = transformed_data[~matching_indices].index\n    return invalid_indices\n</code></pre>"},{"location":"reference/helper_modules/validate/#genie.validate.get_allele_validation_message","title":"<code>get_allele_validation_message(invalid_indices, invalid_col, allowed_comb_alleles, allowed_ind_alleles, fileformat)</code>","text":"<p>Creates the error/warning message for the check for invalid alleles</p> PARAMETER DESCRIPTION <code>invalid_indices</code> <p>the row indices that have invalid alleles</p> <p> TYPE: <code>Series</code> </p> <code>invalid_col</code> <p>The column with the invalid values</p> <p> TYPE: <code>str</code> </p> <code>allowed_comb_alleles</code> <p>The list of allowed allele values (can appear in combinations or individually)</p> <p> TYPE: <code>list</code> </p> <code>allowed_ind_alleles</code> <p>The list of allowed allele values (can only appear individually)</p> <p> TYPE: <code>list</code> </p> <code>fileformat</code> <p>Name of the fileformat</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>The errors and warnings from the allele validation    Defaults to blank strings</p> <p> TYPE: <code>tuple</code> </p> Source code in <code>genie/validate.py</code> <pre><code>def get_allele_validation_message(\n    invalid_indices: pd.Series,\n    invalid_col: str,\n    allowed_comb_alleles: list,\n    allowed_ind_alleles: list,\n    fileformat: str,\n) -&gt; tuple:\n    \"\"\"Creates the error/warning message for the check for invalid alleles\n\n    Args:\n        invalid_indices (pd.Series): the row indices that\n            have invalid alleles\n        invalid_col (str): The column with the invalid values\n        allowed_comb_alleles (list): The list of allowed allele values\n            (can appear in combinations or individually)\n        allowed_ind_alleles (list): The list of allowed allele values\n            (can only appear individually)\n        fileformat (str): Name of the fileformat\n\n    Returns:\n        tuple: The errors and warnings from the allele validation\n               Defaults to blank strings\n    \"\"\"\n    errors = \"\"\n    warnings = \"\"\n    if len(invalid_indices) &gt; 0:\n        errors = (\n            f\"{fileformat}: Your {invalid_col} column has invalid allele values. \"\n            \"This is the list of accepted allele values that can appear individually \"\n            f\"or in combination with each other: {','.join(allowed_comb_alleles)}.\\n\"\n            \"This is the list of accepted allele values that can only appear individually: \"\n            f\"{','.join(allowed_ind_alleles)}\\n\"\n        )\n    return errors, warnings\n</code></pre>"},{"location":"reference/main_pipeline_commands/consortium_to_public/","title":"Consortium to Public","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public","title":"<code>bin.consortium_to_public</code>","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public.PWD","title":"<code>PWD = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public.parser","title":"<code>parser = argparse.ArgumentParser()</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public.generate_dashboard_html","title":"<code>generate_dashboard_html(genie_version, staging=False, testing=False)</code>","text":"<p>Generates dashboard html writeout that gets uploaded to the release folder</p> PARAMETER DESCRIPTION <code>genie_version</code> <p>GENIE release</p> <p> TYPE: <code>str</code> </p> <code>staging</code> <p>Use staging files. Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>testing</code> <p>Use testing files. Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>bin/consortium_to_public.py</code> <pre><code>def generate_dashboard_html(\n    genie_version: str, staging: bool = False, testing: bool = False\n):\n    \"\"\"Generates dashboard html writeout that gets uploaded to the\n    release folder\n\n    Args:\n        genie_version: GENIE release\n        staging: Use staging files. Default is False\n        testing: Use testing files. Default is False\n\n    \"\"\"\n    markdown_render_cmd = [\n        \"Rscript\",\n        os.path.join(PWD, \"../R/dashboard_markdown_generator.R\"),\n        genie_version,\n        \"--template_path\",\n        os.path.join(PWD, \"../templates/dashboardTemplate.Rmd\"),\n    ]\n    if staging:\n        markdown_render_cmd.append(\"--staging\")\n    if testing:\n        markdown_render_cmd.append(\"--testing\")\n    subprocess.check_call(markdown_render_cmd)\n</code></pre>"},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public.generate_data_guide","title":"<code>generate_data_guide(genie_version, oncotree_version=None, database_mapping=None)</code>","text":"<p>Generates the GENIE data guide</p> Source code in <code>bin/consortium_to_public.py</code> <pre><code>def generate_data_guide(genie_version, oncotree_version=None, database_mapping=None):\n    \"\"\"Generates the GENIE data guide\"\"\"\n\n    template_path = os.path.join(PWD, \"../templates/data_guide_template.Rnw\")\n    with open(template_path, \"r\") as template_file:\n        template_str = template_file.read()\n\n    replacements = {\n        \"{{release}}\": genie_version,\n        \"{{database_synid}}\": database_mapping,\n        \"{{oncotree}}\": oncotree_version.replace(\"_\", \"\\\\_\"),\n        \"{{genie_banner}}\": os.path.join(PWD, \"../genie_banner.png\"),\n    }\n\n    for search in replacements:\n        replacement = replacements[search]\n        # If no replacement value is passed in, don't replace\n        if replacement is not None:\n            template_str = template_str.replace(search, replacement)\n\n    with open(os.path.join(PWD, \"data_guide.Rnw\"), \"w\") as data_guide_file:\n        data_guide_file.write(template_str)\n\n    subprocess.check_call(\n        [\"R\", \"CMD\", \"Sweave\", \"--pdf\", os.path.join(PWD, \"data_guide.Rnw\")]\n    )\n    return \"data_guide.pdf\"\n</code></pre>"},{"location":"reference/main_pipeline_commands/consortium_to_public/#bin.consortium_to_public.main","title":"<code>main(args)</code>","text":"Source code in <code>bin/consortium_to_public.py</code> <pre><code>def main(args):\n    # HACK: Delete all existing files first\n    process_functions.rmFiles(database_to_staging.GENIE_RELEASE_DIR)\n\n    cbioValidatorPath = os.path.join(\n        args.cbioportalPath, \"core/src/main/scripts/importer/validateData.py\"\n    )\n    assert os.path.exists(cbioValidatorPath), \"Please specify correct cbioportalPath\"\n    assert not (\n        args.test and args.staging\n    ), \"You can only specify --test or --staging, not both\"\n    try:\n        processingDate = datetime.datetime.strptime(args.processingDate, \"%b-%Y\")\n    except ValueError:\n        raise ValueError(\n            \"Process date must be in the format \" \"abbreviated_month-YEAR ie. Oct-2017\"\n        )\n\n    syn = process_functions.synapse_login(debug=args.debug)\n\n    # Get all the possible public releases\n    # Get configuration\n    if args.test:\n        databaseSynIdMappingId = \"syn11600968\"\n        args.genieVersion = \"TESTpublic\"\n    elif args.staging:\n        databaseSynIdMappingId = \"syn12094210\"\n    else:\n        databaseSynIdMappingId = \"syn10967259\"\n    databaseSynIdMapping = syn.tableQuery(\"select * from %s\" % databaseSynIdMappingId)\n    databaseSynIdMappingDf = databaseSynIdMapping.asDataFrame()\n    public_synid = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"public\"\n    ].values[0]\n    # Use release folder fileview\n    releaseSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"releaseFolder\"\n    ].values[0]\n    processTrackerSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"processTracker\"\n    ].values[0]\n    # TEST run of the infrastructure will always\n    # Map to a specific folder\n    if args.test:\n        officialPublic = {\"TESTpublic\": \"syn12299959\"}\n    else:\n        officialPublic = extract.get_public_to_consortium_synid_mapping(\n            syn, releaseSynId\n        )\n    if args.genieVersion not in officialPublic.keys():\n        allowed_public_release_names = \", \".join(officialPublic.keys())\n        raise ValueError(\n            f\"genieVersion must be one of these: {allowed_public_release_names}.\"\n        )\n\n    args.releaseId = officialPublic[args.genieVersion]\n    if not args.test and not args.staging:\n        load.update_process_trackingdf(\n            syn=syn,\n            process_trackerdb_synid=processTrackerSynId,\n            center=\"SAGE\",\n            process_type=\"public\",\n            start=True,\n        )\n\n    caseListEntities, genePanelEntities = consortium_to_public.consortiumToPublic(\n        syn,\n        processingDate,\n        args.genieVersion,\n        args.releaseId,\n        databaseSynIdMappingDf,\n        publicReleaseCutOff=args.publicReleaseCutOff,\n    )\n\n    database_to_staging.revise_metadata_files(syn, public_synid, args.genieVersion)\n\n    logger.info(\"CBIO VALIDATION\")\n    # Must be exit 0 because the validator sometimes fails,\n    # but we still want to capture the output\n    command = [\n        cbioValidatorPath,\n        \"-s\",\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"-n\",\n        \"; exit 0\",\n    ]\n    cbio_output = subprocess.check_output(\" \".join(command), shell=True)\n    cbio_decoded_output = cbio_output.decode(\"utf-8\")\n    logger.info(cbio_decoded_output)\n    if not args.test and not args.staging:\n        log_folder_synid = databaseSynIdMappingDf[\"Id\"][\n            databaseSynIdMappingDf[\"Database\"] == \"logs\"\n        ].values[0]\n        # Use tempfiles\n        cbio_log_file = \"cbioValidatorLogsPublic_{}.txt\".format(args.genieVersion)\n        with open(cbio_log_file, \"w\") as cbioLog:\n            cbioLog.write(cbio_decoded_output)\n        syn.store(synapseclient.File(cbio_log_file, parentId=log_folder_synid))\n        os.remove(cbio_log_file)\n    # logger.info(\"REMOVING OLD FILES\")\n    # process_functions.rmFiles(database_to_staging.CASE_LIST_PATH)\n    # seg_meta_file = \"{}/genie_public_meta_cna_hg19_seg.txt\".format(\n    #     database_to_staging.GENIE_RELEASE_DIR\n    # )\n    # if os.path.exists(seg_meta_file):\n    #     os.unlink(seg_meta_file)\n\n    logger.info(\"CREATING LINK VERSION\")\n    folders = database_to_staging.create_link_version(\n        syn,\n        args.genieVersion,\n        caseListEntities,\n        genePanelEntities,\n        databaseSynIdMappingDf,\n        release_type=\"public\",\n    )\n    # Don't update process tracker is testing or staging\n    if not args.test and not args.staging:\n        load.update_process_trackingdf(\n            syn=syn,\n            process_trackerdb_synid=processTrackerSynId,\n            center=\"SAGE\",\n            process_type=\"public\",\n            start=False,\n        )\n\n    logger.info(\"DASHBOARD UPDATE\")\n    # Only run dashboard update if not testing or staging\n    if not args.test and not args.staging:\n        dashboard_table_updater.run_dashboard(\n            syn,\n            databaseSynIdMappingDf,\n            args.genieVersion,\n            staging=args.staging,\n        )\n    generate_dashboard_html(args.genieVersion, staging=args.staging, testing=args.test)\n    logger.info(\"DASHBOARD UPDATE COMPLETE\")\n    logger.info(\"AUTO GENERATE DATA GUIDE\")\n\n    # TODO: remove data guide code\n    # onco_link = databaseSynIdMappingDf[\"Id\"][\n    #    databaseSynIdMappingDf[\"Database\"] == \"oncotreeLink\"\n    # ].values[0]\n    # onco_link_ent = syn.get(onco_link)\n    # oncotree_link = onco_link_ent.externalURL\n    # oncotree_version = oncotree_link.split(\"=\")[1]\n\n    # data_guide_pdf = generate_data_guide(\n    #    args.genieVersion,\n    #    oncotree_version=oncotree_version,\n    #    database_mapping=databaseSynIdMappingId,\n    # )\n    # data_guide_ent = synapseclient.File(\n    #    data_guide_pdf, parent=folders[\"release_folder\"]\n    # )\n    # syn.store(data_guide_ent)\n    logger.info(\"COMPLETED CONSORTIUM TO PUBLIC\")\n</code></pre>"},{"location":"reference/main_pipeline_commands/database_to_staging/","title":"Database to Staging","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging","title":"<code>bin.database_to_staging</code>","text":"<p>Workflow to trigger a consortium release</p> <pre><code>flowchart TD\n    A[\"Start consortium release\"] --&gt; B[\"Remove old files in GENIE release dir\"]\n    %% B --&gt; C[\"Login to Synapse\"]\n    %% C --&gt; D{\"Test / Staging / Prod Environment?\"}\n    %% D --&gt;|Test| E[\"Set test databaseSynIdMappingId\"]\n    %% D --&gt;|Staging| F[\"Set staging databaseSynIdMappingId, skip mutations in cis\"]\n    %% D --&gt;|Production| G[\"Set production databaseSynIdMappingId\"]\n    B --&gt; H[\"Prepare for processing by getting databaseSynIdMapping table, oncotree link, cbioportal folder, database synapse ids, etc\"]\n    %% H --&gt; I{\"Oncotree link provided?\"}\n    %% I --&gt;|No| J[\"Extract Oncotree URL from database\"]\n    %% I --&gt;|Yes| K[\"Use provided Oncotree URL\"]\n    %% J &amp; K --&gt; L[\"Check Oncotree URL accessibility\"]\n    %% L --&gt; M[\"Validate cBioPortal path exists\"]\n    %% M --&gt; N[\"Get Synapse IDs for consortium, process tracker, etc.\"]\n    %% N --&gt; O[\"Create or retrieve case_lists folder\"]\n    %% O --&gt; P{\"Staging?\"}\n    %% P -- No --&gt; Q[\"Start process tracking\"]\n    H --&gt; R[\"Query for all the centers for which data should be released\"]\n\n    %% Expanded stagingToCbio logic\n    %% R --&gt; S2[\"Create GENIE release directory if missing\"]\n    %% S1 --&gt; S2[\"Extract Synapse Table IDs (patient, sample, maf, bed, seg, etc.)\"]\n    R --&gt; S3[\"Create snapshots &amp; pull patient, sample, bed data\"]\n    S3 --&gt; S4[\"Merge patient + sample tables into clinicalDf\"]\n    S4 --&gt; S5[\"Run GENIE filters\"]\n    S5 --&gt; SD1[\"Variant Filters\"]\n    SD1 --&gt; SD2[\"Germline filter\"]\n    SD1 --&gt; SD3[\"MAF in BED\"]\n    SD2 &amp; SD3 --&gt; SD5[\"List of variants to remove\"]\n    S5 --&gt; SE1[\"Sample Filters\"]\n    SE1 --&gt; SE2[\"SEQ_DATE\"]\n    SE1 --&gt; SE3[\"No Bed file\"]\n    SE1 --&gt; SE4[\"Oncotree\"]\n    SE1 --&gt; SE5[\"Mutation In Cis\"]\n    SE2 &amp; SE3 &amp; SE4 &amp; SE5 --&gt; SE6[\"List of samples to remove\"]\n    SE6 --&gt; SS3[\"Merge/Filter/store clinical file\"]\n    %% SS3 --&gt; S6[\"Merge/Filter/store clinical file\"]\n    SD5 &amp; SS3 --&gt; S7[\"Merge/Filter/store MAF file\"]\n    SS3 --&gt; S8[\"Merge/Filter/store CNA file\"]\n    SS3 --&gt; S9[\"Merge/Filter/store Assay information file\"]\n    SS3 --&gt; S10[\"Merge/Filter/store SV file\"]\n    S8 &amp; S7 &amp; S9 &amp; S10 --&gt; S11[\"Merge/Filter/store Data Gene Matrix\"]\n    S11 --&gt; S12[\"Download and upload gene panel files\"]\n    SS3 --&gt; S13[\"Merge/Filter/store SEG file\"]\n    SS3 --&gt; S14[\"Merge/Filter/store BED files\"]\n    %% SS3 --&gt; S15[\"Return list of gene panel entities\"]\n\n    SS3 &amp; S12 &amp; S13 &amp; S14 --&gt; T[\"Remove old case list files\"]\n    T --&gt; U[\"Generate new case lists and upload to Synapse\"]\n    U --&gt; V[\"Revise metadata files with correct GENIE version\"]\n    V --&gt; W[\"Run cBioPortal validation script\"]\n    %% W --&gt; X{\"Production?\"}\n    %% X --&gt;|Yes| Y[\"Upload validation logs to Synapse\"]\n    W --&gt; Z[\"Create release folder with links to files\"]\n    %% Z --&gt; AA{\"Production?\"}\n    %% AA --&gt;|Yes| AB[\"End process tracking\"]\n    Z --&gt; AC[\"Run dashboard updater\"]\n    AC --&gt; AD[\"Generate dashboard HTML\"]\n    AD --&gt; AF[\"End\"]</code></pre>"},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.PWD","title":"<code>PWD = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.parser","title":"<code>parser = argparse.ArgumentParser(description='Release GENIE consortium files')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.test_group","title":"<code>test_group = parser.add_mutually_exclusive_group()</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.generate_dashboard_html","title":"<code>generate_dashboard_html(genie_version, staging=False, testing=False)</code>","text":"<p>Generates dashboard html writeout that gets uploaded to the release folder</p> PARAMETER DESCRIPTION <code>genie_version</code> <p>GENIE release</p> <p> TYPE: <code>str</code> </p> <code>staging</code> <p>Use staging files. Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>testing</code> <p>Use testing files. Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>bin/database_to_staging.py</code> <pre><code>def generate_dashboard_html(\n    genie_version: str, staging: bool = False, testing: bool = False\n):\n    \"\"\"Generates dashboard html writeout that gets uploaded to the\n    release folder\n\n    Args:\n        genie_version: GENIE release\n        staging: Use staging files. Default is False\n        testing: Use testing files. Default is False\n\n    \"\"\"\n    markdown_render_cmd = [\n        \"Rscript\",\n        os.path.join(PWD, \"../R/dashboard_markdown_generator.R\"),\n        genie_version,\n        \"--template_path\",\n        os.path.join(PWD, \"../templates/dashboardTemplate.Rmd\"),\n    ]\n\n    if staging:\n        markdown_render_cmd.append(\"--staging\")\n    if testing:\n        markdown_render_cmd.append(\"--testing\")\n    subprocess.check_call(markdown_render_cmd)\n</code></pre>"},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.generate_data_guide","title":"<code>generate_data_guide(genie_version, oncotree_version=None, database_mapping=None)</code>","text":"<p>Generates the GENIE data guide</p> Source code in <code>bin/database_to_staging.py</code> <pre><code>def generate_data_guide(genie_version, oncotree_version=None, database_mapping=None):\n    \"\"\"Generates the GENIE data guide\"\"\"\n\n    template_path = os.path.join(PWD, \"../templates/data_guide_template.Rnw\")\n    with open(template_path, \"r\") as template_file:\n        template_str = template_file.read()\n\n    replacements = {\n        \"{{release}}\": genie_version,\n        \"{{database_synid}}\": database_mapping,\n        \"{{oncotree}}\": oncotree_version.replace(\"_\", \"\\\\_\"),\n        \"{{genie_banner}}\": os.path.join(PWD, \"../genie_banner.png\"),\n    }\n\n    for search in replacements:\n        replacement = replacements[search]\n        # If no replacement value is passed in, don't replace\n        if replacement is not None:\n            template_str = template_str.replace(search, replacement)\n\n    with open(os.path.join(PWD, \"data_guide.Rnw\"), \"w\") as data_guide_file:\n        data_guide_file.write(template_str)\n\n    subprocess.check_call(\n        [\"R\", \"CMD\", \"Sweave\", \"--pdf\", os.path.join(PWD, \"data_guide.Rnw\")]\n    )\n    return \"data_guide.pdf\"\n</code></pre>"},{"location":"reference/main_pipeline_commands/database_to_staging/#bin.database_to_staging.main","title":"<code>main(genie_version, processing_date, cbioportal_path, oncotree_link=None, consortium_release_cutoff=184, test=False, staging=False, debug=False, skip_mutationsincis=False)</code>","text":"<ul> <li>Does parameter checks</li> <li>Updates process tracking start</li> <li>initiates database to staging</li> <li>create case lists</li> <li>revise meta files</li> <li>run cBioPortal validation</li> <li>create link versions</li> <li>update process tracking end</li> <li>Create dashboard tables and plots</li> </ul> PARAMETER DESCRIPTION <code>genie_version</code> <p>GENIE version,</p> <p> </p> <code>processing_date</code> <p>processing date</p> <p> </p> <code>cbioportal_path</code> <p>Path to cbioportal validator</p> <p> </p> <code>oncotree_link</code> <p>Link to oncotree codes</p> <p> DEFAULT: <code>None</code> </p> <code>consortium_release_cutoff</code> <p>release cut off value in days</p> <p> DEFAULT: <code>184</code> </p> <code>test</code> <p>Test flag, uses test databases</p> <p> DEFAULT: <code>False</code> </p> <code>staging</code> <p>Staging flag, uses staging databases</p> <p> DEFAULT: <code>False</code> </p> <code>debug</code> <p>Synapse debug flag</p> <p> DEFAULT: <code>False</code> </p> <code>skip_mutationsincis</code> <p>Skip mutation in cis filter</p> <p> DEFAULT: <code>False</code> </p> Source code in <code>bin/database_to_staging.py</code> <pre><code>def main(\n    genie_version,\n    processing_date,\n    cbioportal_path,\n    oncotree_link=None,\n    consortium_release_cutoff=184,\n    test=False,\n    staging=False,\n    debug=False,\n    skip_mutationsincis=False,\n):\n    \"\"\"\n    - Does parameter checks\n    - Updates process tracking start\n    - initiates database to staging\n    - create case lists\n    - revise meta files\n    - run cBioPortal validation\n    - create link versions\n    - update process tracking end\n    - Create dashboard tables and plots\n\n    Args:\n        genie_version: GENIE version,\n        processing_date: processing date\n        cbioportal_path: Path to cbioportal validator\n        oncotree_link: Link to oncotree codes\n        consortium_release_cutoff: release cut off value in days\n        test: Test flag, uses test databases\n        staging: Staging flag, uses staging databases\n        debug:  Synapse debug flag\n        skip_mutationsincis: Skip mutation in cis filter\n    \"\"\"\n    # HACK: Delete all existing files first\n    process_functions.rmFiles(database_to_staging.GENIE_RELEASE_DIR)\n\n    syn = process_functions.synapse_login(debug=debug)\n    # HACK: Use project id instead of this...\n    if test:\n        databaseSynIdMappingId = \"syn11600968\"\n        genie_version = \"TESTING\"\n    elif staging:\n        databaseSynIdMappingId = \"syn12094210\"\n    else:\n        databaseSynIdMappingId = \"syn10967259\"\n    # Database/folder syn id mapping\n    databaseSynIdMapping = syn.tableQuery(\n        \"select * from {}\".format(databaseSynIdMappingId)\n    )\n    databaseSynIdMappingDf = databaseSynIdMapping.asDataFrame()\n    # databaseSynIdMappingDf.index = databaseSynIdMappingDf.Database\n    # del databaseSynIdMappingDf['Database']\n    # databaseSynIdMappingDf.to_dict()\n\n    if oncotree_link is None:\n        oncoLink = databaseSynIdMappingDf[\"Id\"][\n            databaseSynIdMappingDf[\"Database\"] == \"oncotreeLink\"\n        ].values[0]\n        oncoLinkEnt = syn.get(oncoLink)\n        oncotree_link = oncoLinkEnt.externalURL\n\n    # Check if you can connect to oncotree link,\n    # if not then don't run validation / processing\n    process_functions.checkUrl(oncotree_link)\n\n    cbioValidatorPath = os.path.join(\n        cbioportal_path, \"core/src/main/scripts/importer/validateData.py\"\n    )\n    assert os.path.exists(cbioValidatorPath), \"Please specify correct cbioportalPath\"\n    syn.table_query_timeout = 50000\n\n    consortiumSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"consortium\"\n    ].values[0]\n    processTrackerSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"processTracker\"\n    ].values[0]\n    # get syn id of case list folder in consortium release\n    # caseListSynId = findCaseListId(syn, consortiumSynId)\n    caseListSynId = database_to_staging.search_or_create_folder(\n        syn, consortiumSynId, \"case_lists\"\n    )\n\n    if not staging:\n        load.update_process_trackingdf(\n            syn=syn,\n            process_trackerdb_synid=processTrackerSynId,\n            center=\"SAGE\",\n            process_type=\"dbToStage\",\n            start=True,\n        )\n\n    centerMappingSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"centerMapping\"\n    ].values[0]\n    # Only release files where release is true\n    center_mapping = syn.tableQuery(\n        \"SELECT * FROM {} where release is true\".format(centerMappingSynId)\n    )\n    center_mappingdf = center_mapping.asDataFrame()\n    processingDate = datetime.datetime.strptime(processing_date, \"%b-%Y\")\n\n    logger.info(\"STAGING TO CONSORTIUM\")\n    genePanelEntities = database_to_staging.stagingToCbio(\n        syn,\n        processingDate,\n        genie_version,\n        center_mappingdf,\n        databaseSynIdMappingDf,\n        oncotree_url=oncotree_link,\n        consortiumReleaseCutOff=consortium_release_cutoff,\n        current_release_staging=staging,\n        skipMutationsInCis=skip_mutationsincis,\n        test=test,\n    )\n\n    # Create case lists files\n    logger.info(\"CREATE CASE LIST FILES\")\n    # Remove old caselists first\n    if not os.path.exists(database_to_staging.CASE_LIST_PATH):\n        os.mkdir(database_to_staging.CASE_LIST_PATH)\n    caselists = os.listdir(database_to_staging.CASE_LIST_PATH)\n    for caselist in caselists:\n        os.remove(os.path.join(database_to_staging.CASE_LIST_PATH, caselist))\n    clinical_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"data_clinical.txt\",\n    )\n    assay_information_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"assay_information.txt\",\n    )\n    create_case_lists.main(\n        clinical_path,\n        assay_information_path,\n        database_to_staging.CASE_LIST_PATH,\n        \"genie_private\",\n    )\n    caseListFiles = os.listdir(database_to_staging.CASE_LIST_PATH)\n    caseListEntities = []\n    for casePath in caseListFiles:\n        casePath = os.path.join(database_to_staging.CASE_LIST_PATH, casePath)\n        caseListEntities.append(\n            load.store_file(\n                syn=syn,\n                filepath=casePath,\n                parentid=caseListSynId,\n                version_comment=genie_version,\n            )\n        )\n\n    logger.info(\"REMOVING UNNECESSARY FILES\")\n    # genie_files = os.listdir(database_to_staging.GENIE_RELEASE_DIR)\n    # for genie_file in genie_files:\n    #     if (\n    #         genie_version not in genie_file\n    #         and \"meta\" not in genie_file\n    #         and \"case_lists\" not in genie_file\n    #     ):\n    #         os.remove(os.path.join(database_to_staging.GENIE_RELEASE_DIR, genie_file))\n    # os.remove(clinical_path)\n\n    logger.info(\"REVISE METADATA FILES\")\n    database_to_staging.revise_metadata_files(syn, consortiumSynId, genie_version)\n\n    logger.info(\"CBIO VALIDATION\")\n\n    # Must be exit 0 because the validator sometimes fails,\n    # but we still want to capture the output\n\n    command = [\n        cbioValidatorPath,\n        \"-s\",\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"-n\",\n        \"; exit 0\",\n    ]\n    cbioOutput = subprocess.check_output(\" \".join(command), shell=True)\n    logger.info(cbioOutput.decode(\"utf-8\"))\n\n    cbio_validator_log = f\"cbioValidatorLogsConsortium_{genie_version}.txt\"\n    if not test and not staging:\n        log_folder_synid = databaseSynIdMappingDf[\"Id\"][\n            databaseSynIdMappingDf[\"Database\"] == \"logs\"\n        ].values[0]\n        with open(cbio_validator_log, \"w\") as cbio_log:\n            cbio_log.write(cbioOutput.decode(\"utf-8\"))\n        syn.store(synapseclient.File(cbio_validator_log, parentId=log_folder_synid))\n        os.remove(cbio_validator_log)\n    # HACK: Instead of doing this, files should be written to a tempdir...\n    # logger.info(\"REMOVING OLD FILES\")\n\n    # process_functions.rmFiles(database_to_staging.CASE_LIST_PATH)\n    # private_cna_meta_path = os.path.join(\n    #     database_to_staging.GENIE_RELEASE_DIR, \"genie_private_meta_cna_hg19_seg.txt\"\n    # )\n    # if os.path.exists(private_cna_meta_path):\n    #     os.unlink(private_cna_meta_path)\n\n    logger.info(\"CREATING LINK VERSION\")\n    # Returns release and case list folder\n    _ = database_to_staging.create_link_version(\n        syn, genie_version, caseListEntities, genePanelEntities, databaseSynIdMappingDf\n    )\n\n    if not staging:\n        load.update_process_trackingdf(\n            syn=syn,\n            process_trackerdb_synid=processTrackerSynId,\n            center=\"SAGE\",\n            process_type=\"dbToStage\",\n            start=False,\n        )\n\n    logger.info(\"DASHBOARD UPDATE\")\n    # Only run dashboard update if not testing or staging\n    if not args.test and not args.staging:\n        dashboard_table_updater.run_dashboard(\n            syn, databaseSynIdMappingDf, genie_version, staging=staging\n        )\n    generate_dashboard_html(genie_version, staging=staging, testing=test)\n    logger.info(\"DASHBOARD UPDATE COMPLETE\")\n    logger.info(\"AUTO GENERATE DATA GUIDE\")\n\n    # TODO: remove data guide code\n    # oncotree_version = oncotree_link.split(\"=\")[1]\n    # data_guide_pdf = generate_data_guide(\n    #    genie_version,\n    #    oncotree_version=oncotree_version,\n    #    database_mapping=databaseSynIdMappingId,\n    # )\n    # load.store_file(\n    #    syn=syn,\n    #    filepath=data_guide_pdf,\n    #    version_comment=genie_version,\n    #    parentid=folders[\"release_folder\"],\n    # )\n    logger.info(\"COMPLETED DATABASE TO STAGING\")\n</code></pre>"},{"location":"reference/main_pipeline_commands/input_to_database/","title":"Input to Database","text":""},{"location":"reference/main_pipeline_commands/input_to_database/#bin.input_to_database","title":"<code>bin.input_to_database</code>","text":"<p>Workflow  crawl Synapse folder for a center, validate, and update database tables.</p> <p>Known issues:</p> <ul> <li> <p>All data per center is downloaded every time (even if not changed)</p> <p>This increases the length of validation because not all data is changed. The complexity is around cross file validation.  There needs to be a way to determine relationships between files so that if a file depends on another only those files are downloaded.  This is harder for VCFs as there may be thousands of them</p> </li> <li> <p>Retraction from the sample/patient table is inefficient.  The sample and patient tables will first contain the data, and it will shortly be removed from the synapse tables.</p> </li> <li>Newer implementation of cross-file validation is available but clinical data is still using the older implementation which unnecesssarily complicates the code.</li> </ul> <pre><code>flowchart TD\n    A[\"For each center\"] --&gt; B[\"Extract center input files\"]\n    B --&gt; D{\"Has the center uploaded any data\"}\n    D -- No --&gt; E[\"Log: No files uploaded\"]\n    E --&gt; F[\"End\"]\n    D -- Yes --&gt; G[\"Validate files\"]\n    G --&gt; H{\"Are there valid files?\"}\n    H -- No --&gt; I[\"Log: No valid files\"]\n    I --&gt; F\n    H -- Yes --&gt; J{\"only_validate flag set?\"}\n    J -- Yes --&gt; K[\"Log: Validation only\"]\n    K --&gt; F\n    J -- No --&gt; L[\"Process valid files\"]\n    %% L --&gt; M[\"Update process tracker with start time\"]\n    L --&gt; N[\"Process files based on fileType\"]\n    %% N --&gt; O[\"Update process tracker with end time\"]\n    N --&gt; P[\"Upload processed data per file type into internal Synapse Tables and Center staging folders\"]\n    P --&gt; Q[\"Retract samples and patients based on retraction tables\"]\n    Q --&gt; F\n\n    subgraph \"File Type Processing\"\n        N --&gt; N1[\"clinical\"]\n        N1 --&gt; N1a[\"Remap NAACCR and other values to cbioportal accepted values\"]\n        N1a --&gt; N1b[\"Map SEQ_DATE to SEQ_YEAR\"]\n        N1b --&gt; N1d[\"Redact PHI\"]\n        N1d --&gt; N1e[\"Exclude all samples with invalid oncotree codes\"]\n\n        N --&gt; N2[\"maf\"]\n        N2 --&gt; N2a[\"Preprocessing: light edits to maf\"]\n        N2a --&gt; N2b[\"Re-annotate using Genome Nexus\"]\n\n        N --&gt; N3[\"vcf\"]\n        N3 --&gt; N3a[\"Preprocessing: Convert VCF to MAF\"]\n        N3a --&gt; N2b\n\n        N --&gt; N5[\"cna\"]\n        N5 --&gt; N5a[\"Remap gene symbols using processed BED data\"]\n        N5a --&gt; N5b[\"Duplicate genes are merged if possible after remapping\"]\n\n        N --&gt; N9[\"BED\"]\n        N9 --&gt; N9a[\"Remap genes to hg19 positions\"]\n        N9a --&gt; N9b[\"Gene panel files are created and uploaded to staging folder\"]\n\n        N --&gt; N6[\"assay information, mutation in cis, sample/patient retraction, seg, Structural Variants\"]\n        N6 --&gt; N6a[\"Extract data and mild transforms\"]\n\n    end\n\n    %% %% Subgraph for get_center_input_files function\n    %% subgraph \"get_center_input_files Function\"\n    %%     B --&gt; B1[\"Iterate over all files uploaded by center\"]\n    %%     B1 --&gt; B6{\"Does file name end with '.vcf' and process != 'mutation'?\"}\n    %%     B6 -- Yes --&gt; B7[\"Skip this file\"]\n    %%     B6 -- No --&gt; B8[\"Download file\"]\n    %%     B8 --&gt; B10{\"Is it a clinical file?\"}\n    %%     B10 -- Yes --&gt; B11[\"Append entity to clinicalpair_entities\"]\n    %%     B10 -- No --&gt; B12[\"Append [entity] to prepared_center_file_list\"]\n    %%     B11 --&gt; B13\n    %%     B12 --&gt; B13\n    %%     B13{\"Is clinicalpair_entities not empty after loop?\"}\n    %%     B13 -- Yes --&gt; B14[\"Append clinicalpair_entities to prepared_center_file_list\"]\n    %%     B14 --&gt; B15[\"Return prepared_center_file_list\"]\n    %%     B13 -- No --&gt; B15\n    %% end</code></pre>"},{"location":"reference/main_pipeline_commands/input_to_database/#bin.input_to_database-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_commands/input_to_database/#bin.input_to_database.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/input_to_database/#bin.input_to_database.parser","title":"<code>parser = argparse.ArgumentParser(description='GENIE center ')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/input_to_database/#bin.input_to_database.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_commands/input_to_database/#bin.input_to_database-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_commands/input_to_database/#bin.input_to_database.main","title":"<code>main(process, project_id, center=None, delete_old=False, only_validate=False, oncotree_link=None, genie_annotation_pkg=None, create_new_maf_database=False, debug=False, format_registry=None)</code>","text":"<p>Invoke the GENIE ETL pipeline from data input files to synapse tables</p> PARAMETER DESCRIPTION <code>process</code> <p>main or mutation processing</p> <p> TYPE: <code>str</code> </p> <code>project_id</code> <p>Synapse project id that houses GENIE project</p> <p> TYPE: <code>str</code> </p> <code>center</code> <p>GENIE center. Defaults to None.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>delete_old</code> <p>True to delete all old input/processed files.                          Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>only_validate</code> <p>True if only validate files. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>oncotree_link</code> <p>Link to oncotree version. Defaults to None.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>genie_annotation_pkg</code> <p>vcf/maf conversion tools.                                   Defaults to None.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>create_new_maf_database</code> <p>To create new maf table.                                       Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>debug</code> <p>Debug mode. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>format_registry</code> <p>File format registry python package.                              Defaults to None.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If invalid center name is specified</p> <code>Exception</code> <p>If processing is already happening.</p> Source code in <code>bin/input_to_database.py</code> <pre><code>def main(\n    process: str,\n    project_id: str,\n    center=None,\n    delete_old=False,\n    only_validate=False,\n    oncotree_link=None,\n    genie_annotation_pkg=None,\n    create_new_maf_database=False,\n    debug=False,\n    format_registry=None,\n):\n    \"\"\"Invoke the GENIE ETL pipeline from data input files to synapse tables\n\n    Args:\n        process (str): main or mutation processing\n        project_id (str): Synapse project id that houses GENIE project\n        center (str, optional): GENIE center. Defaults to None.\n        delete_old (bool, optional): True to delete all old input/processed files.\n                                     Defaults to False.\n        only_validate (bool, optional): True if only validate files. Defaults to False.\n        oncotree_link (str, optional): Link to oncotree version. Defaults to None.\n        genie_annotation_pkg (str, optional): vcf/maf conversion tools.\n                                              Defaults to None.\n        create_new_maf_database (bool, optional): To create new maf table.\n                                                  Defaults to False.\n        debug (bool, optional): Debug mode. Defaults to False.\n        format_registry (str, optional): File format registry python package.\n                                         Defaults to None.\n\n    Raises:\n        ValueError: If invalid center name is specified\n        Exception: If processing is already happening.\n    \"\"\"\n\n    syn = process_functions.synapse_login(debug=debug)\n\n    # Get project GENIE configurations\n    genie_config = extract.get_genie_config(syn=syn, project_id=project_id)\n\n    # Filter for specific center\n    if center is not None:\n        if center not in genie_config[\"center_config\"].keys():\n            raise ValueError(\n                \"Must specify one of these centers: {}\".format(\n                    \", \".join(genie_config[\"center_config\"].keys())\n                )\n            )\n        centers = [center]\n    else:\n        # TODO: add in logic to exclude sites from processing\n        centers = list(genie_config[\"center_config\"].keys())\n\n    # HACK: Modify oncotree link config\n    if oncotree_link is None:\n        onco_link_ent = syn.get(genie_config[\"oncotreeLink\"])\n        oncotree_link = onco_link_ent.externalURL\n    genie_config[\"oncotreeLink\"] = oncotree_link\n    # Check if you can connect to oncotree link,\n    # if not then don't run validation / processing\n    process_functions.checkUrl(genie_config[\"oncotreeLink\"])\n\n    # HACK: Add genie annotation package to config\n    if process == \"mutation\" and genie_annotation_pkg is None:\n        raise ValueError(\"Must define genie annotation pkg if mutation processing\")\n    genie_config[\"genie_annotation_pkg\"] = genie_annotation_pkg\n\n    # HACK: This is essential, because Synapse has concurrency update issues\n    center_mapping_ent = syn.get(genie_config[\"centerMapping\"])\n    if center_mapping_ent.get(\"isProcessing\", [\"True\"])[0] == \"True\":\n        raise Exception(\n            \"Processing/validation is currently happening.  Please change/add the \"\n            f\"'isProcessing' annotation on {genie_config['centerMapping']} \"\n            \"to False to enable processing\"\n        )\n    else:\n        center_mapping_ent.isProcessing = \"True\"\n        center_mapping_ent = syn.store(center_mapping_ent)\n\n    # HACK: Create new maf database, should only happen once if its specified\n    # Will modify genie configuration\n    if create_new_maf_database:\n        today = date.today()\n        table_name = f\"Narrow MAF Database - {today}\"\n        # filetype = \"vcf2maf\"\n        # save maf table to testing or production project as the mode\n        new_tables = process_functions.create_new_fileformat_table(\n            syn, \"vcf2maf\", table_name, project_id, project_id\n        )\n        syn.setPermissions(new_tables[\"newdb_ent\"].id, 3326313, [])\n        genie_config[\"vcf2maf\"] = new_tables[\"newdb_ent\"].id\n\n    # Get file format classes\n    format_registry = config.collect_format_types(args.format_registry_packages)\n\n    # Start GENIE processing\n    for process_center in centers:\n        # Check if the genie genome nexus is up, if not then don't run\n        # processing\n        process_functions.checkUrl(\"http://genie.genomenexus.org/\")\n        input_to_database.center_input_to_database(\n            syn=syn,\n            project_id=project_id,\n            center=process_center,\n            process=process,\n            only_validate=only_validate,\n            delete_old=delete_old,\n            format_registry=format_registry,\n            genie_config=genie_config,\n        )\n\n    # HACK: To ensure that this is the new entity\n    center_mapping_ent = syn.get(genie_config[\"centerMapping\"])\n    center_mapping_ent.isProcessing = \"False\"\n    # No need to return ent variable because it is unused\n    syn.store(center_mapping_ent)\n\n    error_tracker_synid = genie_config[\"errorTracker\"]\n    # Only write out invalid reasons if the center\n    # isnt specified and if only validate\n    if center is None and only_validate:\n        logger.info(\"WRITING INVALID REASONS TO CENTER STAGING DIRS\")\n        write_invalid_reasons.write(\n            syn, genie_config[\"centerMapping\"], error_tracker_synid\n        )\n    logger.info(\"INPUT TO DATABASE COMPLETE\")\n</code></pre>"},{"location":"reference/main_pipeline_steps/consortium_to_public/","title":"Consortium to Public","text":""},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public","title":"<code>genie.consortium_to_public</code>","text":"<p>Converts consortium release files to public release files</p>"},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public.stdout_handler","title":"<code>stdout_handler = logging.StreamHandler(stream=(sys.stdout))</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public._copyRecursive","title":"<code>_copyRecursive(syn, entity, destinationId, mapping=None, skipCopyAnnotations=False, **kwargs)</code>","text":"<p>NOTE: This is a copy of the function found here: https://github.com/Sage-Bionetworks/synapsePythonClient/blob/develop/synapseutils/copy_functions.py#L409 This was copied because there is a restriction that doesn't allow for copying entities with access requirements</p> <p>Recursively copies synapse entites, but does not copy the wikis</p> PARAMETER DESCRIPTION <code>syn</code> <p>A Synapse object with user's login</p> <p> TYPE: <code>Synapse</code> </p> <code>entity</code> <p>A synapse entity ID</p> <p> TYPE: <code>str</code> </p> <code>destinationId</code> <p>Synapse ID of a folder/project that the copied entity is being copied to</p> <p> TYPE: <code>str</code> </p> <code>mapping</code> <p>A mapping of the old entities to the new entities</p> <p> TYPE: <code>Dict[str, str]</code> DEFAULT: <code>None</code> </p> <code>skipCopyAnnotations</code> <p>Skips copying the annotations                     Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>a mapping between the original and copied entity: {'syn1234':'syn33455'}</p> Source code in <code>genie/load.py</code> <pre><code>def _copyRecursive(\n    syn: synapseclient.Synapse,\n    entity: str,\n    destinationId: str,\n    mapping: Dict[str, str] = None,\n    skipCopyAnnotations: bool = False,\n    **kwargs,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    NOTE: This is a copy of the function found here: https://github.com/Sage-Bionetworks/synapsePythonClient/blob/develop/synapseutils/copy_functions.py#L409\n    This was copied because there is a restriction that doesn't allow for copying entities with access requirements\n\n    Recursively copies synapse entites, but does not copy the wikis\n\n    Arguments:\n        syn: A Synapse object with user's login\n        entity: A synapse entity ID\n        destinationId: Synapse ID of a folder/project that the copied entity is being copied to\n        mapping: A mapping of the old entities to the new entities\n        skipCopyAnnotations: Skips copying the annotations\n                                Default is False\n\n    Returns:\n        a mapping between the original and copied entity: {'syn1234':'syn33455'}\n    \"\"\"\n\n    version = kwargs.get(\"version\", None)\n    setProvenance = kwargs.get(\"setProvenance\", \"traceback\")\n    excludeTypes = kwargs.get(\"excludeTypes\", [])\n    updateExisting = kwargs.get(\"updateExisting\", False)\n    if mapping is None:\n        mapping = dict()\n    # Check that passed in excludeTypes is file, table, and link\n    if not isinstance(excludeTypes, list):\n        raise ValueError(\"Excluded types must be a list\")\n    elif not all([i in [\"file\", \"link\", \"table\"] for i in excludeTypes]):\n        raise ValueError(\n            \"Excluded types can only be a list of these values: file, table, and link\"\n        )\n\n    ent = syn.get(entity, downloadFile=False)\n    if ent.id == destinationId:\n        raise ValueError(\"destinationId cannot be the same as entity id\")\n\n    if (isinstance(ent, Project) or isinstance(ent, Folder)) and version is not None:\n        raise ValueError(\"Cannot specify version when copying a project of folder\")\n\n    if not isinstance(ent, (Project, Folder, File, Link, Schema, Entity)):\n        raise ValueError(\"Not able to copy this type of file\")\n\n    permissions = syn.restGET(\"/entity/{}/permissions\".format(ent.id))\n    # Don't copy entities without DOWNLOAD permissions\n    if not permissions[\"canDownload\"]:\n        syn.logger.warning(\n            \"%s not copied - this file lacks download permission\" % ent.id\n        )\n        return mapping\n\n    # HACK: These lines of code were removed to allow for data with access requirements to be copied\n    # https://github.com/Sage-Bionetworks/synapsePythonClient/blob/2909fa778e814f62f6fe6ce2d951ce58c0080a4e/synapseutils/copy_functions.py#L464-L470\n\n    copiedId = None\n\n    if isinstance(ent, Project):\n        project = syn.get(destinationId)\n        if not isinstance(project, Project):\n            raise ValueError(\n                \"You must give a destinationId of a new project to copy projects\"\n            )\n        copiedId = destinationId\n        # Projects include Docker repos, and Docker repos cannot be copied\n        # with the Synapse rest API. Entity views currently also aren't\n        # supported\n        entities = syn.getChildren(\n            entity, includeTypes=[\"folder\", \"file\", \"table\", \"link\"]\n        )\n        for i in entities:\n            mapping = _copyRecursive(\n                syn,\n                i[\"id\"],\n                destinationId,\n                mapping=mapping,\n                skipCopyAnnotations=skipCopyAnnotations,\n                **kwargs,\n            )\n\n        if not skipCopyAnnotations:\n            project.annotations = ent.annotations\n            syn.store(project)\n    elif isinstance(ent, Folder):\n        copiedId = synu.copy_functions._copyFolder(\n            syn,\n            ent.id,\n            destinationId,\n            mapping=mapping,\n            skipCopyAnnotations=skipCopyAnnotations,\n            **kwargs,\n        )\n    elif isinstance(ent, File) and \"file\" not in excludeTypes:\n        copiedId = synu.copy_functions._copyFile(\n            syn,\n            ent.id,\n            destinationId,\n            version=version,\n            updateExisting=updateExisting,\n            setProvenance=setProvenance,\n            skipCopyAnnotations=skipCopyAnnotations,\n        )\n    elif isinstance(ent, Link) and \"link\" not in excludeTypes:\n        copiedId = synu.copy_functions._copyLink(\n            syn, ent.id, destinationId, updateExisting=updateExisting\n        )\n    elif isinstance(ent, Schema) and \"table\" not in excludeTypes:\n        copiedId = synu.copy_functions._copyTable(\n            syn, ent.id, destinationId, updateExisting=updateExisting\n        )\n    # This is currently done because copyLink returns None sometimes\n    if copiedId is not None:\n        mapping[ent.id] = copiedId\n        syn.logger.info(\"Copied %s to %s\" % (ent.id, copiedId))\n    else:\n        syn.logger.info(\"%s not copied\" % ent.id)\n    return mapping\n</code></pre>"},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public.commonVariantFilter","title":"<code>commonVariantFilter(mafDf)</code>","text":"<p>This filter returns variants to keep</p> PARAMETER DESCRIPTION <code>mafDf</code> <p>Maf dataframe</p> <p> </p> Source code in <code>genie/consortium_to_public.py</code> <pre><code>def commonVariantFilter(mafDf):\n    \"\"\"\n    This filter returns variants to keep\n\n    Args:\n        mafDf: Maf dataframe\n    \"\"\"\n    mafDf[\"FILTER\"] = mafDf[\"FILTER\"].fillna(\"\")\n    to_keep = [\"common_variant\" not in i for i in mafDf[\"FILTER\"]]\n    mafDf = mafDf[to_keep]\n    return mafDf\n</code></pre>"},{"location":"reference/main_pipeline_steps/consortium_to_public/#genie.consortium_to_public.consortiumToPublic","title":"<code>consortiumToPublic(syn, processingDate, genie_version, releaseId, databaseSynIdMappingDf, publicReleaseCutOff=365)</code>","text":"Source code in <code>genie/consortium_to_public.py</code> <pre><code>def consortiumToPublic(\n    syn,\n    processingDate,\n    genie_version,\n    releaseId,\n    databaseSynIdMappingDf,\n    publicReleaseCutOff=365,\n):\n    cna_path = os.path.join(database_to_staging.GENIE_RELEASE_DIR, \"data_CNA.txt\")\n    clinical_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR, \"data_clinical.txt\"\n    )\n    clinical_sample_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"data_clinical_sample.txt\",\n    )\n    clinicl_patient_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"data_clinical_patient.txt\",\n    )\n    data_gene_panel_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR, \"data_gene_matrix.txt\"\n    )\n    mutations_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"data_mutations_extended.txt\",\n    )\n    seg_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR,\n        \"data_cna_hg19.seg\",\n    )\n    combined_bed_path = os.path.join(\n        database_to_staging.GENIE_RELEASE_DIR, \"genie_combined.bed\"\n    )\n\n    if not os.path.exists(database_to_staging.GENIE_RELEASE_DIR):\n        os.mkdir(database_to_staging.GENIE_RELEASE_DIR)\n    if not os.path.exists(database_to_staging.CASE_LIST_PATH):\n        os.mkdir(database_to_staging.CASE_LIST_PATH)\n\n    # public release preview\n    public_release_preview = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"public\"\n    ].values[0]\n    public_release_preview_caselist = database_to_staging.search_or_create_folder(\n        syn=syn, parentid=public_release_preview, folder_name=\"case_lists\"\n    )\n\n    #######################################################################\n    # Sponsored projects filter\n    #######################################################################\n    # if before release date -&gt; go into staging consortium\n    # if after date -&gt; go into public\n    # sponsoredReleaseDate = syn.tableQuery('SELECT * FROM syn8545108')\n    # sponsoredReleaseDateDf = sponsoredReleaseDate.asDataFrame()\n    # sponsoredProjectSamples = syn.tableQuery('SELECT * FROM syn8545106')\n    # sponsoredProjectSamplesDf = sponsoredProjectSamples.asDataFrame()\n    # sponsoredProjectsDf = sponsoredProjectSamplesDf.merge(\n    #     sponsoredReleaseDateDf, left_on=\"sponsoredProject\",\n    #     right_on=\"sponsoredProjects\")\n    # dates = sponsoredProjectsDf['releaseDate'].apply(\n    #     lambda date: datetime.datetime.strptime(date, '%b-%Y'))\n    # publicReleaseSamples = sponsoredProjectsDf['genieSampleId'][\n    #     dates &lt; processingDate]\n    #######################################################################\n\n    # SEQ_DATE filter\n    # Jun-2015, given processing date (today) -&gt; public release\n    # (processing date - Jun-2015 &gt; 12 months)\n    consortiumReleaseWalk = synapseutils.walk(syn, releaseId)\n\n    consortiumRelease = next(consortiumReleaseWalk)\n    for filename, synid in consortiumRelease[2]:\n        if filename == \"data_clinical.txt\":\n            clinical = syn.get(synid, followLink=True)\n        elif filename == \"data_gene_matrix.txt\":\n            gene_matrix = syn.get(synid, followLink=True)\n        elif filename == \"assay_information.txt\":\n            assay_info = syn.get(synid, followLink=True)\n\n    clinicalDf = pd.read_csv(clinical.path, sep=\"\\t\", comment=\"#\")\n    gene_matrixdf = pd.read_csv(gene_matrix.path, sep=\"\\t\")\n\n    removeForPublicSamples = process_functions.seqDateFilter(\n        clinicalDf, processingDate, publicReleaseCutOff\n    )\n    logger.info(\"SAMPLE CLASS FILTER\")\n    # comment back in when public release filter back on\n    # publicReleaseSamples = publicReleaseSamples.append(keepForPublicSamples)\n    # Make sure all null oncotree codes are removed\n    clinicalDf = clinicalDf[~clinicalDf[\"ONCOTREE_CODE\"].isnull()]\n    publicReleaseSamples = clinicalDf.SAMPLE_ID[\n        ~clinicalDf.SAMPLE_ID.isin(removeForPublicSamples)\n    ]\n\n    existing_seq_dates = clinicalDf.SEQ_DATE[\n        clinicalDf.SAMPLE_ID.isin(publicReleaseSamples)\n    ]\n\n    logger.info(\n        \"SEQ_DATES for public release: \"\n        + \", \".join(set(existing_seq_dates.astype(str)))\n    )\n\n    # Clinical release scope filter\n    clinical_tier_release_scope_synid = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"clinical_tier_release_scope\"\n    ][0]\n    publicRelease = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT * FROM {clinical_tier_release_scope_synid} where releaseScope = 'public'\",\n    )\n\n    # check if SAMPLE_CLASS is present\n    if not process_functions.check_values_in_column(\n        publicRelease, \"fieldName\", \"SAMPLE_CLASS\"\n    ):\n        logger.error(\"Must have SAMPLE_CLASS column in the public release scope.\")\n\n    allClin = clinicalDf[clinicalDf[\"SAMPLE_ID\"].isin(publicReleaseSamples)]\n    # check if cfDNA samples are present\n    if not process_functions.check_values_in_column(allClin, \"SAMPLE_CLASS\", \"cfDNA\"):\n        logger.error(\n            \"cfDNA samples should not be filtered out in the clinical dataframe.\"\n        )\n\n    allClin.to_csv(clinical_path, sep=\"\\t\", index=False)\n\n    gene_matrixdf = gene_matrixdf[gene_matrixdf[\"SAMPLE_ID\"].isin(publicReleaseSamples)]\n    gene_matrixdf.to_csv(data_gene_panel_path, sep=\"\\t\", index=False)\n    load.store_file(\n        syn=syn,\n        filepath=data_gene_panel_path,\n        parentid=public_release_preview,\n        version_comment=genie_version,\n        name=\"data_gene_matrix.txt\",\n    )\n    load.store_file(\n        syn=syn,\n        filepath=clinical_path,\n        parentid=public_release_preview,\n        version_comment=genie_version,\n        name=\"data_clinical.txt\",\n    )\n\n    create_case_lists.main(\n        clinical_path,\n        assay_info.path,\n        database_to_staging.CASE_LIST_PATH,\n        \"genie_public\",\n    )\n\n    caseListFiles = os.listdir(database_to_staging.CASE_LIST_PATH)\n    caseListEntities = []\n    for casePath in caseListFiles:\n        casePath = os.path.join(database_to_staging.CASE_LIST_PATH, casePath)\n        caseListEntities.append(\n            load.store_file(\n                syn=syn,\n                filepath=casePath,\n                parentid=public_release_preview_caselist,\n                version_comment=genie_version,\n            )\n        )\n\n    # Grab mapping table to fill in clinical headers\n    clinical_code_to_desc_map_synid = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"clinical_code_to_desc_map\"\n    ][0]\n    mapping = extract.get_syntabledf(\n        syn=syn, query_string=f\"SELECT * FROM {clinical_code_to_desc_map_synid}\"\n    )\n    genePanelEntities = []\n    for entName, entId in consortiumRelease[2]:\n        is_deprecated_file = entName in [\"data_fusions.txt\"]\n        # skip files to convert\n        if (\n            entName.startswith(\"data_linear\")\n            or \"meta_\" in entName\n            or entName.endswith(\".html\")\n            or entName\n            in [\n                \"data_clinical_sample.txt\",\n                \"data_gene_matrix.txt\",\n                \"data_clinical_patient.txt\",\n                \"data_guide.pdf\",\n                \"release_notes.pdf\",\n                \"samples_to_retract.csv\",\n                \"non_somatic.csv\",\n                \"snv_as_dnp.csv\",\n                \"snv_as_onp.csv\",\n                \"duplicated_variants.csv\",\n            ]\n            or is_deprecated_file\n        ):\n            # data_gene_matrix was processed above because it had to be\n            # used for generating caselists\n            continue\n        if entName == \"data_clinical.txt\":\n            patientCols = publicRelease[\"fieldName\"][\n                publicRelease[\"level\"] == \"patient\"\n            ].tolist()\n            sampleCols = [\"PATIENT_ID\"]\n            sampleCols.extend(\n                publicRelease[\"fieldName\"][publicRelease[\"level\"] == \"sample\"].tolist()\n            )\n            # clinicalDf is defined on line 127\n            clinicalDf = clinicalDf[clinicalDf[\"SAMPLE_ID\"].isin(publicReleaseSamples)]\n\n            # Delete columns that are private scope\n            # for private in privateRelease:\n            #   del clinicalDf[private]\n            process_functions.addClinicalHeaders(\n                clinicalDf,\n                mapping,\n                patientCols,\n                sampleCols,\n                clinical_sample_path,\n                clinicl_patient_path,\n            )\n\n            load.store_file(\n                syn=syn,\n                filepath=clinical_sample_path,\n                parentid=public_release_preview,\n                version_comment=genie_version,\n                name=\"data_clinical_sample.txt\",\n            )\n            load.store_file(\n                syn=syn,\n                filepath=clinicl_patient_path,\n                parentid=public_release_preview,\n                version_comment=genie_version,\n                name=\"data_clinical_patient.txt\",\n            )\n\n        elif \"mutation\" in entName:\n            mutation = syn.get(entId, followLink=True)\n            mutationDf = pd.read_csv(mutation.path, sep=\"\\t\", comment=\"#\")\n            # mutationDf = commonVariantFilter(mutationDf)\n            mutationDf[\"FILTER\"] = \"PASS\"\n            mutationDf = mutationDf[\n                mutationDf[\"Tumor_Sample_Barcode\"].isin(publicReleaseSamples)\n            ]\n            text = process_functions.removeFloat(mutationDf)\n            with open(mutations_path, \"w\") as f:\n                f.write(text)\n            load.store_file(\n                syn=syn,\n                filepath=mutations_path,\n                parentid=public_release_preview,\n                version_comment=genie_version,\n                name=\"data_mutations_extended.txt\",\n            )\n        elif \"CNA\" in entName:\n            cna = syn.get(entId, followLink=True)\n            cnaDf = pd.read_csv(cna.path, sep=\"\\t\")\n            cna_columns = pd.concat([publicReleaseSamples, pd.Series(\"Hugo_Symbol\")])\n            # parse out the CNA columns to keep\n            cnaDf = cnaDf[cnaDf.columns[cnaDf.columns.isin(cna_columns)]]\n            text = process_functions.removeFloat(cnaDf)\n            text = (\n                text.replace(\"\\t\\t\", \"\\tNA\\t\")\n                .replace(\"\\t\\t\", \"\\tNA\\t\")\n                .replace(\"\\t\\n\", \"\\tNA\\n\")\n            )\n            with open(cna_path, \"w\") as cnaFile:\n                cnaFile.write(text)\n            load.store_file(\n                syn=syn,\n                filepath=cna_path,\n                parentid=public_release_preview,\n                version_comment=genie_version,\n                name=\"data_CNA.txt\",\n            )\n        elif entName.endswith(\".seg\"):\n            seg = syn.get(entId, followLink=True)\n            segDf = pd.read_csv(seg.path, sep=\"\\t\")\n            segDf = segDf[segDf[\"ID\"].isin(publicReleaseSamples)]\n            text = process_functions.removeFloat(segDf)\n            with open(seg_path, \"w\") as segFile:\n                segFile.write(text)\n            load.store_file(\n                syn=syn,\n                filepath=seg_path,\n                parentid=public_release_preview,\n                version_comment=genie_version,\n                name=\"data_cna_hg19.seg\",\n            )\n        elif entName == \"genomic_information.txt\":\n            bed = syn.get(entId, followLink=True)\n            bedDf = pd.read_csv(bed.path, sep=\"\\t\")\n            bedDf = bedDf[bedDf.SEQ_ASSAY_ID.isin(allClin.SEQ_ASSAY_ID)]\n            bedDf.to_csv(combined_bed_path, sep=\"\\t\", index=False)\n            load.store_file(\n                syn=syn,\n                filepath=combined_bed_path,\n                parentid=public_release_preview,\n                version_comment=genie_version,\n                name=\"genomic_information.txt\",\n            )\n        else:\n            ent = syn.get(entId, followLink=True, downloadFile=False)\n            copiedId = _copyRecursive(\n                syn,\n                ent,\n                public_release_preview,\n                version=ent.versionNumber,\n                updateExisting=True,\n                setProvenance=None,\n                skipCopyAnnotations=True,\n            )\n            copiedEnt = syn.get(copiedId[ent.id], downloadFile=False)\n            # Set version comment\n            copiedEnt.versionComment = genie_version\n            copiedEnt = syn.store(copiedEnt, forceVersion=False)\n            # There was a time when gene panel files had to be renamed\n            # with an appended genie version. But... GEN-76\n            # No longer appending genie verison to release files\n            # So just need to track gene panel entities\n            if entName.startswith(\"data_gene_panel\"):\n                genePanelEntities.append(copiedEnt)\n\n    return caseListEntities, genePanelEntities\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/","title":"Database to Staging","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging","title":"<code>genie.database_to_staging</code>","text":"<p>Functions for releasing GENIE consortium releases</p>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.GENIE_RELEASE_DIR","title":"<code>GENIE_RELEASE_DIR = os.path.join(os.path.expanduser('~/.synapseCache'), 'GENIE_release')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.CASE_LIST_PATH","title":"<code>CASE_LIST_PATH = os.path.join(GENIE_RELEASE_DIR, 'case_lists')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.CNA_CENTER_PATH","title":"<code>CNA_CENTER_PATH = os.path.join(GENIE_RELEASE_DIR, 'data_CNA_%s.txt')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.SAMPLE_CENTER_PATH","title":"<code>SAMPLE_CENTER_PATH = os.path.join(GENIE_RELEASE_DIR, 'data_clinical_supp_sample_%s.txt')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.PATIENT_CENTER_PATH","title":"<code>PATIENT_CENTER_PATH = os.path.join(GENIE_RELEASE_DIR, 'data_clinical_supp_patient_%s.txt')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.MUTATIONS_CENTER_PATH","title":"<code>MUTATIONS_CENTER_PATH = os.path.join(GENIE_RELEASE_DIR, 'data_mutations_extended_%s.txt')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.FUSIONS_CENTER_PATH","title":"<code>FUSIONS_CENTER_PATH = os.path.join(GENIE_RELEASE_DIR, 'data_fusions_%s.txt')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.SEG_CENTER_PATH","title":"<code>SEG_CENTER_PATH = os.path.join(GENIE_RELEASE_DIR, 'data_cna_hg19_%s.seg')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.SV_CENTER_PATH","title":"<code>SV_CENTER_PATH = os.path.join(GENIE_RELEASE_DIR, 'data_sv_%s.txt')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.BED_DIFFS_SEQASSAY_PATH","title":"<code>BED_DIFFS_SEQASSAY_PATH = os.path.join(GENIE_RELEASE_DIR, 'diff_%s.csv')</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.FULL_MAF_RELEASE_COLUMNS","title":"<code>FULL_MAF_RELEASE_COLUMNS = ['Hugo_Symbol', 'Entrez_Gene_Id', 'Center', 'NCBI_Build', 'Chromosome', 'Start_Position', 'End_Position', 'Strand', 'Consequence', 'Variant_Classification', 'Variant_Type', 'Reference_Allele', 'Tumor_Seq_Allele1', 'Tumor_Seq_Allele2', 'dbSNP_RS', 'dbSNP_Val_Status', 'Tumor_Sample_Barcode', 'Matched_Norm_Sample_Barcode', 'Match_Norm_Seq_Allele1', 'Match_Norm_Seq_Allele2', 'Tumor_Validation_Allele1', 'Tumor_Validation_Allele2', 'Match_Norm_Validation_Allele1', 'Match_Norm_Validation_Allele2', 'Verification_Status', 'Validation_Status', 'Mutation_Status', 'Sequencing_Phase', 'Sequence_Source', 'Validation_Method', 'Score', 'BAM_File', 'Sequencer', 't_ref_count', 't_alt_count', 'n_ref_count', 'n_alt_count', 'HGVSc', 'HGVSp', 'HGVSp_Short', 'Transcript_ID', 'RefSeq', 'Protein_position', 'Codons', 'Exon_Number', 'gnomAD_AF', 'gnomAD_AFR_AF', 'gnomAD_AMR_AF', 'gnomAD_ASJ_AF', 'gnomAD_EAS_AF', 'gnomAD_FIN_AF', 'gnomAD_NFE_AF', 'gnomAD_OTH_AF', 'gnomAD_SAS_AF', 'FILTER', 'Polyphen_Prediction', 'Polyphen_Score', 'SIFT_Prediction', 'SIFT_Score', 'SWISSPROT', 'n_depth', 't_depth', 'Annotation_Status', 'mutationInCis_Flag']</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging._to_redact_interval","title":"<code>_to_redact_interval(df_col)</code>","text":"<p>Determines year values that are \"&lt;18\" and interval values &gt;89 that need to be redacted Returns bool because BIRTH_YEAR needs to be redacted as well based on the results</p> PARAMETER DESCRIPTION <code>df_col</code> <p>Dataframe column/pandas.Series of an interval column</p> <p> TYPE: <code>Series</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>pandas.Series: to redact boolean vector    pandas.Series: to redact pediatric boolean vector</p> <p> TYPE: <code>Tuple[Series, Series]</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def _to_redact_interval(df_col: pd.Series) -&gt; Tuple[pd.Series, pd.Series]:\n    \"\"\"\n    Determines year values that are \"&lt;18\" and interval values &gt;89 that need to be redacted\n    Returns bool because BIRTH_YEAR needs to be redacted as well based\n    on the results\n\n    Args:\n        df_col: Dataframe column/pandas.Series of an interval column\n\n    Returns:\n        tuple: pandas.Series: to redact boolean vector\n               pandas.Series: to redact pediatric boolean vector\n\n    \"\"\"\n    phi_cutoff = 365 * 89\n    # Some centers pre-redact their values by adding &lt; or &gt;. These\n    # must be redacted\n    contain_greaterthan = df_col.astype(str).str.contains(\"&gt;\", na=False)\n    contain_lessthan = df_col.astype(str).str.contains(\"&lt;\", na=False)\n    # Add in errors='coerce' to turn strings into NaN\n    col_int = pd.to_numeric(df_col, errors=\"coerce\")\n    to_redact = (col_int &gt; phi_cutoff) | contain_greaterthan\n    to_redact_pediatric = contain_lessthan\n    return to_redact, to_redact_pediatric\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging._redact_year","title":"<code>_redact_year(df_col)</code>","text":"<p>Redacts year values that have &lt; or &gt;</p> PARAMETER DESCRIPTION <code>df_col</code> <p>Dataframe column/pandas.Series of a year column</p> <p> TYPE: <code>Series</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pandas.Series: Redacted series</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def _redact_year(df_col: pd.Series) -&gt; pd.Series:\n    \"\"\"Redacts year values that have &lt; or &gt;\n\n    Args:\n        df_col: Dataframe column/pandas.Series of a year column\n\n    Returns:\n        pandas.Series: Redacted series\n\n    \"\"\"\n    year = df_col.astype(str)\n    contain_greaterthan = year.str.contains(\"&gt;\", na=False)\n    contain_lessthan = year.str.contains(\"&lt;\", na=False)\n    df_col[contain_greaterthan] = \"cannotReleaseHIPAA\"\n    df_col[contain_lessthan] = \"withheld\"\n    return df_col\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging._redact_ped_year","title":"<code>_redact_ped_year(df_col)</code>","text":"<p>Redacts year values that have &lt;</p> PARAMETER DESCRIPTION <code>df_col</code> <p>Dataframe column/pandas.Series of a year column</p> <p> TYPE: <code>Series</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pandas.Series: Redacted series</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def _redact_ped_year(df_col: pd.Series) -&gt; pd.Series:\n    \"\"\"Redacts year values that have &lt;\n\n    Args:\n        df_col: Dataframe column/pandas.Series of a year column\n\n    Returns:\n        pandas.Series: Redacted series\n\n    \"\"\"\n    year = df_col.astype(str)\n    contain_lessthan = year.str.contains(\"&lt;\", na=False)\n    df_col[contain_lessthan] = \"withheld\"\n    return df_col\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging._to_redact_difference","title":"<code>_to_redact_difference(df_col_year1, df_col_year2)</code>","text":"<p>Determine if difference between year2 and year1 is &gt; 89</p> PARAMETER DESCRIPTION <code>df_col_year1</code> <p>Dataframe column/pandas.Series of a year column</p> <p> TYPE: <code>Series</code> </p> <code>df_col_year2</code> <p>Dataframe column/pandas.Series of a year column</p> <p> TYPE: <code>Series</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pandas.Series: to redact boolean vector</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def _to_redact_difference(\n    df_col_year1: pd.Series, df_col_year2: pd.Series\n) -&gt; pd.Series:\n    \"\"\"Determine if difference between year2 and year1 is &gt; 89\n\n    Args:\n        df_col_year1: Dataframe column/pandas.Series of a year column\n        df_col_year2: Dataframe column/pandas.Series of a year column\n\n    Returns:\n        pandas.Series: to redact boolean vector\n\n    \"\"\"\n    # Add in errors='coerce' to turn strings into NaN\n    year1 = pd.to_numeric(df_col_year1, errors=\"coerce\")\n    year2 = pd.to_numeric(df_col_year2, errors=\"coerce\")\n    to_redact = year2 - year1 &gt; 89\n    return to_redact\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.redact_phi","title":"<code>redact_phi(clinicaldf, interval_cols_to_redact=['AGE_AT_SEQ_REPORT', 'INT_CONTACT', 'INT_DOD'])</code>","text":"<p>Redacts the PHI by re-annotating the clinical file</p> PARAMETER DESCRIPTION <code>clinicaldf</code> <p>merged clinical dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>interval_cols_to_redact</code> <p>List of interval columns to redact.                      Defaults to ['AGE_AT_SEQ_REPORT',                                   'INT_CONTACT',                                   'INT_DOD']</p> <p> TYPE: <code>list</code> DEFAULT: <code>['AGE_AT_SEQ_REPORT', 'INT_CONTACT', 'INT_DOD']</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pandas.DataFrame: Redacted clinical dataframe</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def redact_phi(\n    clinicaldf: pd.DataFrame,\n    interval_cols_to_redact: list = [\"AGE_AT_SEQ_REPORT\", \"INT_CONTACT\", \"INT_DOD\"],\n) -&gt; pd.DataFrame:\n    \"\"\"Redacts the PHI by re-annotating the clinical file\n\n    Args:\n        clinicaldf: merged clinical dataframe\n        interval_cols_to_redact: List of interval columns to redact.\n                                 Defaults to ['AGE_AT_SEQ_REPORT',\n                                              'INT_CONTACT',\n                                              'INT_DOD']\n\n    Returns:\n        pandas.DataFrame: Redacted clinical dataframe\n\n    \"\"\"\n    # Moved to cannotReleaseHIPAA and withheld because the HIPAA\n    # years would change every single year. (e.g. &lt;1926, &gt;1998 would be\n    # inaccurate every year)\n    for col in interval_cols_to_redact:\n        to_redact, to_redactpeds = _to_redact_interval(clinicaldf[col])\n        clinicaldf.loc[to_redact, \"BIRTH_YEAR\"] = \"cannotReleaseHIPAA\"\n        clinicaldf.loc[to_redact, col] = \"&gt;32485\"\n        clinicaldf.loc[to_redactpeds, col] = \"withheld\"\n    # Redact BIRTH_YEAR values that have &lt; or &gt;\n    # Birth year has to be done separately because it is not an interval\n    clinicaldf[\"BIRTH_YEAR\"] = _redact_year(clinicaldf[\"BIRTH_YEAR\"])\n    to_redact = _to_redact_difference(\n        clinicaldf[\"BIRTH_YEAR\"], clinicaldf[\"YEAR_CONTACT\"]\n    )\n    clinicaldf.loc[to_redact, \"BIRTH_YEAR\"] = \"cannotReleaseHIPAA\"\n    to_redact = _to_redact_difference(\n        clinicaldf[\"BIRTH_YEAR\"], clinicaldf[\"YEAR_DEATH\"]\n    )\n    clinicaldf.loc[to_redact, \"BIRTH_YEAR\"] = \"cannotReleaseHIPAA\"\n\n    # redact range year for pediatric data\n    clinicaldf[\"YEAR_CONTACT\"] = _redact_ped_year(clinicaldf[\"YEAR_CONTACT\"])\n    clinicaldf[\"YEAR_DEATH\"] = _redact_ped_year(clinicaldf[\"YEAR_DEATH\"])\n\n    return clinicaldf\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.remove_maf_samples","title":"<code>remove_maf_samples(mafdf, keep_samples)</code>","text":"<p>Remove samples from maf file</p> PARAMETER DESCRIPTION <code>mafdf</code> <p>Maf dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>keep_samples</code> <p>Samples to keep</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Filtered maf dataframe</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def remove_maf_samples(mafdf: pd.DataFrame, keep_samples: list) -&gt; pd.DataFrame:\n    \"\"\"Remove samples from maf file\n\n    Args:\n        mafdf: Maf dataframe\n        keep_samples: Samples to keep\n\n    Returns:\n        Filtered maf dataframe\n\n    \"\"\"\n    keep_maf = mafdf[\"Tumor_Sample_Barcode\"].isin(keep_samples)\n    mafdf = mafdf.loc[keep_maf,]\n    return mafdf\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.get_whitelist_variants_idx","title":"<code>get_whitelist_variants_idx(mafdf)</code>","text":"<p>Get boolean vector for variants that are known somatic sites This is to override the germline filter</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def get_whitelist_variants_idx(mafdf):\n    \"\"\"Get boolean vector for variants that are known somatic sites\n    This is to override the germline filter\n    \"\"\"\n    columns = [\"Chromosome\", \"Start\", \"End\", \"Symbol\"]\n    whitelist = pd.read_csv(\n        \"https://raw.githubusercontent.com/mskcc/vcf2maf/v1.6.19/data/known_somatic_sites.bed\",\n        sep=\"\\t\",\n        comment=\"#\",\n        header=None,\n        names=columns,\n    )\n    rangesdf = mafdf[\n        [\"Chromosome\", \"Start_Position\", \"End_Position\", \"Hugo_Symbol\", \"HGVSp_Short\"]\n    ]\n    rangesdf = rangesdf.rename(\n        columns={\"Start_Position\": \"Start\", \"End_Position\": \"End\"}\n    )\n    maf_ranges = pyranges.PyRanges(rangesdf)\n    whitelist_ranges = pyranges.PyRanges(whitelist)\n    whitelisted_variants = maf_ranges.intersect(whitelist_ranges, how=\"containment\")\n    whitelist_variantsdf = whitelisted_variants.as_df()\n    if not whitelist_variantsdf.empty:\n        variants = (\n            whitelist_variantsdf[\"Hugo_Symbol\"]\n            + \" \"\n            + whitelist_variantsdf[\"HGVSp_Short\"]\n        )\n    else:\n        variants = []\n    maf_variants = mafdf[\"Hugo_Symbol\"] + \" \" + mafdf[\"HGVSp_Short\"]\n    # For some reason intersect and overlap doesn't work when\n    # Start and End are the same. Here is an example that won't be\n    # matched by the intersect function\n    # variant: chr9-10-10\n    # Bed: chr9-9-10\n    match_start_end = mafdf[\"Start_Position\"].isin(whitelist[\"Start\"]) | mafdf[\n        \"End_Position\"\n    ].isin(whitelist[\"End\"])\n    return maf_variants.isin(variants) | match_start_end\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.configure_maf","title":"<code>configure_maf(mafdf, remove_variants, flagged_variants)</code>","text":"<p>Configures each maf dataframe, does germline filtering</p> <p>Germline filtering for MAF files uses the gnomAD columns that refer to the allele frequencies (AF) of variants in different population groups from the gnomAD (Genome Aggregation Database). This filter will filter out variants with a maximum AF &gt; 0.05% across all populations which are typically common germline variants.</p> <p>Germline filtering for MAF files occurs during release instead of during processing because the MAF file gets re-annotated during processing via genome nexus annotation.</p> PARAMETER DESCRIPTION <code>mafdf</code> <p>Maf dataframe</p> <p> </p> <code>remove_variants</code> <p>Variants to remove</p> <p> </p> <code>flagged_variants</code> <p>Variants to flag</p> <p> </p> RETURNS DESCRIPTION <p>configured maf row</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def configure_maf(mafdf, remove_variants, flagged_variants):\n    \"\"\"Configures each maf dataframe, does germline filtering\n\n    Germline filtering for MAF files uses the gnomAD columns that refer to the\n    allele frequencies (AF) of variants in different population groups\n    from the gnomAD (Genome Aggregation Database). This filter will filter out\n    variants with a maximum AF &gt; 0.05% across all populations which are typically\n    common germline variants.\n\n    Germline filtering for MAF files occurs during release instead of during processing\n    because the MAF file gets re-annotated during processing via genome nexus annotation.\n\n    Args:\n        mafdf: Maf dataframe\n        remove_variants: Variants to remove\n        flagged_variants: Variants to flag\n\n    Returns:\n        configured maf row\n    \"\"\"\n    variant = mafdf[\n        [\n            \"Chromosome\",\n            \"Start_Position\",\n            \"End_Position\",\n            \"Reference_Allele\",\n            \"Tumor_Seq_Allele2\",\n            \"Tumor_Sample_Barcode\",\n        ]\n    ].apply(lambda x: \" \".join(x.map(str)), axis=1)\n    mergecheck_variant = mafdf[\n        [\n            \"Chromosome\",\n            \"Start_Position\",\n            \"HGVSp_Short\",\n            \"Reference_Allele\",\n            \"Tumor_Seq_Allele2\",\n            \"Tumor_Sample_Barcode\",\n        ]\n    ].apply(lambda x: \" \".join(x.map(str)), axis=1)\n\n    # Flag mutation in cis variants\n    mafdf[\"mutationInCis_Flag\"] = mergecheck_variant.isin(flagged_variants)\n    # Remove common variants\n    # na=False to resolve this linked error\n    # https://stackoverflow.com/questions/52297740\n    # common_variants = mafdf['FILTER'].astype(str).str.contains(\n    #     \"common_variant\", na=False\n    # )\n    # Germline Filter\n    gnomad_cols = [\n        \"gnomAD_AFR_AF\",\n        \"gnomAD_AMR_AF\",\n        \"gnomAD_ASJ_AF\",\n        \"gnomAD_EAS_AF\",\n        \"gnomAD_FIN_AF\",\n        \"gnomAD_NFE_AF\",\n        \"gnomAD_OTH_AF\",\n        \"gnomAD_SAS_AF\",\n    ]\n    # location of germline variants\n    common_variants_idx = mafdf.loc[:, gnomad_cols].max(axis=1, skipna=True) &gt; 0.0005\n\n    # Remove specific variants\n    to_remove_variants = variant.isin(remove_variants)\n    # Genome Nexus successfully annotated (vcf2maf does not have this column)\n    if mafdf.get(\"Annotation_Status\") is None:\n        mafdf[\"Annotation_Status\"] = \"SUCCESS\"\n    # Make sure to only get variants that were successfully annotated\n    success = mafdf[\"Annotation_Status\"] == \"SUCCESS\"\n    # Get whitelisted variants\n    whitelist_variants_idx = get_whitelist_variants_idx(mafdf)\n    mafdf = mafdf.loc[\n        (\n            (~common_variants_idx | whitelist_variants_idx)\n            &amp; ~to_remove_variants\n            &amp; success\n        ),\n    ]\n    # May not need to do this because these columns are always\n    # returned as numerical values now\n    # fillnas = ['t_depth', 't_ref_count', 't_alt_count',\n    #            'n_depth', 'n_ref_count', 'n_alt_count']\n    # for col in fillnas:\n    #     mafdf[col][mafdf[col].astype(str) == \".\"] = float('nan')\n    n_depth_ind = mafdf[\"n_depth\"].astype(str).isin([\"NA\", \"0.0\", \"0\", \"nan\"])\n    mafdf.loc[n_depth_ind, \"Match_Norm_Seq_Allele2\"] = \"\"\n    mafdf.loc[n_depth_ind, \"Match_Norm_Seq_Allele1\"] = \"\"\n    # Calculate missing t_depth, t_ref_count, t_alt_count\n    t_counts = calculate_missing_variant_counts(\n        depth=mafdf[\"t_depth\"],\n        alt_count=mafdf[\"t_alt_count\"],\n        ref_count=mafdf[\"t_ref_count\"],\n    )\n    mafdf[\"t_depth\"] = t_counts[\"depth\"]\n    mafdf[\"t_ref_count\"] = t_counts[\"ref_count\"]\n    mafdf[\"t_alt_count\"] = t_counts[\"alt_count\"]\n    # Calculate missing n_depth, n_ref_count, n_alt_count\n    n_counts = calculate_missing_variant_counts(\n        depth=mafdf[\"n_depth\"],\n        alt_count=mafdf[\"n_alt_count\"],\n        ref_count=mafdf[\"n_ref_count\"],\n    )\n    mafdf[\"n_depth\"] = n_counts[\"depth\"]\n    mafdf[\"n_ref_count\"] = n_counts[\"ref_count\"]\n    mafdf[\"n_alt_count\"] = n_counts[\"alt_count\"]\n\n    return mafdf\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.calculate_missing_variant_counts","title":"<code>calculate_missing_variant_counts(depth, alt_count, ref_count)</code>","text":"<p>Calculate missing counts. t_depth = t_alt_count + t_ref_count</p> PARAMETER DESCRIPTION <code>depth</code> <p>Allele Depth</p> <p> TYPE: <code>Series</code> </p> <code>alt_count</code> <p>Allele alt counts</p> <p> TYPE: <code>Series</code> </p> <code>ref_count</code> <p>Allele ref counts</p> <p> TYPE: <code>Series</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>filled in depth, alt_count and ref_count values</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def calculate_missing_variant_counts(\n    depth: pd.Series, alt_count: pd.Series, ref_count: pd.Series\n) -&gt; dict:\n    \"\"\"Calculate missing counts. t_depth = t_alt_count + t_ref_count\n\n    Args:\n        depth: Allele Depth\n        alt_count: Allele alt counts\n        ref_count: Allele ref counts\n\n    Returns:\n        filled in depth, alt_count and ref_count values\n\n    \"\"\"\n    # Avoid SettingWithCopyWarning\n    depth = depth.copy()\n    alt_count = alt_count.copy()\n    ref_count = ref_count.copy()\n    # t_depth = t_ref_count + t_alt_count\n    null_depth = depth.isnull()\n    # The notation null_depth_ref means all the reference values for which\n    # depth is NA\n    null_depth_ref = ref_count[null_depth]\n    null_depth_alt = alt_count[null_depth]\n    depth.loc[null_depth] = null_depth_ref + null_depth_alt\n    # t_ref_count = t_depth - t_alt_count\n    null_ref = ref_count.isnull()\n    null_ref_depth = depth[null_ref]\n    null_ref_alt = alt_count[null_ref]\n    ref_count[null_ref] = null_ref_depth - null_ref_alt\n    # t_alt_count = t_depth - t_ref_count\n    null_alt = alt_count.isnull()\n    null_alt_depth = depth[null_alt]\n    null_alt_ref = ref_count[null_alt]\n    alt_count[null_alt] = null_alt_depth - null_alt_ref\n    return {\"depth\": depth, \"ref_count\": ref_count, \"alt_count\": alt_count}\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.runMAFinBED","title":"<code>runMAFinBED(syn, center_mappingdf, test=False, staging=False, genieVersion='test')</code>","text":"<p>Run MAF in BED script, filter data and update MAFinBED files</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse client connection</p> <p> TYPE: <code>Synapse</code> </p> <code>center_mappingdf</code> <p>center mapping dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>test</code> <p>Testing parameter. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>staging</code> <p>Staging parameter. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>genieVersion</code> <p>GENIE version. Defaults to \"test\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'test'</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pd.Series: Variants to remove</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def runMAFinBED(\n    syn: synapseclient.Synapse,\n    center_mappingdf: pd.DataFrame,\n    test: bool = False,\n    staging: bool = False,\n    genieVersion: str = \"test\",\n) -&gt; pd.Series:\n    \"\"\"\n    Run MAF in BED script, filter data and update MAFinBED files\n\n    Args:\n        syn (synapseclient.Synapse): Synapse client connection\n        center_mappingdf (pd.DataFrame): center mapping dataframe\n        test (bool, optional):Testing parameter. Defaults to False.\n        staging (bool, optional): Staging parameter. Defaults to False.\n        genieVersion (str, optional): GENIE version. Defaults to \"test\".\n\n    Returns:\n        pd.Series: Variants to remove\n    \"\"\"\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    # TODO: Use tempfile\n    notinbed_file = os.path.join(script_dir, \"../R/notinbed.csv\")\n    command = get_run_maf_in_bed_script_cmd(\n        notinbed_file=notinbed_file, script_dir=script_dir, test=test, staging=staging\n    )\n    subprocess.check_call(command)\n\n    # mutationSynId = databaseSynIdMappingDf['Id'][\n    #     databaseSynIdMappingDf['Database'] == \"vcf2maf\"][0]\n    # removedVariants = syn.tableQuery(\n    #     \"select Chromosome, Start_Position, End_Position, Reference_Allele, \"\n    #     \"Tumor_Seq_Allele2, Tumor_Sample_Barcode, Center from {} where inBED\"\n    #     \" is False and Center in ('{}')\".format(\n    #         mutationSynId, \"','\".join(center_mappingdf.center)))\n    # removedVariantsDf = removedVariants.asDataFrame()\n    removed_variantsdf = store_maf_in_bed_filtered_variants(\n        syn=syn,\n        notinbed_file=notinbed_file,\n        center_mapping_df=center_mappingdf,\n        genie_version=genieVersion,\n    )\n    return removed_variantsdf[\"removeVariants\"]\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.get_run_maf_in_bed_script_cmd","title":"<code>get_run_maf_in_bed_script_cmd(notinbed_file, script_dir, test, staging)</code>","text":"<p>This function gets the MAFinBED R script command call based on    whether we are running in test, staging or production mode</p> PARAMETER DESCRIPTION <code>notinbed_file</code> <p>Full path to the notinbed csv</p> <p> TYPE: <code>str</code> </p> <code>script_dir</code> <p>directory of the MAFinBED script</p> <p> TYPE: <code>str</code> </p> <code>test</code> <p>Testing parameter.</p> <p> TYPE: <code>bool</code> </p> <code>staging</code> <p>Staging parameter</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Full command call for the MAFinBED script</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def get_run_maf_in_bed_script_cmd(\n    notinbed_file: str, script_dir: str, test: bool, staging: bool\n) -&gt; str:\n    \"\"\"This function gets the MAFinBED R script command call based on\n       whether we are running in test, staging or production mode\n\n    Args:\n        notinbed_file (str): Full path to the notinbed csv\n        script_dir (str): directory of the MAFinBED script\n        test (bool): Testing parameter.\n        staging (bool): Staging parameter\n\n    Returns:\n        str: Full command call for the MAFinBED script\n    \"\"\"\n    mafinbed_script = os.path.join(script_dir, \"../R/MAFinBED.R\")\n    # The MAFinBED script filters out the centers that aren't being processed\n    command = [\"Rscript\", mafinbed_script, notinbed_file]\n    if test:\n        command.append(\"--testing\")\n    elif staging:\n        command.append(\"--staging\")\n    else:\n        pass\n    return command\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_maf_in_bed_filtered_variants","title":"<code>store_maf_in_bed_filtered_variants(syn, notinbed_file, center_mapping_df, genie_version)</code>","text":"<p>This script retrieves and stores the filtered variants generated     from running the MAFinBed script as files on Synapse. This script     then returns the not in bed file with the removed variants column     added.     TODO: Add handling for empty file?</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse client connection</p> <p> TYPE: <code>Synapse</code> </p> <code>notinbed_file</code> <p>input file</p> <p> TYPE: <code>str</code> </p> <code>center_mapping_df</code> <p>the center mapping dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>genie_version</code> <p>version of this genie run</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: filtered variants dataframe with removed variants column added</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_maf_in_bed_filtered_variants(\n    syn: synapseclient.Synapse,\n    notinbed_file: str,\n    center_mapping_df: pd.DataFrame,\n    genie_version: str,\n) -&gt; pd.DataFrame:\n    \"\"\"This script retrieves and stores the filtered variants generated\n        from running the MAFinBed script as files on Synapse. This script\n        then returns the not in bed file with the removed variants column\n        added.\n        TODO: Add handling for empty file?\n\n    Args:\n        syn (synapseclient.Synapse): Synapse client connection\n        notinbed_file (str): input file\n        center_mapping_df (pd.DataFrame): the center mapping dataframe\n        genie_version (str): version of this genie run\n\n    Returns:\n        pd.DataFrame: filtered variants dataframe with\n            removed variants column added\n    \"\"\"\n    removed_variantsdf = pd.read_csv(notinbed_file)\n    removed_variantsdf[\"removeVariants\"] = (\n        removed_variantsdf[\"Chromosome\"].astype(str)\n        + \" \"\n        + removed_variantsdf[\"Start_Position\"].astype(str)\n        + \" \"\n        + removed_variantsdf[\"End_Position\"].astype(str)\n        + \" \"\n        + removed_variantsdf[\"Reference_Allele\"].astype(str)\n        + \" \"\n        + removed_variantsdf[\"Tumor_Seq_Allele2\"].astype(str)\n        + \" \"\n        + removed_variantsdf[\"Tumor_Sample_Barcode\"].astype(str)\n    )\n    # Store filtered variants\n    for center in removed_variantsdf[\"Center\"].unique():\n        center_mutation = removed_variantsdf[removed_variantsdf[\"Center\"] == center]\n        # mafText = process.removePandasDfFloat(center_mutation)\n        center_mutation.to_csv(\"mafinbed_filtered_variants.csv\", index=False)\n        load.store_file(\n            syn=syn,\n            filepath=\"mafinbed_filtered_variants.csv\",\n            parentid=center_mapping_df[\"stagingSynId\"][\n                center_mapping_df[\"center\"] == center\n            ].values[0],\n            version_comment=genie_version,\n        )\n        os.unlink(\"mafinbed_filtered_variants.csv\")\n    return removed_variantsdf\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.seq_date_filter","title":"<code>seq_date_filter(clinicalDf, processingDate, consortiumReleaseCutOff)</code>","text":"<p>Filter samples by seq date</p> PARAMETER DESCRIPTION <code>clinicalDf</code> <p>Clinical dataframe</p> <p> </p> <code>processingDate</code> <p>Processing date in form of Apr-XXXX</p> <p> </p> <code>consortiumReleaseCutOff</code> <p>Release cut off days</p> <p> </p> RETURNS DESCRIPTION <code>list</code> <p>Samples to remove</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def seq_date_filter(clinicalDf, processingDate, consortiumReleaseCutOff):\n    \"\"\"\n    Filter samples by seq date\n\n    Args:\n        clinicalDf: Clinical dataframe\n        processingDate: Processing date in form of Apr-XXXX\n        consortiumReleaseCutOff: Release cut off days\n\n    Returns:\n        list: Samples to remove\n    \"\"\"\n    removeSeqDateSamples = process_functions.seqDateFilter(\n        clinicalDf, processingDate, consortiumReleaseCutOff\n    )\n    return removeSeqDateSamples\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.mutation_in_cis_filter","title":"<code>mutation_in_cis_filter(syn, skipMutationsInCis, variant_filtering_synId, center_mappingdf, genieVersion, test=False, staging=False)</code>","text":"<p>Run mutation in cis filter, look up samples to remove.</p> <p>The mutation in cis script ONLY runs WHEN the skipMutationsInCis parameter is FALSE</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>skipMutationsInCis</code> <p>Skip this filter</p> <p> </p> <code>variant_filtering_synId</code> <p>mergeCheck database dataframe</p> <p> </p> <code>center_mappingdf</code> <p>center mapping dataframe</p> <p> </p> <code>genieVersion</code> <p>GENIE version. Default is test.</p> <p> </p> <code>test</code> <p>Testing parameter. Default is False.</p> <p> DEFAULT: <code>False</code> </p> <code>staging</code> <p>Staging parameter. Default is False.</p> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p>pd.Series: Samples to remove</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def mutation_in_cis_filter(\n    syn,\n    skipMutationsInCis,\n    variant_filtering_synId,\n    center_mappingdf,\n    genieVersion,\n    test=False,\n    staging=False,\n):\n    \"\"\"\n    Run mutation in cis filter, look up samples to remove.\n\n    The mutation in cis script ONLY runs\n    WHEN the skipMutationsInCis parameter is FALSE\n\n    Args:\n        syn: Synapse object\n        skipMutationsInCis: Skip this filter\n        variant_filtering_synId: mergeCheck database dataframe\n        center_mappingdf: center mapping dataframe\n        genieVersion: GENIE version. Default is test.\n        test: Testing parameter. Default is False.\n        staging: Staging parameter. Default is False.\n\n    Returns:\n        pd.Series: Samples to remove\n    \"\"\"\n    if not skipMutationsInCis:\n        command = get_mutation_in_cis_filter_script_cmd(test=test, staging=staging)\n        # TODO: use subprocess.run instead\n        subprocess.check_call(command)\n        store_mutation_in_cis_files_to_staging(\n            syn=syn,\n            center_mappingdf=center_mappingdf,\n            variant_filtering_synId=variant_filtering_synId,\n            genieVersion=genieVersion,\n        )\n    remove_samples = get_mutation_in_cis_filtered_samples(\n        syn=syn, variant_filtering_synId=variant_filtering_synId\n    )\n    flagged_variants = get_mutation_in_cis_flagged_variants(\n        syn=syn, variant_filtering_synId=variant_filtering_synId\n    )\n    return (remove_samples, flagged_variants)\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.get_mutation_in_cis_filter_script_cmd","title":"<code>get_mutation_in_cis_filter_script_cmd(test, staging)</code>","text":"<p>This function gets the mutation_in_cis_filter R script     command call based on whether we are running in test, staging     or production mode</p> PARAMETER DESCRIPTION <code>test</code> <p>Testing parameter.</p> <p> TYPE: <code>bool</code> </p> <code>staging</code> <p>Staging parameter. Default is False.</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Full command call for the mergeCheck script</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def get_mutation_in_cis_filter_script_cmd(test: bool, staging: bool) -&gt; str:\n    \"\"\"This function gets the mutation_in_cis_filter R script\n        command call based on whether we are running in test, staging\n        or production mode\n\n    Args:\n        test (bool): Testing parameter.\n        staging (bool): Staging parameter. Default is False.\n\n    Returns:\n        str: Full command call for the mergeCheck script\n    \"\"\"\n    mergeCheck_script = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"../R/mergeCheck.R\"\n    )\n    command = [\"Rscript\", mergeCheck_script]\n    if test and not staging:\n        command.append(\"--testing\")\n    if staging and not test:\n        command.append(\"--staging\")\n    elif test and staging:\n        raise ValueError(\n            \"Mutation in cis only available in staging or testing mode not both\"\n        )\n    else:\n        pass\n    return command\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.get_mutation_in_cis_filtered_samples","title":"<code>get_mutation_in_cis_filtered_samples(syn, variant_filtering_synId)</code>","text":"<p>Gets the samples to remove in our variant filtering table    TODO: Add handling for when we have 0 row query results Args:     syn (synapseclient.Synapse): synapse client connection     variant_filtering_synId (str): variant filtering synapse id</p> RETURNS DESCRIPTION <code>list</code> <p>pd.Series: removed samples</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def get_mutation_in_cis_filtered_samples(\n    syn: synapseclient.Synapse, variant_filtering_synId: str\n) -&gt; list:\n    \"\"\"Gets the samples to remove in our variant filtering table\n       TODO: Add handling for when we have 0 row query results\n    Args:\n        syn (synapseclient.Synapse): synapse client connection\n        variant_filtering_synId (str): variant filtering synapse id\n\n    Returns:\n        pd.Series: removed samples\n    \"\"\"\n    query_str = (\n        f\"SELECT Tumor_Sample_Barcode FROM {variant_filtering_synId} where \"\n        \"Flag = 'TOSS' and Tumor_Sample_Barcode is not null\"\n    )\n    filtered_samplesdf = extract.get_syntabledf(syn=syn, query_string=query_str)\n    # #Alex script #1 removed patients\n    remove_samples = filtered_samplesdf[\"Tumor_Sample_Barcode\"].drop_duplicates()\n    return remove_samples\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.get_mutation_in_cis_flagged_variants","title":"<code>get_mutation_in_cis_flagged_variants(syn, variant_filtering_synId)</code>","text":"<p>Gets the flagged variants in our variant filtering table which is    a unique string concatenation of the Chromosome, Start_Position,    HGVSp_Short, Reference_Allele, Tumor_Seq_Allele2 and Tumor_Sample_Barcode    columns    TODO: Add handling for when we have 0 row query results</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse client connection</p> <p> TYPE: <code>Synapse</code> </p> <code>variant_filtering_synId</code> <p>variant filtering synapse id</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pd.Series: flagged variants</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def get_mutation_in_cis_flagged_variants(\n    syn: synapseclient.Synapse, variant_filtering_synId: str\n) -&gt; pd.Series:\n    \"\"\"Gets the flagged variants in our variant filtering table which is\n       a unique string concatenation of the Chromosome, Start_Position,\n       HGVSp_Short, Reference_Allele, Tumor_Seq_Allele2 and Tumor_Sample_Barcode\n       columns\n       TODO: Add handling for when we have 0 row query results\n\n    Args:\n        syn (synapseclient.Synapse): synapse client connection\n        variant_filtering_synId (str): variant filtering synapse id\n\n    Returns:\n        pd.Series: flagged variants\n    \"\"\"\n    query_str = (\n        f\"SELECT * FROM {variant_filtering_synId} where \"\n        \"Flag = 'Flag' and Tumor_Sample_Barcode is not null\"\n    )\n    flag_variantsdf = extract.get_syntabledf(syn=syn, query_string=query_str)\n\n    flag_variantsdf[\"flaggedVariants\"] = (\n        flag_variantsdf[\"Chromosome\"].astype(str)\n        + \" \"\n        + flag_variantsdf[\"Start_Position\"].astype(str)\n        + \" \"\n        + flag_variantsdf[\"HGVSp_Short\"].astype(str)\n        + \" \"\n        + flag_variantsdf[\"Reference_Allele\"].astype(str)\n        + \" \"\n        + flag_variantsdf[\"Tumor_Seq_Allele2\"].astype(str)\n        + \" \"\n        + flag_variantsdf[\"Tumor_Sample_Barcode\"].astype(str)\n    )\n    return flag_variantsdf[\"flaggedVariants\"]\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_mutation_in_cis_files_to_staging","title":"<code>store_mutation_in_cis_files_to_staging(syn, center_mappingdf, variant_filtering_synId, genieVersion)</code>","text":"<p>Stores the mutation in cis files to synapse per     center</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse client connection</p> <p> TYPE: <code>Synapse</code> </p> <code>center_mappingdf</code> <p>center mapping dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>variant_filtering_synId</code> <p>variant filtering synapse id</p> <p> TYPE: <code>str</code> </p> <code>genieVersion</code> <p>version of the genie pipeline run</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_mutation_in_cis_files_to_staging(\n    syn: synapseclient.Synapse,\n    center_mappingdf: pd.DataFrame,\n    variant_filtering_synId: str,\n    genieVersion: str,\n) -&gt; None:\n    \"\"\"Stores the mutation in cis files to synapse per\n        center\n\n    Args:\n        syn (synapseclient.Synapse): synapse client connection\n        center_mappingdf (pd.DataFrame): center mapping dataframe\n        variant_filtering_synId (str): variant filtering synapse id\n        genieVersion (str): version of the genie pipeline run\n    \"\"\"\n    # Store each centers mutations in cis to their staging folder\n    center_str = \"','\".join(center_mappingdf.center)\n    query_str = (\n        f\"select * from {variant_filtering_synId} where Center in ('{center_str}')\"\n    )\n    mergeCheckDf = extract.get_syntabledf(syn=syn, query_string=query_str)\n    for center in mergeCheckDf.Center.unique():\n        if not pd.isnull(center):\n            stagingSynId = center_mappingdf.stagingSynId[\n                center_mappingdf[\"center\"] == center\n            ]\n            mergeCheckDf[mergeCheckDf[\"Center\"] == center].to_csv(\n                \"mutationsInCis_filtered_samples.csv\", index=False\n            )\n            load.store_file(\n                syn=syn,\n                filepath=\"mutationsInCis_filtered_samples.csv\",\n                parentid=stagingSynId.values[0],\n                version_comment=genieVersion,\n            )\n            os.unlink(\"mutationsInCis_filtered_samples.csv\")\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.seq_assay_id_filter","title":"<code>seq_assay_id_filter(clinicaldf)</code>","text":"<p>(Deprecated) Remove samples that are part of SEQ_ASSAY_IDs with less than 50 samples</p> PARAMETER DESCRIPTION <code>clinicalDf</code> <p>Sample clinical dataframe</p> <p> </p> RETURNS DESCRIPTION <p>pd.Series: samples to remove</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def seq_assay_id_filter(clinicaldf):\n    \"\"\"\n    (Deprecated)\n    Remove samples that are part of SEQ_ASSAY_IDs with less\n    than 50 samples\n\n    Args:\n        clinicalDf: Sample clinical dataframe\n\n    Returns:\n        pd.Series: samples to remove\n    \"\"\"\n    remove_seqassayid = clinicaldf[\"SEQ_ASSAY_ID\"].value_counts()[\n        clinicaldf[\"SEQ_ASSAY_ID\"].value_counts() &lt; 50\n    ]\n    clinicaldf = clinicaldf[\n        clinicaldf[\"SEQ_ASSAY_ID\"].isin(remove_seqassayid.keys().values)\n    ]\n    return clinicaldf[\"SAMPLE_ID\"]\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.no_genepanel_filter","title":"<code>no_genepanel_filter(clinicaldf, beddf)</code>","text":"<p>Remove samples that don't have bed files associated with them</p> PARAMETER DESCRIPTION <code>clinicaldf</code> <p>Clinical data</p> <p> </p> <code>beddf</code> <p>bed data</p> <p> </p> RETURNS DESCRIPTION <p>pd.Series: samples to remove</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def no_genepanel_filter(clinicaldf, beddf):\n    \"\"\"\n    Remove samples that don't have bed files associated with\n    them\n\n    Args:\n        clinicaldf:  Clinical data\n        beddf: bed data\n\n    Returns:\n        pd.Series: samples to remove\n    \"\"\"\n\n    logger.info(\"NO GENE PANEL FILTER\")\n    has_seq_assay = clinicaldf[\"SEQ_ASSAY_ID\"].isin(beddf[\"SEQ_ASSAY_ID\"])\n    remove_samples = clinicaldf[\"SAMPLE_ID\"][~has_seq_assay]\n    logger.info(\n        \"Removing samples with no bed file: {}\".format(\",\".join(remove_samples))\n    )\n    return remove_samples\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_gene_panel_files","title":"<code>store_gene_panel_files(syn, fileviewSynId, genieVersion, data_gene_panel, consortiumReleaseSynId, wes_seqassayids)</code>","text":"Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_gene_panel_files(\n    syn,\n    fileviewSynId,\n    genieVersion,\n    data_gene_panel,\n    consortiumReleaseSynId,\n    wes_seqassayids,\n):\n    # Only need to upload these files once\n    logger.info(\"STORING GENE PANELS FILES\")\n    wes_genepanel_filenames = [\n        \"data_gene_panel_{}.txt\".format(seqassayid) for seqassayid in wes_seqassayids\n    ]\n    # Format string for tableQuery statement\n    wes_genepanel_str = \"','\".join(wes_genepanel_filenames)\n    # Only need to upload these files once\n    logger.info(\"STORING GENE PANELS FILES\")\n    # This line of code is required to make sure any new files are\n    # pulled into the file view.\n    syn.tableQuery(f\"select * from {fileviewSynId} limit 1\")\n    genePanelDf = extract.get_syntabledf(\n        syn,\n        f\"select id from {fileviewSynId} where \"\n        \"cBioFileFormat = 'genePanel' and \"\n        \"fileStage = 'staging' and \"\n        f\"name not in ('{wes_genepanel_str}')\",\n    )\n    genePanelEntities = []\n    panelNames = set(data_gene_panel[\"mutations\"])\n    print(f\"EXISTING GENE PANELS: {','.join(panelNames)}\")\n    for synId in genePanelDf[\"id\"]:\n        genePanel = syn.get(synId)\n        genePanelName = os.path.basename(genePanel.path)\n        newGenePanelPath = os.path.join(GENIE_RELEASE_DIR, genePanelName)\n        gene_panel = genePanelName.replace(\".txt\", \"\").replace(\"data_gene_panel_\", \"\")\n        print(gene_panel)\n        if gene_panel in panelNames:\n            os.rename(genePanel.path, newGenePanelPath)\n            annotations = {\"cBioFileFormat\": \"genePanel\"}\n            genePanelEntities.append(\n                load.store_file(\n                    syn=syn,\n                    filepath=newGenePanelPath,\n                    parentid=consortiumReleaseSynId,\n                    version_comment=genieVersion,\n                    name=genePanelName,\n                    annotations=annotations,\n                    used=f\"{synId}.{genePanel.versionNumber}\",\n                )\n            )\n    return genePanelEntities\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.filter_out_germline_variants","title":"<code>filter_out_germline_variants(input_data, status_col_str)</code>","text":"<p>Filters out germline variants given a status col str. Genie pipeline     cannot have any of these variants. NOTE: We have to search for the     status column because there's no column name validation in the release     steps so the status column may have different casing.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>input data with germline variants to filter out</p> <p> TYPE: <code>DataFrame</code> </p> <code>status_col_str</code> <p>search string for the status column for the data</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: filtered out germline variant data</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def filter_out_germline_variants(\n    input_data: pd.DataFrame, status_col_str: str\n) -&gt; pd.DataFrame:\n    \"\"\"Filters out germline variants given a status col str. Genie pipeline\n        cannot have any of these variants. NOTE: We have to search for the\n        status column because there's no column name validation in the release\n        steps so the status column may have different casing.\n\n    Args:\n        input_data (pd.DataFrame): input data with germline variants to filter out\n        status_col_str (str): search string for the status column for the data\n\n    Returns:\n        pd.DataFrame: filtered out germline variant data\n    \"\"\"\n    # find status col SV_Status\n    status_col = [\n        col for col in input_data.columns if col.lower() == status_col_str.lower()\n    ][0]\n    return input_data[input_data[status_col] != \"GERMLINE\"].reset_index(drop=True)\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_sv_files","title":"<code>store_sv_files(syn, release_synid, genie_version, synid, keep_for_center_consortium_samples, keep_for_merged_consortium_samples, current_release_staging, center_mappingdf)</code>","text":"<p>Create, filter, configure, and store structural variant file</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> TYPE: <code>str</code> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> TYPE: <code>str</code> </p> <code>synid</code> <p>SV database synid</p> <p> TYPE: <code>str</code> </p> <code>keep_for_center_consortium_samples</code> <p>Samples to keep for center files</p> <p> TYPE: <code>List[str]</code> </p> <code>keep_for_merged_consortium_samples</code> <p>Samples to keep for merged file</p> <p> TYPE: <code>List[str]</code> </p> <code>current_release_staging</code> <p>Staging flag</p> <p> TYPE: <code>bool</code> </p> <code>center_mappingdf</code> <p>Center mapping dataframe</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <p>List of SV Samples</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_sv_files(\n    syn: synapseclient.Synapse,\n    release_synid: str,\n    genie_version: str,\n    synid: str,\n    keep_for_center_consortium_samples: List[str],\n    keep_for_merged_consortium_samples: List[str],\n    current_release_staging: bool,\n    center_mappingdf: pd.DataFrame,\n):\n    \"\"\"\n    Create, filter, configure, and store structural variant file\n\n    Args:\n        syn: Synapse object\n        release_synid: Synapse id to store release file\n        genie_version: GENIE version (ie. v6.1-consortium)\n        synid: SV database synid\n        keep_for_center_consortium_samples: Samples to keep for center files\n        keep_for_merged_consortium_samples: Samples to keep for merged file\n        current_release_staging: Staging flag\n        center_mappingdf: Center mapping dataframe\n\n    Returns:\n        List of SV Samples\n    \"\"\"\n    logger.info(\"MERING, FILTERING, STORING SV FILES\")\n    sv_df = extract.get_syntabledf(\n        syn,\n        f\"select * from {synid}\",\n    )\n    version = syn.create_snapshot_version(synid, comment=genie_version)\n\n    # sv_df[\"ENTREZ_GENE_ID\"].mask(\n    #     sv_df[\"ENTREZ_GENE_ID\"] == 0, float(\"nan\"), inplace=True\n    # )\n    if not current_release_staging:\n        sv_staging_df = sv_df[\n            sv_df[\"SAMPLE_ID\"].isin(keep_for_center_consortium_samples)\n        ]\n        for center in center_mappingdf.center:\n            center_sv = sv_staging_df[sv_staging_df[\"CENTER\"] == center]\n            if not center_sv.empty:\n                center_sv.to_csv(SV_CENTER_PATH % center, sep=\"\\t\", index=False)\n                load.store_file(\n                    syn=syn,\n                    filepath=SV_CENTER_PATH % center,\n                    version_comment=genie_version,\n                    parentid=center_mappingdf[\"stagingSynId\"][\n                        center_mappingdf[\"center\"] == center\n                    ].values[0],\n                )\n\n    sv_df = sv_df[sv_df[\"SAMPLE_ID\"].isin(keep_for_merged_consortium_samples)]\n    sv_df = filter_out_germline_variants(input_data=sv_df, status_col_str=\"SV_STATUS\")\n    sv_df.rename(columns=transform._col_name_to_titlecase, inplace=True)\n    sv_text = process_functions.removePandasDfFloat(sv_df)\n    sv_path = os.path.join(GENIE_RELEASE_DIR, \"data_sv.txt\")\n    with open(sv_path, \"w\") as sv_file:\n        sv_file.write(sv_text)\n    load.store_file(\n        syn=syn,\n        filepath=sv_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"data_sv.txt\",\n        used=f\"{synid}.{version}\",\n    )\n    return sv_df[\"Sample_Id\"].tolist()\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.append_or_create_release_maf","title":"<code>append_or_create_release_maf(dataframe, filepath)</code>","text":"<p>Creates a file with the dataframe or appends to a existing file.</p> PARAMETER DESCRIPTION <code>df</code> <p>pandas.dataframe to write out</p> <p> </p> <code>filepath</code> <p>Filepath to append or create</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def append_or_create_release_maf(dataframe: pd.DataFrame, filepath: str):\n    \"\"\"Creates a file with the dataframe or appends to a existing file.\n\n    Args:\n        df: pandas.dataframe to write out\n        filepath: Filepath to append or create\n\n    \"\"\"\n    if not os.path.exists(filepath) or os.stat(filepath).st_size == 0:\n        data = process_functions.removePandasDfFloat(dataframe)\n        with open(filepath, \"w\") as f_out:\n            f_out.write(data)\n    else:\n        data = process_functions.removePandasDfFloat(dataframe, header=False)\n        with open(filepath, \"a\") as f_out:\n            f_out.write(data)\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_maf_files","title":"<code>store_maf_files(syn, genie_version, flatfiles_view_synid, release_synid, clinicaldf, center_mappingdf, keep_for_merged_consortium_samples, keep_for_center_consortium_samples, remove_mafinbed_variants, flagged_mutationInCis_variants, current_release_staging)</code>","text":"<p>Create, filter, configure, and store maf file</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>flatfiles_view_synid</code> <p>Synapse id of fileview with all the flat files</p> <p> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> </p> <code>clinicaldf</code> <p>Clinical dataframe with SAMPLE_ID and CENTER</p> <p> </p> <code>center_mappingdf</code> <p>Center mapping dataframe</p> <p> </p> <code>keep_for_merged_consortium_samples</code> <p>Samples to keep for merged file</p> <p> </p> <code>keep_for_center_consortium_samples</code> <p>Samples to keep for center files</p> <p> </p> <code>remove_mafinbed_variants</code> <p>Variants to remove</p> <p> </p> <code>flagged_mutationInCis_variants</code> <p>Variants to flag</p> <p> </p> <code>current_release_staging</code> <p>Staging flag</p> <p> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_maf_files(\n    syn,\n    genie_version,\n    flatfiles_view_synid,\n    release_synid,\n    clinicaldf,\n    center_mappingdf,\n    keep_for_merged_consortium_samples,\n    keep_for_center_consortium_samples,\n    remove_mafinbed_variants,\n    flagged_mutationInCis_variants,\n    current_release_staging,\n):\n    \"\"\"\n    Create, filter, configure, and store maf file\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version (ie. v6.1-consortium)\n        flatfiles_view_synid: Synapse id of fileview with all the flat files\n        release_synid: Synapse id to store release file\n        clinicaldf: Clinical dataframe with SAMPLE_ID and CENTER\n        center_mappingdf: Center mapping dataframe\n        keep_for_merged_consortium_samples: Samples to keep for merged file\n        keep_for_center_consortium_samples: Samples to keep for center files\n        remove_mafinbed_variants: Variants to remove\n        flagged_mutationInCis_variants: Variants to flag\n        current_release_staging: Staging flag\n    \"\"\"\n\n    logger.info(\"FILTERING, STORING MUTATION FILES\")\n    centerMafSynIdsDf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"select id from {flatfiles_view_synid} where name like '%mutation%'\",\n    )\n    mutations_path = os.path.join(GENIE_RELEASE_DIR, \"data_mutations_extended.txt\")\n    with open(mutations_path, \"w\"):\n        pass\n    # Create maf file per center for their staging directory\n    for center in clinicaldf[\"CENTER\"].unique():\n        with open(MUTATIONS_CENTER_PATH % center, \"w\"):\n            pass\n    used_entities = []\n    maf_ent = syn.get(centerMafSynIdsDf.id[0])\n    for _, mafSynId in enumerate(centerMafSynIdsDf.id):\n        maf_ent = syn.get(mafSynId)\n        logger.info(maf_ent.path)\n        # Extract center name\n        center = maf_ent.path.split(\"_\")[3].replace(\".txt\", \"\")\n        if center in center_mappingdf.center.tolist():\n            used_entities.append(f\"{maf_ent.id}.{maf_ent.versionNumber}\")\n            mafchunks = pd.read_csv(\n                maf_ent.path, sep=\"\\t\", comment=\"#\", chunksize=100000\n            )\n\n            for mafchunk in mafchunks:\n                # Get center for center staging maf\n                # Configure maf\n                configured_mafdf = configure_maf(\n                    mafchunk, remove_mafinbed_variants, flagged_mutationInCis_variants\n                )\n                configured_mafdf = configured_mafdf[FULL_MAF_RELEASE_COLUMNS]\n                # Create maf for release\n                merged_mafdf = remove_maf_samples(\n                    configured_mafdf, keep_for_merged_consortium_samples\n                )\n                append_or_create_release_maf(merged_mafdf, mutations_path)\n                # Create maf for center staging\n                center_mafdf = remove_maf_samples(\n                    configured_mafdf, keep_for_center_consortium_samples\n                )\n                append_or_create_release_maf(\n                    center_mafdf, MUTATIONS_CENTER_PATH % center\n                )\n\n    load.store_file(\n        syn=syn,\n        filepath=mutations_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"data_mutations_extended.txt\",\n        used=used_entities,\n    )\n\n    if not current_release_staging:\n        for center in clinicaldf[\"CENTER\"].unique():\n            staging_synid = center_mappingdf[\"stagingSynId\"][\n                center_mappingdf[\"center\"] == center\n            ][0]\n            load.store_file(\n                syn=syn,\n                filepath=MUTATIONS_CENTER_PATH % center,\n                version_comment=genie_version,\n                parentid=staging_synid,\n            )\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.run_genie_filters","title":"<code>run_genie_filters(syn, genie_version, variant_filtering_synId, clinicaldf, beddf, center_mappingdf, processing_date, skip_mutationsincis, consortium_release_cutoff, test, current_release_staging)</code>","text":"<p>Run GENIE filters and returns variants and samples to remove</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>variant_filtering_synId</code> <p>Synapse id of mutationInCis table</p> <p> </p> <code>clinicaldf</code> <p>Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID</p> <p> </p> <code>beddf</code> <p>Bed dataframe</p> <p> </p> <code>center_mappingdf</code> <p>Center mapping dataframe</p> <p> </p> <code>processing_date</code> <p>Processing date</p> <p> </p> <code>skip_mutationsincis</code> <p>Skip mutation in cis filter</p> <p> </p> <code>consortium_release_cutoff</code> <p>Release cutoff in days</p> <p> </p> <code>test</code> <p>Test flag</p> <p> </p> <code>current_release_staging</code> <p>Staging flag</p> <p> </p> RETURNS DESCRIPTION <p>pandas.Series: Variants to remove</p> <code>set</code> <p>samples to remove for release files</p> <code>set</code> <p>samples to remove for center files</p> <p>pandas.Series: Variants to flag</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def run_genie_filters(\n    syn,\n    genie_version,\n    variant_filtering_synId,\n    clinicaldf,\n    beddf,\n    center_mappingdf,\n    processing_date,\n    skip_mutationsincis,\n    consortium_release_cutoff,\n    test,\n    current_release_staging,\n):\n    \"\"\"\n    Run GENIE filters and returns variants and samples to remove\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version (ie. v6.1-consortium)\n        variant_filtering_synId: Synapse id of mutationInCis table\n        clinicaldf: Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID\n        beddf: Bed dataframe\n        center_mappingdf: Center mapping dataframe\n        processing_date: Processing date\n        skip_mutationsincis: Skip mutation in cis filter\n        consortium_release_cutoff: Release cutoff in days\n        test: Test flag\n        current_release_staging: Staging flag\n\n    Returns:\n        pandas.Series: Variants to remove\n        set: samples to remove for release files\n        set: samples to remove for center files\n        pandas.Series: Variants to flag\n    \"\"\"\n\n    # ADD CHECKS TO CODE BEFORE UPLOAD.\n    # Throw error if things don't go through\n    logger.info(\"RUN GENIE FILTERS\")\n    # STORING CLINICAL FILES INTO CBIOPORTAL\n    # FILTERING\n    logger.info(\"MAF IN BED FILTER\")\n    remove_mafinbed_variants = runMAFinBED(\n        syn,\n        center_mappingdf,\n        test=test,\n        staging=current_release_staging,\n        genieVersion=genie_version,\n    )\n\n    logger.info(\"MUTATION IN CIS FILTER\")\n    (\n        remove_mutationincis_samples,\n        flagged_mutationincis_variants,\n    ) = mutation_in_cis_filter(\n        syn,\n        skip_mutationsincis,\n        variant_filtering_synId,\n        center_mappingdf,\n        genieVersion=genie_version,\n        test=test,\n        staging=current_release_staging,\n    )\n    remove_no_genepanel_samples = no_genepanel_filter(clinicaldf, beddf)\n\n    logger.info(\"SEQ DATE FILTER\")\n    remove_seqdate_samples = seq_date_filter(\n        clinicaldf, processing_date, consortium_release_cutoff\n    )\n\n    # Only certain samples are removed for the files that go into\n    # staging center folder\n    remove_center_consortium_samples = set(remove_mutationincis_samples).union(\n        set(remove_no_genepanel_samples)\n    )\n    # Most filteres are applied for the files that go into the merged\n    # consortium release\n    remove_merged_consortium_samples = set(remove_seqdate_samples)\n\n    remove_merged_consortium_samples = remove_merged_consortium_samples.union(\n        remove_center_consortium_samples\n    )\n\n    return (\n        remove_mafinbed_variants,\n        remove_merged_consortium_samples,\n        remove_center_consortium_samples,\n        flagged_mutationincis_variants,\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_assay_info_files","title":"<code>store_assay_info_files(syn, genie_version, assay_info_synid, clinicaldf, release_synid)</code>","text":"<p>Creates, stores assay information and gets WES panel list</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>assay_info_synid</code> <p>Assay information database synid</p> <p> </p> <code>clinicaldf</code> <p>Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID</p> <p> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> </p> RETURNS DESCRIPTION <p>List of whole exome sequencing SEQ_ASSAY_IDs</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_assay_info_files(\n    syn, genie_version, assay_info_synid, clinicaldf, release_synid\n):\n    \"\"\"Creates, stores assay information and gets WES panel list\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version (ie. v6.1-consortium)\n        assay_info_synid: Assay information database synid\n        clinicaldf: Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID\n        release_synid: Synapse id to store release file\n\n    Returns:\n        List of whole exome sequencing SEQ_ASSAY_IDs\n    \"\"\"\n    logger.info(\"Creates assay information file\")\n    assay_info_path = os.path.join(GENIE_RELEASE_DIR, \"assay_information.txt\")\n    seq_assay_str = \"','\".join(clinicaldf[\"SEQ_ASSAY_ID\"].unique())\n    version = syn.create_snapshot_version(assay_info_synid, comment=genie_version)\n    assay_infodf = extract.get_syntabledf(\n        syn,\n        f\"select * from {assay_info_synid} where SEQ_ASSAY_ID \"\n        f\"in ('{seq_assay_str}')\",\n    )\n    assay_infodf.to_csv(assay_info_path, sep=\"\\t\", index=False)\n    load.store_file(\n        syn=syn,\n        filepath=assay_info_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"assay_information.txt\",\n        used=f\"{assay_info_synid}.{version}\",\n    )\n    wes_index = assay_infodf[\"library_strategy\"] == \"WXS\"\n    wes_panels = assay_infodf[\"SEQ_ASSAY_ID\"][wes_index]\n    return wes_panels.tolist()\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_clinical_files","title":"<code>store_clinical_files(syn, genie_version, clinicaldf, oncotree_url, sample_cols, patient_cols, remove_center_consortium_samples, remove_merged_consortium_samples, release_synid, current_release_staging, center_mappingdf, databaseSynIdMappingDf, used=None)</code>","text":"<p>Create, filter, configure, and store clinical file</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>clinicaldf</code> <p>Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID</p> <p> </p> <code>oncotree_url</code> <p>Oncotree URL</p> <p> </p> <code>sample_cols</code> <p>Clinical sample columns</p> <p> </p> <code>patient_cols</code> <p>Clinical patient columns</p> <p> </p> <code>remove_center_consortium_samples</code> <p>Samples to remove for center files</p> <p> </p> <code>remove_merged_consortium_samples</code> <p>Samples to remove for merged file</p> <p> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> </p> <code>current_release_staging</code> <p>Staging flag</p> <p> </p> <code>center_mappingdf</code> <p>Center mapping dataframe</p> <p> </p> <code>databaseSynIdMappingDf</code> <p>Database to Synapse Id mapping</p> <p> </p> RETURNS DESCRIPTION <p>pandas.DataFrame: configured clinical dataframe</p> <p>pandas.Series: samples to keep for center files</p> <p>pandas.Series: samples to keep for release files</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_clinical_files(\n    syn,\n    genie_version,\n    clinicaldf,\n    oncotree_url,\n    sample_cols,\n    patient_cols,\n    remove_center_consortium_samples,\n    remove_merged_consortium_samples,\n    release_synid,\n    current_release_staging,\n    center_mappingdf,\n    databaseSynIdMappingDf,\n    used=None,\n):\n    \"\"\"\n    Create, filter, configure, and store clinical file\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version (ie. v6.1-consortium)\n        clinicaldf: Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID\n        oncotree_url: Oncotree URL\n        sample_cols: Clinical sample columns\n        patient_cols: Clinical patient columns\n        remove_center_consortium_samples: Samples to remove for center files\n        remove_merged_consortium_samples: Samples to remove for merged file\n        release_synid: Synapse id to store release file\n        current_release_staging: Staging flag\n        center_mappingdf: Center mapping dataframe\n        databaseSynIdMappingDf: Database to Synapse Id mapping\n\n    Returns:\n        pandas.DataFrame: configured clinical dataframe\n        pandas.Series: samples to keep for center files\n        pandas.Series: samples to keep for release files\n    \"\"\"\n\n    logger.info(\"CONFIGURING CLINICAL FILES\")\n    logger.info(\"REMOVING PHI\")\n    # clinicaldf = redact_phi(clinicaldf)\n    logger.info(\"ADD CANCER TYPES\")\n    # This removes support for both oncotree urls (only support json)\n    oncotree_dict = process_functions.get_oncotree_code_mappings(oncotree_url)\n    # Add in unknown key which maps to UNKNOWN everything\n    oncotree_dict[\"UNKNOWN\"] = {\n        \"CANCER_TYPE\": \"UNKNOWN\",\n        \"CANCER_TYPE_DETAILED\": \"UNKNOWN\",\n        \"ONCOTREE_PRIMARY_NODE\": \"UNKNOWN\",\n        \"ONCOTREE_SECONDARY_NODE\": \"UNKNOWN\",\n    }\n\n    clinicaldf[\"CANCER_TYPE\"] = [\n        (\n            oncotree_dict[code.upper()][\"CANCER_TYPE\"]\n            if code.upper() in oncotree_dict.keys()\n            else float(\"nan\")\n        )\n        for code in clinicaldf[\"ONCOTREE_CODE\"]\n    ]\n\n    clinicaldf[\"CANCER_TYPE_DETAILED\"] = [\n        (\n            oncotree_dict[code.upper()][\"CANCER_TYPE_DETAILED\"]\n            if code.upper() in oncotree_dict.keys()\n            else float(\"nan\")\n        )\n        for code in clinicaldf[\"ONCOTREE_CODE\"]\n    ]\n\n    clinicaldf[\"ONCOTREE_PRIMARY_NODE\"] = [\n        (\n            oncotree_dict[code.upper()][\"ONCOTREE_PRIMARY_NODE\"]\n            if code.upper() in oncotree_dict.keys()\n            else float(\"nan\")\n        )\n        for code in clinicaldf[\"ONCOTREE_CODE\"]\n    ]\n\n    clinicaldf[\"ONCOTREE_SECONDARY_NODE\"] = [\n        (\n            oncotree_dict[code.upper()][\"ONCOTREE_SECONDARY_NODE\"]\n            if code.upper() in oncotree_dict.keys()\n            else float(\"nan\")\n        )\n        for code in clinicaldf[\"ONCOTREE_CODE\"]\n    ]\n\n    # All cancer types that are null contain deprecated oncotree codes\n    # And should be removed\n    clinicaldf = clinicaldf[~clinicaldf[\"CANCER_TYPE\"].isnull()]\n    # Suggest using AGE_AT_SEQ_REPORT_DAYS instead so that the\n    # descriptions can match\n    clinicaldf[\"AGE_AT_SEQ_REPORT_DAYS\"] = clinicaldf[\"AGE_AT_SEQ_REPORT\"]\n    clinicaldf[\"AGE_AT_SEQ_REPORT\"] = [\n        (\n            int(math.floor(int(float(age)) / 365.25))\n            if process_functions.checkInt(age)\n            else age\n        )\n        for age in clinicaldf[\"AGE_AT_SEQ_REPORT\"]\n    ]\n    clinicaldf[\"AGE_AT_SEQ_REPORT\"][clinicaldf[\"AGE_AT_SEQ_REPORT\"] == \"&gt;32485\"] = \"&gt;89\"\n    clinicaldf[\"AGE_AT_SEQ_REPORT\"][clinicaldf[\"AGE_AT_SEQ_REPORT\"] == \"&lt;6570\"] = \"&lt;18\"\n\n    ############################################################\n    # CENTER SPECIFIC CODE FOR RIGHT NOW (REMOVE UHN-555-V1)\n    ############################################################\n    # clinicalDf = clinicalDf[clinicalDf['SEQ_ASSAY_ID'] != \"UHN-555-V1\"]\n    # clinicalDf = clinicalDf[clinicalDf['SEQ_ASSAY_ID'] != \"PHS-TRISEQ-V1\"]\n\n    # clinicalDf = clinicalDf[clinicalDf['CENTER'] != \"WAKE\"]\n    # clinicalDf = clinicalDf[clinicalDf['CENTER'] != \"CRUK\"]\n    ############################################################\n    ############################################################\n\n    clinicaldf.drop_duplicates(\"SAMPLE_ID\", inplace=True)\n\n    logger.info(\"STORING CLINICAL FILES\")\n    # samples must be removed after reading in the clinical file again\n    staging_clinicaldf = clinicaldf[\n        ~clinicaldf[\"SAMPLE_ID\"].isin(remove_center_consortium_samples)\n    ]\n    if not current_release_staging:\n        for center in center_mappingdf.center:\n            center_clinical = staging_clinicaldf[staging_clinicaldf[\"CENTER\"] == center]\n            center_sample = center_clinical[sample_cols].drop_duplicates(\"SAMPLE_ID\")\n            center_patient = center_clinical[patient_cols].drop_duplicates(\"PATIENT_ID\")\n            center_sample.to_csv(SAMPLE_CENTER_PATH % center, sep=\"\\t\", index=False)\n            center_patient.to_csv(PATIENT_CENTER_PATH % center, sep=\"\\t\", index=False)\n            load.store_file(\n                syn=syn,\n                filepath=SAMPLE_CENTER_PATH % center,\n                version_comment=genie_version,\n                parentid=center_mappingdf[\"stagingSynId\"][\n                    center_mappingdf[\"center\"] == center\n                ][0],\n            )\n            load.store_file(\n                syn=syn,\n                filepath=PATIENT_CENTER_PATH % center,\n                version_comment=genie_version,\n                parentid=center_mappingdf[\"stagingSynId\"][\n                    center_mappingdf[\"center\"] == center\n                ][0],\n            )\n\n    clinicaldf = clinicaldf[\n        ~clinicaldf[\"SAMPLE_ID\"].isin(remove_merged_consortium_samples)\n    ]\n\n    keep_center_consortium_samples = staging_clinicaldf.SAMPLE_ID\n    keep_merged_consortium_samples = clinicaldf.SAMPLE_ID\n    # This mapping table is the GENIE clinical code to description\n    # mapping to generate the headers of the clinical file\n    clinical_code_to_desc_map_synid = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"clinical_code_to_desc_map\"\n    ][0]\n    mapping = extract.get_syntabledf(\n        syn=syn, query_string=f\"SELECT * FROM {clinical_code_to_desc_map_synid}\"\n    )\n    clinical_path = os.path.join(GENIE_RELEASE_DIR, \"data_clinical.txt\")\n    clinical_sample_path = os.path.join(GENIE_RELEASE_DIR, \"data_clinical_sample.txt\")\n    clinical_patient_path = os.path.join(GENIE_RELEASE_DIR, \"data_clinical_patient.txt\")\n    process_functions.addClinicalHeaders(\n        clinicaldf,\n        mapping,\n        patient_cols,\n        sample_cols,\n        clinical_sample_path,\n        clinical_patient_path,\n    )\n    load.store_file(\n        syn=syn,\n        filepath=clinical_sample_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"data_clinical_sample.txt\",\n        used=used,\n    )\n\n    load.store_file(\n        syn=syn,\n        filepath=clinical_patient_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"data_clinical_patient.txt\",\n        used=used,\n    )\n\n    clinicaldf.to_csv(clinical_path, sep=\"\\t\", index=False)\n    load.store_file(\n        syn=syn,\n        filepath=clinical_path,\n        parentid=release_synid,\n        name=\"data_clinical.txt\",\n        used=used,\n        version_comment=\"database\",\n    )\n\n    return (clinicaldf, keep_center_consortium_samples, keep_merged_consortium_samples)\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_cna_files","title":"<code>store_cna_files(syn, flatfiles_view_synid, keep_for_center_consortium_samples, keep_for_merged_consortium_samples, center_mappingdf, genie_version, release_synid, current_release_staging)</code>","text":"<p>Create, filter and store cna file</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>flatfiles_view_synid</code> <p>Synapse id of fileview with all the flat files</p> <p> </p> <code>keep_for_center_consortium_samples</code> <p>Samples to keep for center files</p> <p> </p> <code>keep_for_merged_consortium_samples</code> <p>Samples to keep for merged file</p> <p> </p> <code>center_mappingdf</code> <p>Center mapping dataframe</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> </p> <p>Returns:     list: CNA samples</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_cna_files(\n    syn,\n    flatfiles_view_synid,\n    keep_for_center_consortium_samples,\n    keep_for_merged_consortium_samples,\n    center_mappingdf,\n    genie_version,\n    release_synid,\n    current_release_staging,\n):\n    \"\"\"\n    Create, filter and store cna file\n\n    Args:\n        syn: Synapse object\n        flatfiles_view_synid: Synapse id of fileview with all the flat files\n        keep_for_center_consortium_samples: Samples to keep for center files\n        keep_for_merged_consortium_samples: Samples to keep for merged file\n        center_mappingdf: Center mapping dataframe\n        genie_version: GENIE version (ie. v6.1-consortium)\n        release_synid: Synapse id to store release file\n    Returns:\n        list: CNA samples\n    \"\"\"\n    logger.info(\"MERING, FILTERING, STORING CNA FILES\")\n    cna_path = os.path.join(GENIE_RELEASE_DIR, \"data_CNA.txt\")\n    query_str = (\"select id from {} \" \"where name like 'data_CNA%'\").format(\n        flatfiles_view_synid\n    )\n    center_cna_synidsdf = extract.get_syntabledf(syn, query_str)\n    # Grab all unique symbols and form cna_template\n    all_symbols = set()\n    for cna_synid in center_cna_synidsdf[\"id\"]:\n        cna_ent = syn.get(cna_synid)\n        with open(cna_ent.path, \"r\") as cna_file:\n            # Read first line first\n            cna_file.readline()\n            # Get all hugo symbols\n            all_symbols = all_symbols.union(\n                set(line.split(\"\\t\")[0] for line in cna_file)\n            )\n    cna_template = pd.DataFrame({\"Hugo_Symbol\": list(all_symbols)})\n    cna_template.sort_values(\"Hugo_Symbol\", inplace=True)\n    cna_template.to_csv(cna_path, sep=\"\\t\", index=False)\n    # Loop through to create finalized CNA file\n    with_center_hugo_symbol = pd.Series(\"Hugo_Symbol\")\n    with_center_hugo_symbol = pd.concat(\n        [with_center_hugo_symbol, pd.Series(keep_for_center_consortium_samples)]\n    )\n\n    with_merged_hugo_symbol = pd.Series(\"Hugo_Symbol\")\n    with_merged_hugo_symbol = pd.concat(\n        [with_merged_hugo_symbol, pd.Series(keep_for_merged_consortium_samples)]\n    )\n\n    cna_samples = []\n    used_entities = []\n    for cna_synId in center_cna_synidsdf[\"id\"]:\n        cna_ent = syn.get(cna_synId)\n        center = cna_ent.name.replace(\"data_CNA_\", \"\").replace(\".txt\", \"\")\n        logger.info(cna_ent.path)\n        if center in center_mappingdf.center.tolist():\n            used_entities.append(f\"{cna_synId}.{cna_ent.versionNumber}\")\n            center_cna = pd.read_csv(cna_ent.path, sep=\"\\t\")\n            merged_cna = cna_template.merge(center_cna, on=\"Hugo_Symbol\", how=\"outer\")\n            merged_cna.sort_values(\"Hugo_Symbol\", inplace=True)\n\n            if not current_release_staging:\n                merged_cna = merged_cna[\n                    merged_cna.columns[merged_cna.columns.isin(with_center_hugo_symbol)]\n                ]\n\n                cna_text = process_functions.removePandasDfFloat(merged_cna)\n                # Replace blank with NA's\n                cna_text = cna_text.replace(\"\\t\\t\", \"\\tNA\\t\")\n                cna_text = cna_text.replace(\"\\t\\t\", \"\\tNA\\t\")\n                cna_text = cna_text.replace(\"\\t\\n\", \"\\tNA\\n\")\n\n                # Store center CNA file in staging dir\n                with open(CNA_CENTER_PATH % center, \"w\") as cna_file:\n                    cna_file.write(cna_text)\n                load.store_file(\n                    syn=syn,\n                    filepath=CNA_CENTER_PATH % center,\n                    version_comment=genie_version,\n                    parentid=center_mappingdf[\"stagingSynId\"][\n                        center_mappingdf[\"center\"] == center\n                    ][0],\n                )\n            # This is to remove more samples for the final cna file\n            merged_cna = merged_cna[\n                merged_cna.columns[merged_cna.columns.isin(with_merged_hugo_symbol)]\n            ]\n\n            cna_text = process_functions.removePandasDfFloat(merged_cna)\n            cna_text = cna_text.replace(\"\\t\\t\", \"\\tNA\\t\")\n            cna_text = cna_text.replace(\"\\t\\t\", \"\\tNA\\t\")\n            cna_text = cna_text.replace(\"\\t\\n\", \"\\tNA\\n\")\n\n            with open(CNA_CENTER_PATH % center, \"w\") as cna_file:\n                cna_file.write(cna_text)\n            # Join CNA file\n            cna_samples.extend(merged_cna.columns[1:].tolist())\n            linux_join_command = [\"join\", cna_path, CNA_CENTER_PATH % center]\n            output = subprocess.check_output(linux_join_command)\n            with open(cna_path, \"w\") as cna_file:\n                cna_file.write(output.decode(\"utf-8\").replace(\" \", \"\\t\"))\n\n    load.store_file(\n        syn=syn,\n        filepath=cna_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"data_CNA.txt\",\n        used=used_entities,\n    )\n\n    return cna_samples\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_seg_files","title":"<code>store_seg_files(syn, genie_version, seg_synid, release_synid, keep_for_center_consortium_samples, keep_for_merged_consortium_samples, center_mappingdf, current_release_staging)</code>","text":"<p>Create, filter and store seg file</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>seg_synid</code> <p>Seg database synid</p> <p> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> </p> <code>keep_for_center_consortium_samples</code> <p>Samples to keep for center files</p> <p> </p> <code>keep_for_merged_consortium_samples</code> <p>Samples to keep for merged file</p> <p> </p> <code>center_mappingdf</code> <p>Center mapping dataframe</p> <p> </p> <code>current_release_staging</code> <p>Staging flag</p> <p> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_seg_files(\n    syn,\n    genie_version,\n    seg_synid,\n    release_synid,\n    keep_for_center_consortium_samples,\n    keep_for_merged_consortium_samples,\n    center_mappingdf,\n    current_release_staging,\n):\n    \"\"\"\n    Create, filter and store seg file\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version (ie. v6.1-consortium)\n        seg_synid: Seg database synid\n        release_synid: Synapse id to store release file\n        keep_for_center_consortium_samples: Samples to keep for center files\n        keep_for_merged_consortium_samples: Samples to keep for merged file\n        center_mappingdf: Center mapping dataframe\n        current_release_staging: Staging flag\n    \"\"\"\n    logger.info(\"MERING, FILTERING, STORING SEG FILES\")\n    seg_path = os.path.join(GENIE_RELEASE_DIR, \"data_cna_hg19.seg\")\n    version = syn.create_snapshot_version(seg_synid, comment=genie_version)\n\n    segdf = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT ID,CHROM,LOCSTART,LOCEND,NUMMARK,SEGMEAN,CENTER FROM {seg_synid}\",\n    )\n    segdf = segdf.rename(\n        columns={\n            \"CHROM\": \"chrom\",\n            \"LOCSTART\": \"loc.start\",\n            \"LOCEND\": \"loc.end\",\n            \"SEGMEAN\": \"seg.mean\",\n            \"NUMMARK\": \"num.mark\",\n        }\n    )\n    if not current_release_staging:\n        staging_segdf = segdf[segdf[\"ID\"].isin(keep_for_center_consortium_samples)]\n        for center in center_mappingdf.center:\n            center_seg = staging_segdf[staging_segdf[\"CENTER\"] == center]\n            if not center_seg.empty:\n                del center_seg[\"CENTER\"]\n                segtext = process_functions.removePandasDfFloat(center_seg)\n                with open(SEG_CENTER_PATH % center, \"w\") as seg_file:\n                    seg_file.write(segtext)\n                load.store_file(\n                    syn=syn,\n                    filepath=SEG_CENTER_PATH % center,\n                    version_comment=genie_version,\n                    parentid=center_mappingdf[\"stagingSynId\"][\n                        center_mappingdf[\"center\"] == center\n                    ][0],\n                )\n    del segdf[\"CENTER\"]\n    segdf = segdf[segdf[\"ID\"].isin(keep_for_merged_consortium_samples)]\n    segtext = process_functions.removePandasDfFloat(segdf)\n    with open(seg_path, \"w\") as seg_file:\n        seg_file.write(segtext)\n    load.store_file(\n        syn=syn,\n        filepath=seg_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"data_cna_hg19.seg\",\n        used=f\"{seg_synid}.{version}\",\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_data_gene_matrix","title":"<code>store_data_gene_matrix(syn, genie_version, clinicaldf, cna_samples, release_synid, wes_seqassayids, sv_samples)</code>","text":"<p>Create and store data gene matrix file</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>clinicaldf</code> <p>Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID</p> <p> </p> <code>cna_samples</code> <p>Samples with CNA</p> <p> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> </p> <code>wes_seqassayids</code> <p>Whole exome sequencing SEQ_ASSAY_IDs</p> <p> </p> <code>sv_samples</code> <p>Samples with SV</p> <p> </p> RETURNS DESCRIPTION <p>pandas.DataFrame: data gene matrix dataframe</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_data_gene_matrix(\n    syn,\n    genie_version,\n    clinicaldf,\n    cna_samples,\n    release_synid,\n    wes_seqassayids,\n    sv_samples,\n):\n    \"\"\"\n    Create and store data gene matrix file\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version (ie. v6.1-consortium)\n        clinicaldf: Clinical dataframe with SAMPLE_ID and SEQ_ASSAY_ID\n        cna_samples: Samples with CNA\n        release_synid: Synapse id to store release file\n        wes_seqassayids: Whole exome sequencing SEQ_ASSAY_IDs\n        sv_samples: Samples with SV\n\n    Returns:\n        pandas.DataFrame: data gene matrix dataframe\n    \"\"\"\n    logger.info(\"STORING DATA GENE MATRIX FILE\")\n    data_gene_matrix_path = os.path.join(GENIE_RELEASE_DIR, \"data_gene_matrix.txt\")\n    # Samples have already been removed\n    data_gene_matrix = pd.DataFrame(columns=[\"SAMPLE_ID\", \"SEQ_ASSAY_ID\"])\n    data_gene_matrix = pd.concat(\n        [data_gene_matrix, clinicaldf[[\"SAMPLE_ID\", \"SEQ_ASSAY_ID\"]]]\n    )\n    data_gene_matrix = data_gene_matrix.rename(columns={\"SEQ_ASSAY_ID\": \"mutations\"})\n    data_gene_matrix = data_gene_matrix[data_gene_matrix[\"SAMPLE_ID\"] != \"\"]\n    data_gene_matrix.drop_duplicates(\"SAMPLE_ID\", inplace=True)\n\n    # exclude wes assay_ids\n    wes_panel_mut = data_gene_matrix[\"mutations\"].isin(wes_seqassayids)\n    data_gene_matrix = data_gene_matrix[~wes_panel_mut]\n\n    # Add in CNA column into gene panel file\n    data_gene_matrix = process_functions.add_columns_to_data_gene_matrix(\n        data_gene_matrix=data_gene_matrix, sample_list=cna_samples, column_name=\"cna\"\n    )\n\n    # Add SV column into gene panel file\n    data_gene_matrix = process_functions.add_columns_to_data_gene_matrix(\n        data_gene_matrix=data_gene_matrix, sample_list=sv_samples, column_name=\"sv\"\n    )\n\n    data_gene_matrix.to_csv(data_gene_matrix_path, sep=\"\\t\", index=False)\n\n    load.store_file(\n        syn=syn,\n        filepath=data_gene_matrix_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"data_gene_matrix.txt\",\n    )\n    return data_gene_matrix\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.store_bed_files","title":"<code>store_bed_files(syn, genie_version, beddf, seq_assay_ids, center_mappingdf, current_release_staging, release_synid, used=None)</code>","text":"<p>Store bed files, store the bed regions that had symbols remapped Filters bed file by clinical dataframe seq assays</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version (ie. v6.1-consortium)</p> <p> </p> <code>beddf</code> <p>Bed dataframe</p> <p> </p> <code>seq_assay_ids</code> <p>All SEQ_ASSAY_IDs in the clinical file</p> <p> </p> <code>center_mappingdf</code> <p>Center mapping dataframe</p> <p> </p> <code>current_release_staging</code> <p>Staging flag</p> <p> </p> <code>release_synid</code> <p>Synapse id to store release file</p> <p> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def store_bed_files(\n    syn,\n    genie_version,\n    beddf,\n    seq_assay_ids,\n    center_mappingdf,\n    current_release_staging,\n    release_synid,\n    used=None,\n):\n    \"\"\"\n    Store bed files, store the bed regions that had symbols remapped\n    Filters bed file by clinical dataframe seq assays\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version (ie. v6.1-consortium)\n        beddf: Bed dataframe\n        seq_assay_ids: All SEQ_ASSAY_IDs in the clinical file\n        center_mappingdf: Center mapping dataframe\n        current_release_staging: Staging flag\n        release_synid: Synapse id to store release file\n    \"\"\"\n    logger.info(\"STORING COMBINED BED FILE\")\n    combined_bed_path = os.path.join(GENIE_RELEASE_DIR, \"genomic_information.txt\")\n    if not current_release_staging:\n        for seq_assay in beddf[\"SEQ_ASSAY_ID\"].unique():\n            bed_seq_df = beddf[beddf[\"SEQ_ASSAY_ID\"] == seq_assay]\n            center = seq_assay.split(\"-\")[0]\n            bed_seq_df = bed_seq_df[bed_seq_df[\"Hugo_Symbol\"] != bed_seq_df[\"ID\"]]\n            # There should always be a match here, because there should never\n            # be a SEQ_ASSAY_ID that starts without the center name\n            # If there is, check the bed db for SEQ_ASSAY_ID\n            center_ind = center_mappingdf[\"center\"] == center\n            if not bed_seq_df.empty:\n                bed_seq_df.to_csv(BED_DIFFS_SEQASSAY_PATH % seq_assay, index=False)\n                load.store_file(\n                    syn=syn,\n                    filepath=BED_DIFFS_SEQASSAY_PATH % seq_assay,\n                    version_comment=genie_version,\n                    parentid=center_mappingdf[\"stagingSynId\"][center_ind][0],\n                )\n    # This clinicalDf is already filtered through most of the filters\n    beddf = beddf[beddf[\"SEQ_ASSAY_ID\"].isin(seq_assay_ids)]\n    beddf.to_csv(combined_bed_path, sep=\"\\t\", index=False)\n    load.store_file(\n        syn=syn,\n        filepath=combined_bed_path,\n        parentid=release_synid,\n        version_comment=genie_version,\n        name=\"genomic_information.txt\",\n        used=used,\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.stagingToCbio","title":"<code>stagingToCbio(syn, processingDate, genieVersion, CENTER_MAPPING_DF, databaseSynIdMappingDf, oncotree_url=None, consortiumReleaseCutOff=183, current_release_staging=False, skipMutationsInCis=False, test=False)</code>","text":"<p>Main function that takes the GENIE database and creates release files</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>processingDate</code> <p>Processing date in form of Apr-XXXX</p> <p> </p> <code>genieVersion</code> <p>GENIE version. Default is test.</p> <p> </p> <code>CENTER_MAPPING_DF</code> <p>center mapping dataframe</p> <p> </p> <code>databaseSynIdMappingDf</code> <p>Database to Synapse Id mapping</p> <p> </p> <code>oncotree_url</code> <p>Oncotree link</p> <p> DEFAULT: <code>None</code> </p> <code>consortiumReleaseCutOff</code> <p>Release cut off days</p> <p> DEFAULT: <code>183</code> </p> <code>current_release_staging</code> <p>Is it staging. Default is False.</p> <p> DEFAULT: <code>False</code> </p> <code>skipMutationsInCis</code> <p>Skip mutation in cis filter. Default is False.</p> <p> DEFAULT: <code>False</code> </p> <code>test</code> <p>Testing parameter. Default is False.</p> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list</code> <p>Gene panel entities</p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def stagingToCbio(\n    syn,\n    processingDate,\n    genieVersion,\n    CENTER_MAPPING_DF,\n    databaseSynIdMappingDf,\n    oncotree_url=None,\n    consortiumReleaseCutOff=183,\n    current_release_staging=False,\n    skipMutationsInCis=False,\n    test=False,\n):\n    \"\"\"\n    Main function that takes the GENIE database and creates release files\n\n    Args:\n        syn: Synapse object\n        processingDate: Processing date in form of Apr-XXXX\n        genieVersion: GENIE version. Default is test.\n        CENTER_MAPPING_DF: center mapping dataframe\n        databaseSynIdMappingDf: Database to Synapse Id mapping\n        oncotree_url: Oncotree link\n        consortiumReleaseCutOff: Release cut off days\n        current_release_staging: Is it staging. Default is False.\n        skipMutationsInCis: Skip mutation in cis filter. Default is False.\n        test: Testing parameter. Default is False.\n\n    Returns:\n        list: Gene panel entities\n    \"\"\"\n    if not os.path.exists(GENIE_RELEASE_DIR):\n        os.mkdir(GENIE_RELEASE_DIR)\n    consortiumReleaseSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"consortium\"\n    ][0]\n    centerMafFileViewSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"centerMafView\"\n    ][0]\n    patientSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"patient\"\n    ][0]\n    sampleSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"sample\"\n    ][0]\n    bedSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"bed\"\n    ][0]\n    fileviewSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"fileview\"\n    ][0]\n    segSynId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"seg\"\n    ][0]\n    variant_filtering_synId = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"mutationsInCis\"\n    ][0]\n    sv_synid = databaseSynIdMappingDf[\"Id\"][databaseSynIdMappingDf[\"Database\"] == \"sv\"][\n        0\n    ]\n    clinical_tier_release_scope_synid = databaseSynIdMappingDf[\"Id\"][\n        databaseSynIdMappingDf[\"Database\"] == \"clinical_tier_release_scope\"\n    ][0]\n    # Grab assay information\n    assay_info_ind = databaseSynIdMappingDf[\"Database\"] == \"assayinfo\"\n    assay_info_synid = databaseSynIdMappingDf[\"Id\"][assay_info_ind][0]\n\n    # Using center mapping df to gate centers in release fileStage\n    center_query_str = \"','\".join(CENTER_MAPPING_DF.center)\n\n    patient_snapshot = syn.create_snapshot_version(patientSynId, comment=genieVersion)\n    patient_used = f\"{patientSynId}.{patient_snapshot}\"\n    patientDf = extract.get_syntabledf(\n        syn, f\"SELECT * FROM {patientSynId} where CENTER in ('{center_query_str}')\"\n    )\n    sample_snapshot = syn.create_snapshot_version(sampleSynId, comment=genieVersion)\n    sample_used = f\"{sampleSynId}.{sample_snapshot}\"\n    sampleDf = extract.get_syntabledf(\n        syn, f\"SELECT * FROM {sampleSynId} where CENTER in ('{center_query_str}')\"\n    )\n    bed_snapshot = syn.create_snapshot_version(bedSynId, comment=genieVersion)\n    bed_used = f\"{bedSynId}.{bed_snapshot}\"\n    bedDf = extract.get_syntabledf(\n        syn,\n        \"SELECT Chromosome,Start_Position,End_Position,Hugo_Symbol,ID,\"\n        \"SEQ_ASSAY_ID,Feature_Type,includeInPanel,clinicalReported FROM\"\n        f\" {bedSynId} where CENTER in ('{center_query_str}')\",\n    )\n\n    # Clinical release scope filter\n    # If private -&gt; Don't release to public\n    clinicalReleaseScopeDf = extract.get_syntabledf(\n        syn,\n        f\"SELECT * FROM {clinical_tier_release_scope_synid} where releaseScope &lt;&gt; 'private'\",\n    )\n\n    patientCols = clinicalReleaseScopeDf[\"fieldName\"][\n        clinicalReleaseScopeDf[\"level\"] == \"patient\"\n    ].tolist()\n    sampleCols = clinicalReleaseScopeDf[\"fieldName\"][\n        clinicalReleaseScopeDf[\"level\"] == \"sample\"\n    ].tolist()\n\n    # Remove this when these columns are removed from both databases\n    if sampleDf.get(\"AGE_AT_SEQ_REPORT_NUMERICAL\") is not None:\n        del sampleDf[\"AGE_AT_SEQ_REPORT_NUMERICAL\"]\n    del sampleDf[\"CENTER\"]\n    # Remove this when these columns are removed from both databases\n    if patientDf.get(\"BIRTH_YEAR_NUMERICAL\") is not None:\n        del patientDf[\"BIRTH_YEAR_NUMERICAL\"]\n    # del patientDf['BIRTH_YEAR_NUMERICAL']\n\n    totalSample = [\"PATIENT_ID\"]\n    totalSample.extend(sampleCols)\n    sampleCols = totalSample\n    # Make sure to only grab samples that have patient information\n    sampleDf = sampleDf[sampleDf[\"PATIENT_ID\"].isin(patientDf[\"PATIENT_ID\"])]\n    clinicalDf = sampleDf.merge(patientDf, on=\"PATIENT_ID\", how=\"outer\")\n    # Remove patients without any sample or patient ids\n    clinicalDf = clinicalDf[~clinicalDf[\"SAMPLE_ID\"].isnull()]\n    clinicalDf = clinicalDf[~clinicalDf[\"PATIENT_ID\"].isnull()]\n\n    (\n        remove_mafInBed_variants,\n        removeForMergedConsortiumSamples,\n        removeForCenterConsortiumSamples,\n        flagged_mutationInCis_variants,\n    ) = run_genie_filters(\n        syn,\n        genieVersion,\n        variant_filtering_synId,\n        clinicalDf,\n        bedDf,\n        CENTER_MAPPING_DF,\n        processingDate,\n        skipMutationsInCis,\n        consortiumReleaseCutOff,\n        test,\n        current_release_staging=current_release_staging,\n    )\n\n    (\n        clinicalDf,\n        keepForCenterConsortiumSamples,\n        keepForMergedConsortiumSamples,\n    ) = store_clinical_files(\n        syn,\n        genieVersion,\n        clinicalDf,\n        oncotree_url,\n        sampleCols,\n        patientCols,\n        removeForCenterConsortiumSamples,\n        removeForMergedConsortiumSamples,\n        consortiumReleaseSynId,\n        current_release_staging,\n        CENTER_MAPPING_DF,\n        databaseSynIdMappingDf,\n        used=[sample_used, patient_used],\n    )\n\n    assert not clinicalDf[\"SAMPLE_ID\"].duplicated().any()\n\n    store_maf_files(\n        syn,\n        genieVersion,\n        centerMafFileViewSynId,\n        consortiumReleaseSynId,\n        clinicalDf[[\"SAMPLE_ID\", \"CENTER\"]],\n        CENTER_MAPPING_DF,\n        keepForMergedConsortiumSamples,\n        keepForCenterConsortiumSamples,\n        remove_mafInBed_variants,\n        flagged_mutationInCis_variants,\n        current_release_staging,\n    )\n\n    cnaSamples = store_cna_files(\n        syn,\n        centerMafFileViewSynId,\n        keepForCenterConsortiumSamples,\n        keepForMergedConsortiumSamples,\n        CENTER_MAPPING_DF,\n        genieVersion,\n        consortiumReleaseSynId,\n        current_release_staging,\n    )\n\n    wes_panelids = store_assay_info_files(\n        syn, genieVersion, assay_info_synid, clinicalDf, consortiumReleaseSynId\n    )\n\n    svSamples = store_sv_files(\n        syn,\n        consortiumReleaseSynId,\n        genieVersion,\n        sv_synid,\n        keepForCenterConsortiumSamples,\n        keepForMergedConsortiumSamples,\n        current_release_staging,\n        CENTER_MAPPING_DF,\n    )\n\n    data_gene_matrix = store_data_gene_matrix(\n        syn,\n        genieVersion,\n        clinicalDf,\n        cnaSamples,\n        consortiumReleaseSynId,\n        wes_panelids,\n        svSamples,\n    )\n\n    genePanelEntities = store_gene_panel_files(\n        syn,\n        fileviewSynId,\n        genieVersion,\n        data_gene_matrix,\n        consortiumReleaseSynId,\n        wes_panelids,\n    )\n\n    store_seg_files(\n        syn,\n        genieVersion,\n        segSynId,\n        consortiumReleaseSynId,\n        keepForCenterConsortiumSamples,\n        keepForMergedConsortiumSamples,\n        CENTER_MAPPING_DF,\n        current_release_staging,\n    )\n\n    store_bed_files(\n        syn,\n        genieVersion,\n        bedDf,\n        clinicalDf[\"SEQ_ASSAY_ID\"].unique(),\n        CENTER_MAPPING_DF,\n        current_release_staging,\n        consortiumReleaseSynId,\n        used=bed_used,\n    )\n\n    return genePanelEntities\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.revise_metadata_files","title":"<code>revise_metadata_files(syn, consortiumid, genie_version=None)</code>","text":"<p>Rewrite metadata files with the correct GENIE version</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>consortiumid</code> <p>Synapse id of consortium release folder</p> <p> </p> <code>genie_version</code> <p>GENIE version, Default to None</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def revise_metadata_files(syn, consortiumid, genie_version=None):\n    \"\"\"\n    Rewrite metadata files with the correct GENIE version\n\n    Args:\n        syn: Synapse object\n        consortiumid: Synapse id of consortium release folder\n        genie_version: GENIE version, Default to None\n    \"\"\"\n    release_files = syn.getChildren(consortiumid)\n    meta_file_ents = [\n        syn.get(\n            i[\"id\"], downloadLocation=GENIE_RELEASE_DIR, ifcollision=\"overwrite.local\"\n        )\n        for i in release_files\n        if \"meta\" in i[\"name\"] and i[\"name\"] != \"meta_fusions.txt\"\n    ]\n\n    for meta_ent in meta_file_ents:\n        with open(meta_ent.path, \"r+\") as meta:\n            meta_text = meta.read()\n            if \"meta_study\" not in meta_ent.path:\n                version = \"\"\n            else:\n                version = re.search(\".+GENIE.+v(.+)\", meta_text).group(1)\n            # Fix this line\n            genie_version = version if genie_version is None else genie_version\n\n            if version != genie_version:\n                meta_text = meta_text.replace(\n                    \"GENIE Cohort v{}\".format(version),\n                    \"GENIE Cohort v{}\".format(genie_version),\n                )\n\n                meta_text = meta_text.replace(\n                    \"GENIE v{}\".format(version), \"GENIE v{}\".format(genie_version)\n                )\n\n                meta.seek(0)\n                meta.write(meta_text)\n                meta.truncate()\n        load.store_file(\n            syn=syn,\n            filepath=meta_ent.path,\n            parentid=consortiumid,\n            version_comment=genie_version,\n        )\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.search_or_create_folder","title":"<code>search_or_create_folder(syn, parentid, folder_name)</code>","text":"<p>Searches for an existing Synapse Folder given a parent id and creates the Synapse folder if it doesn't exist</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>parentid</code> <p>Synapse Id of a project or folder</p> <p> TYPE: <code>str</code> </p> <code>folder_name</code> <p>Folde rname</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Synapse Folder id</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def search_or_create_folder(\n    syn: synapseclient.Synapse, parentid: str, folder_name: str\n) -&gt; str:\n    \"\"\"\n    Searches for an existing Synapse Folder given a parent id\n    and creates the Synapse folder if it doesn't exist\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        parentid (str): Synapse Id of a project or folder\n        folder_name (str): Folde rname\n\n    Returns:\n        str: Synapse Folder id\n    \"\"\"\n    folder_id = syn.findEntityId(name=folder_name, parent=parentid)\n    # if case_lists doesn't exist\n    if folder_id is None:\n        folder_ent = synapseclient.Folder(name=folder_name, parent=parentid)\n        folder_id = syn.store(folder_ent).id\n    return folder_id\n</code></pre>"},{"location":"reference/main_pipeline_steps/database_to_staging/#genie.database_to_staging.create_link_version","title":"<code>create_link_version(syn, genie_version, case_list_entities, gene_panel_entities, database_synid_mappingdf, release_type='consortium')</code>","text":"<p>Create release links from the actual entity and version</p> <p>TODO: Refactor to use fileviews</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>genie_version</code> <p>GENIE version number</p> <p> </p> <code>case_list_entities</code> <p>Case list entities</p> <p> </p> <code>gene_panel_entities</code> <p>Gene panel entities</p> <p> </p> <code>database_synid_mappingdf</code> <p>dataframe containing database to                       synapse id mapping</p> <p> </p> <code>release_type</code> <p>'consortium' or 'public' release</p> <p> DEFAULT: <code>'consortium'</code> </p> Source code in <code>genie/database_to_staging.py</code> <pre><code>def create_link_version(\n    syn,\n    genie_version,\n    case_list_entities,\n    gene_panel_entities,\n    database_synid_mappingdf,\n    release_type=\"consortium\",\n):\n    \"\"\"\n    Create release links from the actual entity and version\n\n    TODO: Refactor to use fileviews\n\n    Args:\n        syn: Synapse object\n        genie_version: GENIE version number\n        case_list_entities: Case list entities\n        gene_panel_entities: Gene panel entities\n        database_synid_mappingdf: dataframe containing database to\n                                  synapse id mapping\n        release_type: 'consortium' or 'public' release\n    \"\"\"\n    # Grab major release numbers (ie 1,2,3 ...)\n    major_release = genie_version.split(\".\")[0]\n    all_releases_synid = database_synid_mappingdf[\"Id\"][\n        database_synid_mappingdf[\"Database\"] == \"release\"\n    ].values[0]\n    # Create major release folder\n    major_release_folder_synid = search_or_create_folder(\n        syn, all_releases_synid, f\"Release {major_release}\"\n    )\n    # If the major release folder didn't exist, go ahead and create the\n    # release folder\n    release_folder_synid = search_or_create_folder(\n        syn, major_release_folder_synid, genie_version\n    )\n    # Search or create case lists folder\n    caselist_folder_synid = search_or_create_folder(\n        syn, release_folder_synid, \"case_lists\"\n    )\n\n    # caselistId = findCaseListId(syn, release_folder_synid)\n    consortium_synid = database_synid_mappingdf[\"Id\"][\n        database_synid_mappingdf[\"Database\"] == release_type\n    ].values[0]\n    consortium_release_files = syn.getChildren(consortium_synid)\n\n    for release_file in consortium_release_files:\n        not_folder = release_file[\"type\"] != \"org.sagebionetworks.repo.model.Folder\"\n        # data_clinical.txt MUST be pulled in when doing consortium release\n        not_public = (\n            release_file[\"name\"] != \"data_clinical.txt\" or release_type == \"consortium\"\n        )\n        is_gene_panel = release_file[\"name\"].startswith(\"data_gene_panel\")\n        is_deprecated_file = release_file[\"name\"] in [\"data_fusions.txt\"]\n\n        if not_folder and not_public and not is_gene_panel and not is_deprecated_file:\n            syn.store(\n                synapseclient.Link(\n                    release_file[\"id\"],\n                    parent=release_folder_synid,\n                    targetVersion=release_file[\"versionNumber\"],\n                )\n            )\n\n    release_files = syn.getChildren(release_folder_synid)\n    clinical_ent = [\n        ents[\"id\"] for ents in release_files if ents[\"name\"] == \"data_clinical.txt\"\n    ]\n    if clinical_ent:\n        # Set private permission for the data_clinical.txt link\n        syn.setPermissions(clinical_ent[0], principalId=3326313, accessType=[])\n\n    for ents in case_list_entities:\n        syn.store(\n            synapseclient.Link(\n                ents.id, parent=caselist_folder_synid, targetVersion=ents.versionNumber\n            )\n        )\n\n    # Store gene panels\n    for ents in gene_panel_entities:\n        syn.store(\n            synapseclient.Link(\n                ents.id, parent=release_folder_synid, targetVersion=ents.versionNumber\n            )\n        )\n\n    return {\n        \"release_folder\": release_folder_synid,\n        \"caselist_folder\": caselist_folder_synid,\n    }\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/","title":"input_to_database","text":""},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database","title":"<code>genie.input_to_database</code>","text":""},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.DUPLICATED_FILE_ERROR","title":"<code>DUPLICATED_FILE_ERROR = 'Duplicated filename! Files should be uploaded as new versions and the entire dataset should be uploaded.'</code>  <code>module-attribute</code>","text":"<p>TODO: Could potentially get all the inforamation of the file entity right here To avoid the syn.get rest call later which doesn't actually download the file</p>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.entity_date_to_unix_epoch_time","title":"<code>entity_date_to_unix_epoch_time(entity_date_time)</code>","text":"<p>Convert Synapse object date/time string (from modifiedOn or createdOn properties) to UNIX time</p> PARAMETER DESCRIPTION <code>entity_date_time</code> <p>Synapse object date/time string in this format: 2018-10-25T20:16:07.959Z</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>int</code> <p>unix epoch time</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def entity_date_to_unix_epoch_time(entity_date_time: str):\n    \"\"\"Convert Synapse object date/time string (from modifiedOn or createdOn properties) to UNIX time\n\n    Args:\n        entity_date_time: Synapse object date/time string in this format:\n            2018-10-25T20:16:07.959Z\n\n    Returns:\n        int: unix epoch time\n    \"\"\"\n    date_and_time = entity_date_time.split(\".\")[0]\n    date_time_obj = datetime.datetime.strptime(date_and_time, \"%Y-%m-%dT%H:%M:%S\")\n    return process_functions.to_unix_epoch_time_utc(date_time_obj)\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.check_existing_file_status","title":"<code>check_existing_file_status(validation_status_table, error_tracker_table, entities)</code>","text":"<p>This function checks input files against the existing validation and error tracking dataframe</p> PARAMETER DESCRIPTION <code>validation_status_table</code> <p>Validation status Synapse Table query result</p> <p> </p> <code>error_tracker_table</code> <p>Error tracking Synapse Table query result</p> <p> </p> <code>entities</code> <p>list of center input entites</p> <p> </p> RETURNS DESCRIPTION <code>dict</code> <p>Input file status status_list: file validation status error_list: Errors of the files if they exist, to_validate: Boolean value for whether of not an input              file needs to be validated</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def check_existing_file_status(validation_status_table, error_tracker_table, entities):\n    \"\"\"\n    This function checks input files against the existing validation and error\n    tracking dataframe\n\n    Args:\n        validation_status_table: Validation status Synapse Table query result\n        error_tracker_table: Error tracking Synapse Table query result\n        entities: list of center input entites\n\n    Returns:\n        dict: Input file status\n            status_list: file validation status\n            error_list: Errors of the files if they exist,\n            to_validate: Boolean value for whether of not an input\n                         file needs to be validated\n    \"\"\"\n    if len(entities) &gt; 2:\n        raise ValueError(\"There should never be more than 2 files being validated.\")\n\n    statuses = []\n    errors = []\n\n    validation_statusdf = validation_status_table.asDataFrame()\n    error_trackerdf = error_tracker_table.asDataFrame()\n    # This should be outside fo the forloop so that it doesn't\n    # get reset\n    to_validate = False\n    for ent in entities:\n        # Get the current status and errors from the tables.\n        current_status = validation_statusdf[validation_statusdf[\"id\"] == ent.id]\n        current_error = error_trackerdf[error_trackerdf[\"id\"] == ent.id]\n\n        if current_status.empty:\n            to_validate = True\n        else:\n            # This to_validate is here, because the following is a\n            # sequential check of whether files need to be validated\n            statuses.append(current_status[\"status\"].values[0])\n            if current_error.empty:\n                to_validate = current_status[\"status\"].values[0] == \"INVALID\"\n            else:\n                errors.append(current_error[\"errors\"].values[0])\n            # Add Name check here (must add name of the entity as a column)\n            if (\n                current_status[\"md5\"].values[0] != ent.md5\n                or current_status[\"name\"].values[0] != ent.name\n            ):\n                to_validate = True\n            else:\n                status_str = \"{filename} ({id}) FILE STATUS IS: {filestatus}\"\n                logger.info(\n                    status_str.format(\n                        filename=ent.name,\n                        id=ent.id,\n                        filestatus=current_status[\"status\"].values[0],\n                    )\n                )\n\n    return {\"status_list\": statuses, \"error_list\": errors, \"to_validate\": to_validate}\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database._send_validation_error_email","title":"<code>_send_validation_error_email(syn, user, message_objs)</code>","text":"<p>Sends validation error email</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>user</code> <p>username to send message to</p> <p> </p> <code>message_objs</code> <p>list of dicts with 'filenames' and 'messages' to send</p> <p> </p> Source code in <code>genie/input_to_database.py</code> <pre><code>def _send_validation_error_email(syn, user, message_objs):\n    \"\"\"\n    Sends validation error email\n\n    Args:\n        syn: Synapse object\n        user: username to send message to\n        message_objs: list of dicts with 'filenames' and 'messages' to send\n    \"\"\"\n\n    username = syn.getUserProfile(user)[\"userName\"]\n\n    errors = \"\"\n    for message_obj in message_objs:\n        file_names = \", \".join(message_obj[\"filenames\"])\n        error_message = message_obj[\"messages\"]\n        errors += f\"Filenames: {file_names}, Errors:\\n {error_message}\\n\\n\"\n\n    email_message = (\n        f\"Dear {username},\\n\\n\"\n        \"You have invalid files! \"\n        f\"Here are the reasons why:\\n\\n{errors}\"\n    )\n\n    date_now = datetime.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    syn.sendMessage(\n        userIds=[user],\n        messageSubject=f\"GENIE Validation Error - {date_now}\",\n        messageBody=email_message,\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database._get_status_and_error_list","title":"<code>_get_status_and_error_list(valid, message, entities)</code>","text":"<p>Helper function to return the status and error list of the files based on validation result.</p> PARAMETER DESCRIPTION <code>valid</code> <p>Boolean value of results of validation</p> <p> </p> <code>message</code> <p>Validation message</p> <p> </p> <code>entities</code> <p>List of Synapse Entities</p> <p> </p> RETURNS DESCRIPTION <code>tuple</code> <p>input_status_list - status of input files list,    invalid_errors_list - error list</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def _get_status_and_error_list(valid, message, entities):\n    \"\"\"\n    Helper function to return the status and error list of the\n    files based on validation result.\n\n    Args:\n        valid: Boolean value of results of validation\n        message: Validation message\n        entities: List of Synapse Entities\n\n    Returns:\n        tuple: input_status_list - status of input files list,\n               invalid_errors_list - error list\n    \"\"\"\n    if valid:\n        input_status_list = [{\"entity\": ent, \"status\": \"VALIDATED\"} for ent in entities]\n        invalid_errors_list = []\n    else:\n        input_status_list = [{\"entity\": ent, \"status\": \"INVALID\"} for ent in entities]\n        invalid_errors_list = [{\"entity\": ent, \"errors\": message} for ent in entities]\n    return input_status_list, invalid_errors_list\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.validatefile","title":"<code>validatefile(syn, project_id, entities, validation_status_table, error_tracker_table, center, format_registry=None, genie_config=None, ancillary_files=None)</code>","text":"<p>Validate a list of entities.</p> <p>If a file has not changed, then it doesn't need to be validated.</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> TYPE: <code>Synapse</code> </p> <code>project_id</code> <p>GENIE Synapse project id</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>A list of entities for a single file 'type' (usually a single file, but clinical can have two)</p> <p> TYPE: <code>List[File]</code> </p> <code>validation_status_table</code> <p>Validation status dataframe</p> <p> TYPE: <code>CsvFileTable</code> </p> <code>error_tracker_table</code> <p>Invalid files error tracking dataframe</p> <p> TYPE: <code>CsvFileTable</code> </p> <code>center</code> <p>Center of interest</p> <p> TYPE: <code>str</code> </p> <code>format_registry</code> <p>GENIE file format registry.                                      Defaults to None.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>genie_config</code> <p>See example of genie config at                                   ./genie_config.json. Defaults to None.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>ancillary_files</code> <p>all files downloaded for validation</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>input_status_list - status of input files,    invalid_errors_list - error list    messages_to_send - list of tuples with (filenames, message, file_users)</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def validatefile(\n    syn: synapseclient.Synapse,\n    project_id: str,\n    entities: List[synapseclient.File],\n    validation_status_table: synapseclient.table.CsvFileTable,\n    error_tracker_table: synapseclient.table.CsvFileTable,\n    center: str,\n    format_registry: Optional[dict] = None,\n    genie_config: Optional[dict] = None,\n    ancillary_files: Optional[list] = None,\n):\n    \"\"\"Validate a list of entities.\n\n    If a file has not changed, then it doesn't need to be validated.\n\n    Args:\n        syn: Synapse object\n        project_id (str): GENIE Synapse project id\n        entities: A list of entities for a single file 'type' (usually a single file, but clinical can have two)\n        validation_status_table: Validation status dataframe\n        error_tracker_table: Invalid files error tracking dataframe\n        center: Center of interest\n        format_registry (list, optional): GENIE file format registry.\n                                                 Defaults to None.\n        genie_config (list, optional): See example of genie config at\n                                              ./genie_config.json. Defaults to None.\n        ancillary_files (list): all files downloaded for validation\n\n    Returns:\n        tuple: input_status_list - status of input files,\n               invalid_errors_list - error list\n               messages_to_send - list of tuples with (filenames, message, file_users)\n\n    \"\"\"\n    # TODO: Look into if errors should be thrown if these are None\n    # Aka. should these actually be optional params\n    if genie_config is None:\n        genie_config = {}\n    if format_registry is None:\n        format_registry = {}\n\n    # filepaths = [entity.path for entity in entities]\n    filenames = [entity.name for entity in entities]\n\n    logger.info(\"VALIDATING {filenames}\".format(filenames=\", \".join(filenames)))\n\n    file_users = [entities[0].modifiedBy, entities[0].createdBy]\n\n    check_file_status = check_existing_file_status(\n        validation_status_table, error_tracker_table, entities\n    )\n\n    status_list = check_file_status[\"status_list\"]\n    error_list = check_file_status[\"error_list\"]\n\n    messages_to_send = []\n    # Need to figure out to how to remove this\n    # This must pass in filenames, because filetype is determined by entity\n    # name not by actual path of file\n    validator = validate.GenieValidationHelper(\n        syn=syn,\n        project_id=project_id,\n        center=center,\n        entitylist=entities,\n        format_registry=format_registry,\n        genie_config=genie_config,\n        ancillary_files=ancillary_files,\n    )\n    filetype = validator.file_type\n    if check_file_status[\"to_validate\"]:\n        # HACK: Don't download again if only_validate is not True, but all\n        # files need to be downloaded currently when validation + processing\n        # isn't split up\n        # if entities[0].get(\"path\") is None:\n        #    validator.entitylist = [syn.get(entity) for entity in entities]\n\n        valid_cls, message = validator.validate_single_file(\n            oncotree_link=genie_config[\"oncotreeLink\"], nosymbol_check=False\n        )\n\n        logger.info(\"VALIDATION COMPLETE\")\n        input_status_list, invalid_errors_list = _get_status_and_error_list(\n            valid_cls.is_valid(), message, entities\n        )\n        # Send email the first time the file is invalid\n        if invalid_errors_list:\n            messages_to_send.append((filenames, message, file_users))\n    else:\n        input_status_list = [\n            {\"entity\": entity, \"status\": status}\n            for entity, status in zip(entities, status_list)\n        ]\n        invalid_errors_list = [\n            {\"entity\": entity, \"errors\": errors}\n            for entity, errors in zip(entities, error_list)\n        ]\n    # add in static filetype and center information\n    for input_status in input_status_list:\n        input_status.update({\"fileType\": filetype, \"center\": center})\n    # An empty list is returned if there are no errors,\n    # so nothing will be appended\n    for invalid_errors in invalid_errors_list:\n        invalid_errors.update({\"fileType\": filetype, \"center\": center})\n    return input_status_list, invalid_errors_list, messages_to_send\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.processfiles","title":"<code>processfiles(syn, validfiles, center, path_to_genie, processing='main', format_registry=None, genie_config=None)</code>","text":"<p>Processing validated files</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>validfiles</code> <p>pandas dataframe containing validated files         has 'id', 'path', and 'fileType' column</p> <p> </p> <code>center</code> <p>GENIE center name</p> <p> </p> <code>path_to_genie</code> <p>Path to GENIE workdir</p> <p> </p> <code>center_mapping_df</code> <p>Center mapping dataframe</p> <p> </p> <code>oncotree_link</code> <p>Link to oncotree</p> <p> </p> <code>databaseToSynIdMappingDf</code> <p>Database to synapse id mapping dataframe</p> <p> </p> <code>processing</code> <p>Processing type. Defaults to main</p> <p> DEFAULT: <code>'main'</code> </p> Source code in <code>genie/input_to_database.py</code> <pre><code>def processfiles(\n    syn,\n    validfiles,\n    center,\n    path_to_genie,\n    processing=\"main\",\n    format_registry=None,\n    genie_config=None,\n):\n    \"\"\"Processing validated files\n\n    Args:\n        syn: Synapse object\n        validfiles: pandas dataframe containing validated files\n                    has 'id', 'path', and 'fileType' column\n        center: GENIE center name\n        path_to_genie: Path to GENIE workdir\n        center_mapping_df: Center mapping dataframe\n        oncotree_link: Link to oncotree\n        databaseToSynIdMappingDf: Database to synapse id mapping dataframe\n        processing: Processing type. Defaults to main\n    \"\"\"\n    logger.info(f\"PROCESSING {center} FILES: {len(validfiles)}\")\n    center_staging_folder = os.path.join(path_to_genie, center)\n    center_staging_synid = genie_config[\"center_config\"][center][\"stagingSynId\"]\n\n    if not os.path.exists(center_staging_folder):\n        os.makedirs(center_staging_folder)\n\n    if processing == \"main\":\n        for _, row in validfiles.iterrows():\n            filetype = row[\"fileType\"]\n            # filename = os.path.basename(filePath)\n            newpath = os.path.join(center_staging_folder, row[\"name\"])\n            # store = True\n            # Table id can be None\n            tableid = genie_config.get(filetype)\n\n            if filetype is not None and filetype != \"other\":\n                # Example GENIE config can be found in tests/conftest.py\n                processor = format_registry[filetype](\n                    syn=syn, center=center, genie_config=genie_config\n                )\n                processor.process(\n                    filePath=row[\"path\"],\n                    newPath=newpath,\n                    parentId=center_staging_synid,\n                    databaseSynId=tableid,\n                    fileSynId=row[\"id\"],\n                )\n    else:\n        process_mutation.process_mutation_workflow(\n            syn=syn,\n            center=center,\n            validfiles=validfiles,\n            genie_config=genie_config,\n            workdir=path_to_genie,\n        )\n\n    logger.info(\"ALL DATA STORED IN DATABASE\")\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.append_duplication_errors","title":"<code>append_duplication_errors(duplicated_filesdf, user_message_dict)</code>","text":"<p>Duplicated files can occur because centers can upload files with the same filename in different folders.  This is to append duplication errors to the list of errors to email</p> PARAMETER DESCRIPTION <code>duplicated_filesdf</code> <p>Dataframe of duplciated files</p> <p> </p> <code>user_message_dict</code> <p>Dictionary containing list of error messages to                send to each user.</p> <p> </p> RETURNS DESCRIPTION <p>Dictionary containing list of error messages to send to each user.</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def append_duplication_errors(duplicated_filesdf, user_message_dict):\n    \"\"\"Duplicated files can occur because centers can upload files with the\n    same filename in different folders.  This is to append duplication\n    errors to the list of errors to email\n\n    Args:\n        duplicated_filesdf: Dataframe of duplciated files\n        user_message_dict: Dictionary containing list of error messages to\n                           send to each user.\n\n    Returns:\n        Dictionary containing list of error messages to send to each user.\n    \"\"\"\n    duplication_error = (\n        \"Duplicated filename! Files should be uploaded as new versions \"\n        \"and the entire dataset should be uploaded.\"\n    )\n    if not duplicated_filesdf.empty:\n        filenames = []\n        users = []\n        for entity in duplicated_filesdf[\"entity\"]:\n            users.append(entity.modifiedBy)\n            users.append(entity.createdBy)\n            filenames.append(entity.name)\n        file_messages = dict(filenames=filenames, messages=duplication_error)\n        # Must get unique set of users or there\n        # will be duplicated error messages sent in the email\n        for user in set(users):\n            user_message_dict[user].append(file_messages)\n    return user_message_dict\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.get_duplicated_files","title":"<code>get_duplicated_files(validation_statusdf)</code>","text":"<p>Check for duplicated files.  There should be no duplication, files should be uploaded as new versions and the entire dataset should be uploaded everytime</p>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.get_duplicated_files--todo-this-is-a-custom-genie-function","title":"TODO: This is a custom GENIE function","text":"PARAMETER DESCRIPTION <code>validation_statusdf</code> <p>dataframe with 'name' and 'id' column</p> <p> </p> <code>duplicated_error_message</code> <p>Error message for duplicated files</p> <p> </p> RETURNS DESCRIPTION <p>dataframe with 'id', 'name', 'errors', 'center', 'fileType'</p> <p>and 'entity' of duplicated files</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def get_duplicated_files(validation_statusdf):\n    \"\"\"\n    Check for duplicated files.  There should be no duplication,\n    files should be uploaded as new versions and the entire dataset\n    should be uploaded everytime\n\n    #TODO: This is a custom GENIE function\n\n    Args:\n        validation_statusdf: dataframe with 'name' and 'id' column\n        duplicated_error_message: Error message for duplicated files\n\n    Returns:\n        dataframe with 'id', 'name', 'errors', 'center', 'fileType'\n        and 'entity' of duplicated files\n    \"\"\"\n    # This is special\n    logger.info(\"CHECK FOR DUPLICATED FILES\")\n    duplicated_filesdf = validation_statusdf[\n        validation_statusdf[\"name\"].duplicated(keep=False)\n    ]\n    # Define filename str vector\n    filename_str = validation_statusdf.name.str\n    # cbs/seg files should not be duplicated.\n    cbs_seg_index = filename_str.endswith((\"cbs\", \"seg\"))\n    cbs_seg_files = validation_statusdf[cbs_seg_index]\n    if len(cbs_seg_files) &gt; 1:\n        duplicated_filesdf = pd.concat([duplicated_filesdf, cbs_seg_files])\n    # clinical files should not be duplicated.\n    clinical_index = filename_str.startswith(\"data_clinical_supp\")\n    clinical_files = validation_statusdf[clinical_index]\n    if len(clinical_files) &gt; 2:\n        duplicated_filesdf = pd.concat([duplicated_filesdf, clinical_files])\n    duplicated_filesdf.drop_duplicates(\"id\", inplace=True)\n    logger.info(\"THERE ARE {} DUPLICATED FILES\".format(len(duplicated_filesdf)))\n    duplicated_filesdf[\"errors\"] = DUPLICATED_FILE_ERROR\n    return duplicated_filesdf\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.build_validation_status_table","title":"<code>build_validation_status_table(input_valid_statuses)</code>","text":"<p>Build validation status dataframe</p> PARAMETER DESCRIPTION <code>input_valid_statuses</code> <p>list of file validation status</p> <p> TYPE: <code>List[dict]</code> </p> RETURNS DESCRIPTION <p>Validation status dataframe</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def build_validation_status_table(input_valid_statuses: List[dict]):\n    \"\"\"Build validation status dataframe\n\n    Args:\n        input_valid_statuses: list of file validation status\n\n    Returns:\n        Validation status dataframe\n\n    \"\"\"\n    status_table_columns = [\n        \"id\",\n        \"path\",\n        \"md5\",\n        \"status\",\n        \"name\",\n        \"modifiedOn\",\n        \"fileType\",\n        \"center\",\n        \"version\",\n        \"entity\",\n    ]\n    input_status_rows = []\n    for input_status in input_valid_statuses:\n        entity = input_status[\"entity\"]\n        row = {\n            \"id\": entity.id,\n            \"path\": entity.path,\n            \"md5\": entity.md5,\n            \"status\": input_status[\"status\"],\n            \"name\": entity.name,\n            \"modifiedOn\": entity_date_to_unix_epoch_time(entity.properties.modifiedOn),\n            \"fileType\": input_status[\"fileType\"],\n            \"center\": input_status[\"center\"],\n            \"version\": entity.versionNumber,\n            \"entity\": entity,\n        }\n        input_status_rows.append(row)\n    if input_status_rows:\n        input_valid_statusdf = pd.DataFrame(input_status_rows)\n    else:\n        input_valid_statusdf = pd.DataFrame(\n            input_status_rows, columns=status_table_columns\n        )\n    return input_valid_statusdf\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.build_error_tracking_table","title":"<code>build_error_tracking_table(invalid_errors)</code>","text":"<p>Build error tracking dataframe</p> PARAMETER DESCRIPTION <code>invalid_errors</code> <p>list of file invalid errors</p> <p> TYPE: <code>List[dict]</code> </p> RETURNS DESCRIPTION <p>Error tracking dataframe</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def build_error_tracking_table(invalid_errors: List[dict]):\n    \"\"\"Build error tracking dataframe\n\n    Args:\n        invalid_errors: list of file invalid errors\n\n    Returns:\n        Error tracking dataframe\n\n    \"\"\"\n    error_table_columns = [\n        \"id\",\n        \"errors\",\n        \"name\",\n        \"fileType\",\n        \"center\",\n        \"version\",\n        \"entity\",\n    ]\n    invalid_error_rows = []\n    for invalid_error in invalid_errors:\n        entity = invalid_error[\"entity\"]\n        row = {\n            \"id\": entity.id,\n            \"errors\": invalid_error[\"errors\"],\n            \"name\": entity.name,\n            \"fileType\": invalid_error[\"fileType\"],\n            \"center\": invalid_error[\"center\"],\n            \"version\": entity.versionNumber,\n            \"entity\": entity,\n        }\n        invalid_error_rows.append(row)\n    if invalid_error_rows:\n        invalid_errorsdf = pd.DataFrame(invalid_error_rows)\n    else:\n        invalid_errorsdf = pd.DataFrame(invalid_error_rows, columns=error_table_columns)\n    return invalid_errorsdf\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.update_status_and_error_tables","title":"<code>update_status_and_error_tables(syn, input_valid_statusdf, invalid_errorsdf, validation_status_table, error_tracker_table)</code>","text":"<p>Update validation status and error tracking table</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>center</code> <p>Center</p> <p> </p> <code>input_valid_status</code> <p>list of lists of validation status</p> <p> </p> <code>invalid_errors</code> <p>List of lists of invalid errors</p> <p> </p> <code>validation_status_table</code> <p>Synapse table query of validation status</p> <p> </p> <code>error_tracker_table</code> <p>Synapse table query of error tracker</p> <p> </p> Source code in <code>genie/input_to_database.py</code> <pre><code>def update_status_and_error_tables(\n    syn,\n    input_valid_statusdf,\n    invalid_errorsdf,\n    validation_status_table,\n    error_tracker_table,\n):\n    \"\"\"\n    Update validation status and error tracking table\n\n    Args:\n        syn: Synapse object\n        center: Center\n        input_valid_status: list of lists of validation status\n        invalid_errors: List of lists of invalid errors\n        validation_status_table: Synapse table query of validation status\n        error_tracker_table: Synapse table query of error tracker\n\n    \"\"\"\n    logger.info(\"UPDATE VALIDATION STATUS DATABASE\")\n\n    load._update_table(\n        syn,\n        error_tracker_table.asDataFrame(),\n        invalid_errorsdf,\n        error_tracker_table.tableId,\n        [\"id\"],\n        to_delete=True,\n    )\n\n    load._update_table(\n        syn,\n        validation_status_table.asDataFrame(),\n        input_valid_statusdf,\n        validation_status_table.tableId,\n        [\"id\"],\n        to_delete=True,\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database._update_tables_content","title":"<code>_update_tables_content(validation_statusdf, error_trackingdf)</code>","text":"<p>Update validation status and error tracking dataframes with duplicated files.  Also update the error table to only contain errors - centers may have fixed their files so will want to remove old errors.</p> PARAMETER DESCRIPTION <code>validation_statusdf</code> <p>Validation status dataframe</p> <p> </p> <code>error_trackingdf</code> <p>Error tracking dataframe</p> <p> </p> RETURNS DESCRIPTION <code>dict</code> <p>validation_statusdf: Updated validation status dataframe   error_trackingdf: Updated error tracking dataframe   duplicated_filesdf:  Duplicated files dataframe</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def _update_tables_content(validation_statusdf, error_trackingdf):\n    \"\"\"Update validation status and error tracking dataframes with duplicated\n    files.  Also update the error table to only contain errors - centers\n    may have fixed their files so will want to remove old errors.\n\n    Args:\n        validation_statusdf: Validation status dataframe\n        error_trackingdf: Error tracking dataframe\n\n    Returns:\n        dict: validation_statusdf: Updated validation status dataframe\n              error_trackingdf: Updated error tracking dataframe\n              duplicated_filesdf:  Duplicated files dataframe\n\n    \"\"\"\n    # Get duplicated files\n    duplicated_filesdf = get_duplicated_files(validation_statusdf)\n    # index of all duplicated files\n    duplicated_idx = validation_statusdf[\"id\"].isin(duplicated_filesdf[\"id\"])\n    validation_statusdf[\"status\"][duplicated_idx] = \"INVALID\"\n    duplicated_idx = error_trackingdf[\"id\"].isin(duplicated_filesdf[\"id\"])\n    error_trackingdf[\"errors\"][duplicated_idx] = DUPLICATED_FILE_ERROR\n\n    # Old errors are pulled down in validation, so obtain list of\n    # files with duplicated file errors\n    dup_ids = error_trackingdf[\"id\"][\n        error_trackingdf[\"errors\"] == DUPLICATED_FILE_ERROR\n    ]\n    # Checks to see if the old duplicated files are still duplicated\n    remove_ids = dup_ids[~dup_ids.isin(duplicated_filesdf[\"id\"])]\n\n    # Remove fixed duplicated files\n    error_trackingdf = error_trackingdf[~error_trackingdf[\"id\"].isin(remove_ids)]\n    validation_statusdf = validation_statusdf[\n        ~validation_statusdf[\"id\"].isin(remove_ids)\n    ]\n\n    # Append duplicated file errors\n    duplicated_filesdf[\"id\"].isin(error_trackingdf[\"id\"][duplicated_idx])\n    error_trackingdf = pd.concat(\n        [error_trackingdf, duplicated_filesdf[error_trackingdf.columns]]\n    )\n    # Remove duplicates if theres already an error that exists for the file\n    error_trackingdf.drop_duplicates(\"id\", inplace=True)\n\n    # Since old errors are retained, make sure to only update\n    # files that are actually invalid\n    invalid_ids = validation_statusdf[\"id\"][validation_statusdf[\"status\"] == \"INVALID\"]\n    error_trackingdf = error_trackingdf[error_trackingdf[\"id\"].isin(invalid_ids)]\n    # Fill blank file type values with 'other'\n    error_trackingdf[\"fileType\"].fillna(\"other\", inplace=True)\n    validation_statusdf[\"fileType\"].fillna(\"other\", inplace=True)\n\n    return {\n        \"validation_statusdf\": validation_statusdf,\n        \"error_trackingdf\": error_trackingdf,\n        \"duplicated_filesdf\": duplicated_filesdf,\n    }\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.validation","title":"<code>validation(syn, project_id, center, process, center_files, format_registry, genie_config, ancillary_files=None)</code>","text":"<p>Validation of all center files</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse object</p> <p> </p> <code>center</code> <p>Center name</p> <p> </p> <code>process</code> <p>main, mutation</p> <p> </p> <code>center_files</code> <p> </p> <code>format_registry</code> <p> </p> <code>genie_config</code> <p> </p> <code>ancillary_files</code> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: Dataframe of valid GENIE files</p> Source code in <code>genie/input_to_database.py</code> <pre><code>def validation(\n    syn,\n    project_id,\n    center,\n    process,\n    center_files,\n    format_registry,\n    genie_config,\n    ancillary_files=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Validation of all center files\n\n    Args:\n        syn: Synapse object\n        center: Center name\n        process: main, mutation\n        center_files:\n        format_registry:\n        genie_config:\n        ancillary_files:\n\n    Returns:\n        pd.DataFrame: Dataframe of valid GENIE files\n    \"\"\"\n    logger.info(f\"{center} has uploaded {len(center_files)} files.\")\n    validation_status_synid = genie_config[\"validationStatus\"]\n    error_tracker_synid = genie_config[\"errorTracker\"]\n\n    # Make sure the vcf validation statuses don't get wiped away\n    # If process is not vcf, the vcf files are not downloaded\n    # TODO: Add parameter to exclude types\n    exclude_type = \"vcf\" if process != \"mutation\" else \"\"\n    # id, md5, status, name, center, modifiedOn, fileType\n    validation_status_table = syn.tableQuery(\n        f\"SELECT * FROM {validation_status_synid} where \"\n        f\"center = '{center}' and fileType &lt;&gt; '{exclude_type}'\"\n    )\n    # id, center, errors, name, fileType\n    error_tracker_table = syn.tableQuery(\n        f\"SELECT * FROM {error_tracker_synid} where \"\n        f\"center = '{center}' and fileType &lt;&gt; '{exclude_type}'\"\n    )\n\n    input_valid_statuses = []\n    invalid_errors = []\n\n    # This default dict will capture all the error messages to send to\n    # particular users\n    user_message_dict = defaultdict(list)\n\n    for ents in center_files:\n        status, errors, messages_to_send = validatefile(\n            syn=syn,\n            project_id=project_id,\n            entities=ents,\n            validation_status_table=validation_status_table,\n            error_tracker_table=error_tracker_table,\n            center=center,\n            format_registry=format_registry,\n            genie_config=genie_config,\n            ancillary_files=ancillary_files,\n        )\n\n        input_valid_statuses.extend(status)\n        if errors is not None:\n            invalid_errors.extend(errors)\n\n        if messages_to_send:\n            logger.debug(\"Collating messages to send to users.\")\n            for filenames, messages, users in messages_to_send:\n                file_messages = dict(filenames=filenames, messages=messages)\n                # Must get unique set of users or there\n                # will be duplicated error messages sent in the email\n                for user in set(users):\n                    user_message_dict[user].append(file_messages)\n\n    validation_statusdf = build_validation_status_table(input_valid_statuses)\n\n    error_trackingdf = build_error_tracking_table(invalid_errors)\n\n    new_tables = _update_tables_content(validation_statusdf, error_trackingdf)\n\n    validation_statusdf = new_tables[\"validation_statusdf\"]\n    error_trackingdf = new_tables[\"error_trackingdf\"]\n    duplicated_filesdf = new_tables[\"duplicated_filesdf\"]\n\n    # In GENIE, we not only want to send out file format errors, but\n    # also when there are duplicated errors.  The function below will\n    # append duplication errors as an email to send to users (if applicable)\n    user_message_dict = append_duplication_errors(duplicated_filesdf, user_message_dict)\n\n    for user, message_objs in user_message_dict.items():\n        logger.debug(\"Sending messages to user {user}.\".format(user=user))\n        # HACK: Avoid sending emails for staging project\n        if project_id != \"syn22033066\":\n            _send_validation_error_email(syn=syn, user=user, message_objs=message_objs)\n    # \\n write out new lines when they exist in the middle of a column\n    # So the \\n never gets uploaded into synapse table\n    # change the delimiting to '|'.\n    error_trackingdf[\"errors\"] = [\n        error.replace(\"\\n\", \"|\") for error in error_trackingdf[\"errors\"]\n    ]\n    update_status_and_error_tables(\n        syn=syn,\n        input_valid_statusdf=validation_statusdf,\n        invalid_errorsdf=error_trackingdf,\n        validation_status_table=validation_status_table,\n        error_tracker_table=error_tracker_table,\n    )\n    valid_filesdf = validation_statusdf.query('status == \"VALIDATED\"')\n    return valid_filesdf[[\"id\", \"path\", \"fileType\", \"name\"]]\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/input_to_database/#genie.input_to_database.center_input_to_database","title":"<code>center_input_to_database(syn, project_id, center, process, only_validate, delete_old=False, format_registry=None, genie_config=None)</code>","text":"<p>Processing per center</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>project_id</code> <p>GENIE Synapse project id</p> <p> TYPE: <code>str</code> </p> <code>center</code> <p>GENIE center</p> <p> TYPE: <code>str</code> </p> <code>process</code> <p>main or mutation processing</p> <p> TYPE: <code>str</code> </p> <code>only_validate</code> <p>Only validate or not</p> <p> TYPE: <code>bool</code> </p> <code>delete_old</code> <p>Delete old files. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>format_registry</code> <p>GENIE file format registry.                                      Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>genie_config</code> <p>See example of genie config at                                   ./genie_config.json. Defaults to None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> Source code in <code>genie/input_to_database.py</code> <pre><code>def center_input_to_database(\n    syn: Synapse,\n    project_id: str,\n    center: str,\n    process: str,\n    only_validate: bool,\n    delete_old: bool = False,\n    format_registry: Optional[dict] = None,\n    genie_config: Optional[dict] = None,\n):\n    \"\"\"Processing per center\n\n    Args:\n        syn (Synapse): Synapse connection\n        project_id (str): GENIE Synapse project id\n        center (str): GENIE center\n        process (str): main or mutation processing\n        only_validate (bool): Only validate or not\n        delete_old (bool, optional): Delete old files. Defaults to False.\n        format_registry (dict, optional): GENIE file format registry.\n                                                 Defaults to None.\n        genie_config (dict, optional): See example of genie config at\n                                              ./genie_config.json. Defaults to None.\n    \"\"\"\n    # TODO: Look into if errors should be thrown if these are None\n    # Aka. should these actually be optional params\n    if genie_config is None:\n        genie_config = {}\n    if format_registry is None:\n        format_registry = {}\n\n    if only_validate:\n        log_path = os.path.join(\n            process_functions.SCRIPT_DIR, f\"{center}_validation_log.txt\"\n        )\n    else:\n        log_path = os.path.join(\n            process_functions.SCRIPT_DIR, f\"{center}_{process}_log.txt\"\n        )\n    # Set up logger to write to a log file as well as streaming logs\n    logFormatter = logging.Formatter(\n        \"%(asctime)s [%(name)s][%(levelname)s] %(message)s\"\n    )\n    fileHandler = logging.FileHandler(log_path, mode=\"w\")\n    fileHandler.setFormatter(logFormatter)\n    logger.addHandler(fileHandler)\n\n    # ----------------------------------------\n    # Start processing\n    # ----------------------------------------\n\n    # path_to_genie = os.path.realpath(\n    #   os.path.join(process_functions.SCRIPT_DIR, \"../\")\n    # )\n    # HACK:\n    # Make the synapse cache dir the genie input folder for now\n    # The main reason for this is because the .synaspeCache dir\n    # is mounted by batch\n    path_to_genie = os.path.expanduser(\"~/.synapseCache\")\n    # Create input and staging folders\n    if not os.path.exists(os.path.join(path_to_genie, center, \"input\")):\n        os.makedirs(os.path.join(path_to_genie, center, \"input\"))\n    if not os.path.exists(os.path.join(path_to_genie, center, \"staging\")):\n        os.makedirs(os.path.join(path_to_genie, center, \"staging\"))\n\n    if delete_old:\n        process_functions.rmFiles(os.path.join(path_to_genie, center))\n\n    center_input_synid = genie_config[\"center_config\"][center][\"inputSynId\"]\n    logger.info(\"Center: \" + center)\n    center_files = extract.get_center_input_files(\n        syn=syn,\n        synid=center_input_synid,\n        center=center,\n        process=process,\n        # HACK: Don't download all the files when only validate\n        # downloadFile=(not only_validate),\n    )\n\n    # ancillary_files = get_ancillary_files(\n    #    syn=syn,\n    #    synid=center_input_synid,\n    #    project_id=project_id,\n    #    center=center,\n    #    process=process,\n    #    format_registry=format_registry,\n    #    genie_config=genie_config,\n    # )\n\n    # only validate if there are center files\n    if center_files:\n        validFiles = validation(\n            syn=syn,\n            project_id=project_id,\n            center=center,\n            process=process,\n            center_files=center_files,\n            format_registry=format_registry,\n            genie_config=genie_config,\n            ancillary_files=center_files,\n        )\n    else:\n        logger.info(f\"{center} has not uploaded any files\")\n        return\n\n    if len(validFiles) &gt; 0 and not only_validate:\n        # Reorganize so BED file are always validated and processed first\n        bed_files = validFiles[\"fileType\"] == \"bed\"\n        beds = validFiles[bed_files]\n        validFiles = pd.concat([beds, validFiles])\n        validFiles.drop_duplicates(inplace=True)\n        # merge clinical files into one row\n        clinical_ind = validFiles[\"fileType\"] == \"clinical\"\n        if clinical_ind.any():\n            clinical_files = validFiles[clinical_ind].to_dict(orient=\"list\")\n            # The [] implies the values in the dict as a list\n            merged_clinical = pd.DataFrame([clinical_files])\n            merged_clinical[\"fileType\"] = \"clinical\"\n            merged_clinical[\"name\"] = f\"data_clinical_supp_{center}.txt\"\n            validFiles = pd.concat([validFiles[~clinical_ind], merged_clinical])\n\n        processTrackerSynId = genie_config[\"processTracker\"]\n        # Add process tracker for time start\n        processTrackerDf = extract.get_syntabledf(\n            syn=syn,\n            query_string=(\n                f\"SELECT timeStartProcessing FROM {processTrackerSynId} \"\n                f\"where center = '{center}' and \"\n                f\"processingType = '{process}'\"\n            ),\n        )\n        if processTrackerDf.empty:\n            new_rows = [\n                [\n                    center,\n                    str(int(time.time() * 1000)),\n                    str(int(time.time() * 1000)),\n                    process,\n                ]\n            ]\n\n            syn.store(synapseclient.Table(processTrackerSynId, new_rows))\n        else:\n            processTrackerDf[\"timeStartProcessing\"].iloc[0] = str(\n                int(time.time() * 1000)\n            )\n            syn.store(synapseclient.Table(processTrackerSynId, processTrackerDf))\n\n        # Start transformations\n        processfiles(\n            syn=syn,\n            validfiles=validFiles,\n            center=center,\n            path_to_genie=path_to_genie,\n            processing=process,\n            format_registry=format_registry,\n            genie_config=genie_config,\n        )\n\n        # Should add in this process end tracking before the deletion of samples\n        processTrackerDf = extract.get_syntabledf(\n            syn=syn,\n            query_string=(\n                f\"SELECT timeEndProcessing FROM {processTrackerSynId} \"\n                f\"where center = '{center}' and \"\n                f\"processingType = '{process}'\"\n            ),\n        )\n        processTrackerDf[\"timeEndProcessing\"].iloc[0] = str(int(time.time() * 1000))\n        syn.store(synapseclient.Table(processTrackerSynId, processTrackerDf))\n\n        logger.info(\"SAMPLE/PATIENT RETRACTION\")\n        toRetract.retract(syn, project_id=project_id)\n\n    else:\n        messageOut = (\n            f\"{center} does not have any valid files\"\n            if not only_validate\n            else f\"ONLY VALIDATION OCCURED FOR {center}\"\n        )\n        logger.info(messageOut)\n\n    # Store and remove log file\n    load.store_file(syn=syn, filepath=log_path, parentid=genie_config[\"logs\"])\n    os.remove(log_path)\n    logger.info(\"ALL PROCESSES COMPLETE\")\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/","title":"process_mutation","text":""},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation","title":"<code>genie.process_mutation</code>","text":"<p>Process mutation files TODO deprecate this module and spread functions around</p>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.MAF_COL_MAPPING","title":"<code>MAF_COL_MAPPING = {'HUGO_SYMBOL': 'Hugo_Symbol', 'ENTREZ_GENE_ID': 'Entrez_Gene_Id', 'CENTER': 'Center', 'NCBI_BUILD': 'NCBI_Build', 'CHROMOSOME': 'Chromosome', 'START_POSITION': 'Start_Position', 'END_POSITION': 'End_Position', 'STRAND': 'Strand', 'VARIANT_CLASSIFICATION': 'Variant_Classification', 'VARIANT_TYPE': 'Variant_Type', 'REFERENCE_ALLELE': 'Reference_Allele', 'TUMOR_SEQ_ALLELE1': 'Tumor_Seq_Allele1', 'TUMOR_SEQ_ALLELE2': 'Tumor_Seq_Allele2', 'DBSNP_RS': 'dbSNP_RS', 'DBSNP_VAL_STATUS': 'dbSNP_Val_Status', 'TUMOR_SAMPLE_BARCODE': 'Tumor_Sample_Barcode', 'MATCHED_NORM_SAMPLE_BARCODE': 'Matched_Norm_Sample_Barcode', 'MATCH_NORM_SEQ_ALLELE1': 'Match_Norm_Seq_Allele1', 'MATCH_NORM_SEQ_ALLELE2': 'Match_Norm_Seq_Allele2', 'TUMOR_VALIDATION_ALLELE1': 'Tumor_Validation_Allele1', 'TUMOR_VALIDATION_ALLELE2': 'Tumor_Validation_Allele2', 'MATCH_NORM_VALIDATION_ALLELE1': 'Match_Norm_Validation_Allele1', 'MATCH_NORM_VALIDATION_ALLELE2': 'Match_Norm_Validation_Allele2', 'VERIFICATION_STATUS': 'Verification_Status', 'VALIDATION_STATUS': 'Validation_Status', 'MUTATION_STATUS': 'Mutation_Status', 'SEQUENCING_PHASE': 'Sequencing_Phase', 'SEQUENCE_SOURCE': 'Sequence_Source', 'VALIDATION_METHOD': 'Validation_Method', 'SCORE': 'Score', 'BAM_FILE': 'BAM_File', 'SEQUENCER': 'Sequencer', 'T_REF_COUNT': 't_ref_count', 'T_ALT_COUNT': 't_alt_count', 'N_REF_COUNT': 'n_ref_count', 'N_ALT_COUNT': 'n_alt_count', 'ALLELE': 'Allele', 'AMINO_ACID_CHANGE': 'amino_acid_change', 'AMINO_ACIDS': 'Amino_acids', 'CDS_POSITION': 'CDS_position', 'CODONS': 'Codons', 'CONSEQUENCE': 'Consequence', 'EXISTING_VARIATION': 'Existing_variation', 'EXON_NUMBER': 'Exon_Number', 'FEATURE': 'Feature', 'FEATURE_TYPE': 'Feature_type', 'GENE': 'Gene', 'HGVSC': 'HGVSc', 'HGVSP': 'HGVSp', 'HGVSP_SHORT': 'HGVSp_Short', 'HOTSPOT': 'Hotspot', 'MA:FIMPACT': 'MA:FImpact', 'MA:LINK.MSA': 'MA:link.MSA', 'MA:LINK.PDB': 'MA:link.PDB', 'MA:LINK.VAR': 'MA:link.var', 'MA:PROTEIN.CHANGE': 'MA:protein.change', 'POLYPHEN': 'PolyPhen', 'PROTEIN_POSITION': 'Protein_position', 'REFSEQ': 'RefSeq', 'TRANSCRIPT': 'transcript', 'TRANSCRIPT_ID': 'Transcript_ID', 'ALL_EFFECTS': 'all_effects', 'CDNA_CHANGE': 'cdna_change', 'CDNA_POSITION': 'cDNA_position', 'N_DEPTH': 'n_depth', 'T_DEPTH': 't_depth'}</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.KNOWN_STRING_COLS","title":"<code>KNOWN_STRING_COLS = ['IS_NEW', 'ALLELE_NUM', 'Chromosome', 'CLIN_SIG', 'MOTIF_NAME', 'HIGH_INF_POS', 'MINIMISED', 'CHROMOSOME', 'VERIFICATION_STATUS', 'VALIDATION_STATUS', 'MUTATION_STATUS', 'SEQUENCE_SOURCE', 'SEQUENCER', 'REPORT_AF', 'CDNA_CHANGE', 'AMINO_ACID_CHANGE', 'TRANSCRIPT', 'transcript', 'STRAND_VEP', 'HGNC_ID', 'PUBMED', 'PICK', 'Exon_Number', 'genomic_location_explanation', 'Annotation_Status', 'Variant_Classification', 'Transcript_Exon']</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation._convert_to_str_dtype","title":"<code>_convert_to_str_dtype(column_types, known_string_cols)</code>","text":"<p>Sometimes the deteremined dtype is incorrect based off the first 100 rows, update the incorrect dtypes.</p> Source code in <code>genie/process_mutation.py</code> <pre><code>def _convert_to_str_dtype(column_types, known_string_cols):\n    \"\"\"Sometimes the deteremined dtype is incorrect based off the first\n    100 rows, update the incorrect dtypes.\n    \"\"\"\n    for str_col in known_string_cols:\n        if column_types.get(str_col):\n            column_types[str_col] = \"object\"\n    return column_types\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.determine_dtype","title":"<code>determine_dtype(path)</code>","text":"<p>Reads in a dataframe partially and determines the dtype of columns</p> Source code in <code>genie/process_mutation.py</code> <pre><code>def determine_dtype(path: str):\n    \"\"\"Reads in a dataframe partially and determines the dtype of columns\"\"\"\n    # Change this nrows to 5000 so that it better encapsulates the types\n    subset_df = pd.read_csv(path, nrows=5000, sep=\"\\t\", comment=\"#\")\n    column_types = subset_df.dtypes.to_dict()\n    return column_types\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.move_and_configure_maf","title":"<code>move_and_configure_maf(mutation_path, input_files_dir)</code>","text":"<p>Moves maf files into processing directory. Maf file's column headers are renamed if necessary and .0 are stripped.</p> PARAMETER DESCRIPTION <code>mutation_path</code> <p>Mutation file path</p> <p> TYPE: <code>str</code> </p> <code>input_files_dir</code> <p>Input file directory</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Filepath to moved and configured maf</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/process_mutation.py</code> <pre><code>def move_and_configure_maf(mutation_path: str, input_files_dir: str) -&gt; str:\n    \"\"\"Moves maf files into processing directory. Maf file's column headers\n    are renamed if necessary and .0 are stripped.\n\n    Args:\n        mutation_path (str): Mutation file path\n        input_files_dir (str): Input file directory\n\n    Returns:\n        str: Filepath to moved and configured maf\n    \"\"\"\n    filename = os.path.basename(mutation_path)\n    new_filepath = os.path.join(input_files_dir, filename)\n    column_types = determine_dtype(mutation_path)\n    new_column_types = _convert_to_str_dtype(column_types, KNOWN_STRING_COLS)\n    mafdf = pd.read_csv(mutation_path, sep=\"\\t\", dtype=new_column_types, comment=\"#\")\n    # If any column headers need to be remapped, remap\n    mafdf = mafdf.rename(columns=MAF_COL_MAPPING)\n    # Must remove floating .0 or else processing will fail for genome nexus\n    maf_text = process_functions.removePandasDfFloat(mafdf)\n    with open(new_filepath, \"w\") as new_maf_f:\n        new_maf_f.write(maf_text)\n    return new_filepath\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.move_mutation","title":"<code>move_mutation(mutation_path, input_files_dir)</code>","text":"<p>Move mutation file into processing directory</p> Source code in <code>genie/process_mutation.py</code> <pre><code>def move_mutation(mutation_path, input_files_dir):\n    \"\"\"Move mutation file into processing directory\"\"\"\n    # If mutation file is vcf, just copy\n    if mutation_path.endswith(\".vcf\"):\n        shutil.copy(mutation_path, input_files_dir)\n    else:\n        move_and_configure_maf(mutation_path, input_files_dir)\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.process_mutation_workflow","title":"<code>process_mutation_workflow(syn, center, validfiles, genie_config, workdir)</code>","text":"<p>Process vcf/maf workflow</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>Center name</p> <p> TYPE: <code>str</code> </p> <code>validfiles</code> <p>Center validated files</p> <p> TYPE: <code>DataFrame</code> </p> <code>genie_config</code> <p>GENIE configuration.</p> <p> TYPE: <code>dict</code> </p> <code>workdir</code> <p>Working directory</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[str]</code> <p>Annotated Maf Path. None if there are no valid mutation files.</p> Source code in <code>genie/process_mutation.py</code> <pre><code>def process_mutation_workflow(\n    syn: Synapse,\n    center: str,\n    validfiles: pd.DataFrame,\n    genie_config: dict,\n    workdir: str,\n) -&gt; Optional[str]:\n    \"\"\"Process vcf/maf workflow\n\n    Args:\n        syn: Synapse connection\n        center: Center name\n        validfiles: Center validated files\n        genie_config: GENIE configuration.\n        workdir: Working directory\n\n    Returns:\n        Annotated Maf Path. None if there are no valid mutation files.\n\n    \"\"\"\n    # setting maf table upload timeout time\n    syn.table_query_timeout = 50000\n\n    # Get valid files\n    mutation_files = validfiles[\"fileType\"].isin([\"maf\", \"vcf\"])\n    valid_mutation_files = validfiles[\"path\"][mutation_files].tolist()\n    # If there are no valid mutation files, return\n    if not valid_mutation_files:\n        logger.info(\"No mutation data\")\n        return None\n    # Certificate to use GENIE Genome Nexus\n\n    syn.get(\n        \"syn22053204\",\n        ifcollision=\"overwrite.local\",\n        downloadLocation=genie_config[\"genie_annotation_pkg\"],\n        # version=1,  # TODO: This should pull from a config file in the future\n    )\n\n    # Genome Nexus Jar file\n    syn.get(\n        \"syn22084320\",\n        ifcollision=\"overwrite.local\",\n        downloadLocation=genie_config[\"genie_annotation_pkg\"],\n        # version=13,  # TODO: This should pull from a config file in the future\n    )\n\n    annotation_paths = create_annotation_paths(center=center, workdir=workdir)\n    annotate_mutation(\n        annotation_paths=annotation_paths,\n        center=center,\n        mutation_files=valid_mutation_files,\n        genie_annotation_pkg=genie_config[\"genie_annotation_pkg\"],\n    )\n\n    maf_tableid = genie_config[\"vcf2maf\"]\n    flatfiles_synid = genie_config[\"centerMaf\"]\n    # Split into narrow maf and store into db / flat file\n    split_and_store_maf(\n        syn=syn,\n        center=center,\n        maf_tableid=maf_tableid,\n        annotation_paths=annotation_paths,\n        flatfiles_synid=flatfiles_synid,\n    )\n\n    full_error_report = concat_annotation_error_reports(\n        center=center,\n        input_dir=annotation_paths.error_dir,\n    )\n    check_annotation_error_reports(\n        syn=syn,\n        maf_table_synid=maf_tableid,\n        full_error_report=full_error_report,\n        center=center,\n    )\n    store_annotation_error_reports(\n        full_error_report=full_error_report,\n        full_error_report_path=annotation_paths.full_error_report_path,\n        syn=syn,\n        errors_folder_synid=genie_config[\"center_config\"][center][\"errorsSynId\"],\n    )\n    return annotation_paths.merged_maf_path\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.create_annotation_paths","title":"<code>create_annotation_paths(center, workdir)</code>","text":"<p>Creates the filepaths required in the annotation process</p> PARAMETER DESCRIPTION <code>center</code> <p>name of the center</p> <p> TYPE: <code>str</code> </p> <code>workdir</code> <p>work directory to create paths in</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>namedtuple</code> <p>tuple with all the paths</p> <p> TYPE: <code>namedtuple</code> </p> Source code in <code>genie/process_mutation.py</code> <pre><code>def create_annotation_paths(center: str, workdir: str) -&gt; namedtuple:\n    \"\"\"Creates the filepaths required in the annotation process\n\n    Args:\n        center (str): name of the center\n        workdir (str): work directory to create paths in\n\n    Returns:\n        namedtuple: tuple with all the paths\n    \"\"\"\n    input_files_dir = tempfile.mkdtemp(dir=workdir)\n    output_files_dir = tempfile.mkdtemp(dir=workdir)\n    Filepaths = namedtuple(\n        \"Filepaths\",\n        [\n            \"input_files_dir\",\n            \"output_files_dir\",\n            \"error_dir\",\n            \"merged_maf_path\",\n            \"full_maf_path\",\n            \"narrow_maf_path\",\n            \"full_error_report_path\",\n        ],\n    )\n    annotation_paths = Filepaths(\n        input_files_dir=input_files_dir,\n        output_files_dir=output_files_dir,\n        error_dir=os.path.join(output_files_dir, f\"{center}_error_reports\"),\n        merged_maf_path=os.path.join(\n            output_files_dir, f\"data_mutations_extended_{center}.txt\"\n        ),\n        full_maf_path=os.path.join(\n            workdir, center, \"staging\", f\"data_mutations_extended_{center}.txt\"\n        ),\n        narrow_maf_path=os.path.join(\n            workdir,\n            center,\n            \"staging\",\n            f\"data_mutations_extended_{center}_MAF_narrow.txt\",\n        ),\n        full_error_report_path=os.path.join(\n            workdir,\n            center,\n            \"staging\",\n            f\"failed_annotations_error_report.txt\",\n        ),\n    )\n    return annotation_paths\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.concat_annotation_error_reports","title":"<code>concat_annotation_error_reports(center, input_dir)</code>","text":"<p>Concatenates the annotation error reports</p> PARAMETER DESCRIPTION <code>center</code> <p>name of center associated with error report</p> <p> TYPE: <code>str</code> </p> <code>input_dir</code> <p>directory where error reports are</p> <p> TYPE: <code>str</code> </p> <p>Returns:     pd.DataFrame: full annotation error report</p> Source code in <code>genie/process_mutation.py</code> <pre><code>def concat_annotation_error_reports(\n    center: str,\n    input_dir: str,\n) -&gt; pd.DataFrame:\n    \"\"\"Concatenates the annotation error reports\n\n    Args:\n        center (str): name of center associated with error report\n        input_dir (str): directory where error reports are\n    Returns:\n        pd.DataFrame: full annotation error report\n    \"\"\"\n    error_files = os.listdir(input_dir)\n    chunk_size = 10000\n    error_reports = []\n\n    # Read and concatenate TSV files in chunks\n    for file in error_files:\n        for chunk in pd.read_csv(\n            os.path.join(input_dir, file), sep=\"\\t\", chunksize=chunk_size\n        ):\n            error_reports.append(chunk)\n    full_error_report = pd.concat(error_reports)\n    full_error_report[\"Center\"] = center\n    return full_error_report\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.check_annotation_error_reports","title":"<code>check_annotation_error_reports(syn, maf_table_synid, full_error_report, center)</code>","text":"<p>A simple QC check to make sure our genome nexus error report    failed annotations matches our final processed maf table's failed    annotations</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse client</p> <p> TYPE: <code>Synapse</code> </p> <code>maf_table_synid</code> <p>synapse_id of the narrow maf table</p> <p> TYPE: <code>str</code> </p> <code>full_error_report</code> <p>the failed annotations error report</p> <p> TYPE: <code>DataFrame</code> </p> <code>center</code> <p>the center this is for</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/process_mutation.py</code> <pre><code>def check_annotation_error_reports(\n    syn: Synapse, maf_table_synid: str, full_error_report: pd.DataFrame, center: str\n) -&gt; None:\n    \"\"\"A simple QC check to make sure our genome nexus error report\n       failed annotations matches our final processed maf table's failed\n       annotations\n\n    Args:\n        syn (Synapse): synapse client\n        maf_table_synid (str): synapse_id of the narrow maf table\n        full_error_report (pd.DataFrame): the failed annotations error report\n        center (str): the center this is for\n\n    \"\"\"\n    maf_table_df = extract.get_syntabledf(\n        syn=syn,\n        query_string=(\n            f\"SELECT * FROM {maf_table_synid} \"\n            f\"WHERE Center = '{center}' AND \"\n            \"Annotation_Status = 'FAILED'\"\n        ),\n    )\n    if len(maf_table_df) != len(full_error_report):\n        logger.warning(\n            \"Genome nexus's failed annotations error report rows doesn't match\"\n            f\"maf table's failed annotations for {center}\"\n        )\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.store_annotation_error_reports","title":"<code>store_annotation_error_reports(full_error_report, full_error_report_path, syn, errors_folder_synid)</code>","text":"<p>Stores the annotation error reports to synapse</p> PARAMETER DESCRIPTION <code>full_error_report</code> <p>full error report to store</p> <p> TYPE: <code>DataFrame</code> </p> <code>syn</code> <p>synapse client object</p> <p> TYPE: <code>Synapse</code> </p> <code>errors_folder_synid</code> <p>synapse id of error report folder to store reports in</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/process_mutation.py</code> <pre><code>def store_annotation_error_reports(\n    full_error_report: pd.DataFrame,\n    full_error_report_path: str,\n    syn: Synapse,\n    errors_folder_synid: str,\n) -&gt; None:\n    \"\"\"Stores the annotation error reports to synapse\n\n    Args:\n        full_error_report (pd.DataFrame): full error report to store\n        full_error_report_path (str) where to store the flat file of the full error report\n        syn (synapseclient.Synapse): synapse client object\n        errors_folder_synid (str): synapse id of error report folder\n            to store reports in\n    \"\"\"\n    full_error_report.to_csv(full_error_report_path, sep=\"\\t\", index=False)\n    load.store_file(\n        syn=syn,\n        filepath=full_error_report_path,\n        parentid=errors_folder_synid,\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.annotate_mutation","title":"<code>annotate_mutation(annotation_paths, mutation_files, genie_annotation_pkg, center)</code>","text":"<p>Process vcf/maf files</p> PARAMETER DESCRIPTION <code>center</code> <p>Center name</p> <p> TYPE: <code>str</code> </p> <code>mutation_files</code> <p>list of mutation files</p> <p> TYPE: <code>list</code> </p> <code>genie_annotation_pkg</code> <p>Path to GENIE annotation package</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>None</code> <p>Path to final maf</p> Source code in <code>genie/process_mutation.py</code> <pre><code>def annotate_mutation(\n    annotation_paths: namedtuple,\n    mutation_files: list,\n    genie_annotation_pkg: str,\n    center: str,\n) -&gt; None:\n    \"\"\"Process vcf/maf files\n\n    Args:\n        center: Center name\n        mutation_files: list of mutation files\n        genie_annotation_pkg: Path to GENIE annotation package\n\n    Returns:\n        Path to final maf\n    \"\"\"\n    for mutation_file in mutation_files:\n        move_mutation(mutation_file, annotation_paths.input_files_dir)\n\n    annotater_cmd = [\n        \"bash\",\n        os.path.join(genie_annotation_pkg, \"annotation_suite_wrapper.sh\"),\n        f\"-i={annotation_paths.input_files_dir}\",\n        f\"-o={annotation_paths.output_files_dir}\",\n        f\"-e={annotation_paths.error_dir}\",\n        f\"-m={annotation_paths.merged_maf_path}\",\n        f\"-c={center}\",\n        \"-s=WXS\",\n        f\"-p={genie_annotation_pkg}\",\n    ]\n\n    subprocess.check_call(annotater_cmd)\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.append_or_createdf","title":"<code>append_or_createdf(dataframe, filepath)</code>","text":"<p>Creates a file with the dataframe or appends to a existing file.</p> PARAMETER DESCRIPTION <code>df</code> <p>pandas.dataframe to write out</p> <p> </p> <code>filepath</code> <p>Filepath to append or create</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/process_mutation.py</code> <pre><code>def append_or_createdf(dataframe: pd.DataFrame, filepath: str):\n    \"\"\"Creates a file with the dataframe or appends to a existing file.\n\n    Args:\n        df: pandas.dataframe to write out\n        filepath: Filepath to append or create\n\n    \"\"\"\n    if not os.path.exists(filepath) or os.stat(filepath).st_size == 0:\n        dataframe.to_csv(filepath, sep=\"\\t\", index=False)\n    else:\n        dataframe.to_csv(filepath, sep=\"\\t\", mode=\"a\", index=False, header=None)\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.format_maf","title":"<code>format_maf(mafdf, center)</code>","text":"<p>Format maf file, shortens the maf file length</p> PARAMETER DESCRIPTION <code>mafdf</code> <p>mutation dataframe</p> <p> TYPE: <code>DataFrame</code> </p> <code>center</code> <p>Center name</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Formatted mutation dataframe</p> Source code in <code>genie/process_mutation.py</code> <pre><code>def format_maf(mafdf: pd.DataFrame, center: str) -&gt; pd.DataFrame:\n    \"\"\"Format maf file, shortens the maf file length\n\n    Args:\n        mafdf: mutation dataframe\n        center: Center name\n\n    Returns:\n        Formatted mutation dataframe\"\"\"\n    mafdf[\"Center\"] = center\n    # Leaving here for safe guarding.\n    mafdf[\"Tumor_Sample_Barcode\"] = [\n        process_functions.checkGenieId(i, center) for i in mafdf[\"Tumor_Sample_Barcode\"]\n    ]\n    mafdf[\"Sequence_Source\"] = float(\"nan\")\n    mafdf[\"Sequencer\"] = float(\"nan\")\n    mafdf[\"Validation_Status\"][\n        mafdf[\"Validation_Status\"].isin([\"Unknown\", \"unknown\"])\n    ] = \"\"\n\n    return mafdf\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/process_mutation/#genie.process_mutation.split_and_store_maf","title":"<code>split_and_store_maf(syn, center, maf_tableid, annotation_paths, flatfiles_synid)</code>","text":"<p>Separates annotated maf file into narrow and full maf and stores them</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>center</code> <p>Center</p> <p> TYPE: <code>str</code> </p> <code>maf_tableid</code> <p>Mutation table synapse id</p> <p> TYPE: <code>str</code> </p> <code>annotation_paths</code> <p>filepaths in the annotation process</p> <p> TYPE: <code>namedtuple</code> </p> <code>flatfiles_synid</code> <p>GENIE flat files folder</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/process_mutation.py</code> <pre><code>def split_and_store_maf(\n    syn: Synapse,\n    center: str,\n    maf_tableid: str,\n    annotation_paths: namedtuple,\n    flatfiles_synid: str,\n):\n    \"\"\"Separates annotated maf file into narrow and full maf and stores them\n\n    Args:\n        syn: Synapse connection\n        center: Center\n        maf_tableid: Mutation table synapse id\n        annotation_paths: filepaths in the annotation process\n        flatfiles_synid: GENIE flat files folder\n\n    \"\"\"\n    narrow_maf_cols = [\n        col[\"name\"]\n        for col in syn.getTableColumns(maf_tableid)\n        if col[\"name\"] != \"inBED\"\n    ]\n    maf_chunks = pd.read_csv(\n        annotation_paths.merged_maf_path, sep=\"\\t\", chunksize=100000, comment=\"#\"\n    )\n    for maf_chunk in maf_chunks:\n        maf_chunk = format_maf(maf_chunk, center)\n        append_or_createdf(maf_chunk, annotation_paths.full_maf_path)\n        narrow_maf_chunk = maf_chunk[narrow_maf_cols]\n        append_or_createdf(narrow_maf_chunk, annotation_paths.narrow_maf_path)\n\n    load.store_table(\n        syn=syn, filepath=annotation_paths.narrow_maf_path, tableid=maf_tableid\n    )\n    # Store MAF flat file into synapse\n    load.store_file(\n        syn=syn, filepath=annotation_paths.full_maf_path, parentid=flatfiles_synid\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/to_retract/","title":"To retract","text":""},{"location":"reference/main_pipeline_steps/input_to_database/to_retract/#genie.toRetract","title":"<code>genie.toRetract</code>","text":""},{"location":"reference/main_pipeline_steps/input_to_database/to_retract/#genie.toRetract-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_steps/input_to_database/to_retract/#genie.toRetract.retract_samples","title":"<code>retract_samples(syn, database_synid, col, remove_values)</code>","text":"<p>Helper retraction function that help remove values from a column in a database</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>databse_synid</code> <p>synapse id of database</p> <p> </p> <code>col</code> <p>column in database</p> <p> </p> <code>remove_values</code> <p>list of values to remove from the column</p> <p> </p> Source code in <code>genie/toRetract.py</code> <pre><code>def retract_samples(syn, database_synid, col, remove_values):\n    \"\"\"\n    Helper retraction function that help remove values from a column in\n    a database\n\n    params:\n        syn: synapse object\n        databse_synid:  synapse id of database\n        col: column in database\n        remove_values: list of values to remove from the column\n    \"\"\"\n    # schema = syn.get(database_synid)\n    remove_values_query = \"','\".join(remove_values)\n    remove_rows = syn.tableQuery(\n        \"select %s from %s where %s in ('%s')\"\n        % (col, database_synid, col, remove_values_query)\n    )\n    if len(remove_rows.asRowSet()[\"rows\"]) &gt; 0:\n        syn.delete(remove_rows.asRowSet())\n    else:\n        print(\"Nothing to retract\")\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/to_retract/#genie.toRetract.retract","title":"<code>retract(syn, project_id)</code>","text":"<p>Main retraction function</p> PARAMETER DESCRIPTION <code>syn</code> <p>synapse object</p> <p> </p> <code>project_id</code> <p>Synapse Project ID with a database mapping table</p> <p> </p> Source code in <code>genie/toRetract.py</code> <pre><code>def retract(syn, project_id):\n    \"\"\"\n    Main retraction function\n\n    params:\n        syn: synapse object\n        project_id: Synapse Project ID with a database mapping table\n    \"\"\"\n\n    pat_retraction_synid = extract.getDatabaseSynId(\n        syn=syn, tableName=\"patientRetraction\", project_id=project_id\n    )\n    patientRetractIds = extract.get_syntabledf(\n        syn=syn, query_string=f\"select * from {pat_retraction_synid}\"\n    )\n    # grab all clinical samples that belong to patients in the patient\n    # clinical file and append to sample list\n    sample_synid = extract.getDatabaseSynId(\n        syn=syn, tableName=\"sample\", project_id=project_id\n    )\n    sampleClinicalDf = extract.get_syntabledf(\n        syn=syn, query_string=f\"select * from {sample_synid}\"\n    )\n\n    appendSamples = sampleClinicalDf[\"SAMPLE_ID\"][\n        sampleClinicalDf[\"PATIENT_ID\"].isin(patientRetractIds.geniePatientId)\n    ]\n\n    sample_retract_synid = extract.getDatabaseSynId(\n        syn=syn, tableName=\"sampleRetraction\", project_id=project_id\n    )\n    sampleRetractIds = extract.get_syntabledf(\n        syn=syn, query_string=f\"select * from {sample_retract_synid}\"\n    )\n    allRetractedSamples = pd.concat([sampleRetractIds[\"genieSampleId\"], appendSamples])\n\n    # Only need to retract clinical data, because the rest of the data is filtered by clinical data\n    # Sample Clinical Data\n    retract_samples(\n        syn=syn,\n        database_synid=sample_synid,\n        col=\"SAMPLE_ID\",\n        remove_values=allRetractedSamples,\n    )\n    # Patient Clinical Data\n    retract_samples(\n        syn=syn,\n        database_synid=extract.getDatabaseSynId(\n            syn, tableName=\"patient\", project_id=project_id\n        ),\n        col=\"PATIENT_ID\",\n        remove_values=patientRetractIds[\"geniePatientId\"],\n    )\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/to_retract/#genie.toRetract.main","title":"<code>main()</code>","text":"<p>Main block with argparse and calls the main retract function</p> Source code in <code>genie/toRetract.py</code> <pre><code>def main():\n    \"\"\"\n    Main block with argparse and calls the main retract function\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Sample retraction\")\n    parser.add_argument(\"--project_id\", type=str, help=\"Synapse Project ID to use.\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Synapse Debug Feature\")\n    args = parser.parse_args()\n    syn = process_functions.synapse_login(debug=args.debug)\n    retract(syn, args.project_id)\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/","title":"write_invalid_reasons","text":""},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/#genie.write_invalid_reasons","title":"<code>genie.write_invalid_reasons</code>","text":"<p>Write invalid reasons</p>"},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/#genie.write_invalid_reasons-attributes","title":"Attributes","text":""},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/#genie.write_invalid_reasons.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/#genie.write_invalid_reasons-functions","title":"Functions","text":""},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/#genie.write_invalid_reasons.write","title":"<code>write(syn, center_mapping_synid, error_tracker_synid)</code>","text":"<p>Write center errors to a file</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>center_mapping_synid</code> <p>Center mapping Synapse id</p> <p> TYPE: <code>str</code> </p> <code>error_tracker_synid</code> <p>Error tracking Synapse id</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/write_invalid_reasons.py</code> <pre><code>def write(\n    syn: synapseclient.Synapse, center_mapping_synid: str, error_tracker_synid: str\n):\n    \"\"\"Write center errors to a file\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        center_mapping_synid (str): Center mapping Synapse id\n        error_tracker_synid (str): Error tracking Synapse id\n\n    \"\"\"\n    center_mapping_df = extract.get_syntabledf(\n        syn=syn,\n        query_string=f\"SELECT * FROM {center_mapping_synid} where release is true\",\n    )\n    center_errors = get_center_invalid_errors(syn, error_tracker_synid)\n    for center in center_mapping_df[\"center\"]:\n        logger.info(center)\n        staging_synid = center_mapping_df[\"stagingSynId\"][\n            center_mapping_df[\"center\"] == center\n        ][0]\n        with open(center + \"_errors.txt\", \"w\") as errorfile:\n            if center not in center_errors:\n                errorfile.write(\"No errors!\")\n            else:\n                errorfile.write(center_errors[center])\n\n        ent = synapseclient.File(center + \"_errors.txt\", parentId=staging_synid)\n        syn.store(ent)\n        os.remove(center + \"_errors.txt\")\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/#genie.write_invalid_reasons._combine_center_file_errors","title":"<code>_combine_center_file_errors(syn, center_errorsdf)</code>","text":"<p>Combine all center errors into one printable string</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>center_errorsdf</code> <p>Center errors dataframe</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Center errors in a pretty formatted string</p> <p> TYPE: <code>str</code> </p> Source code in <code>genie/write_invalid_reasons.py</code> <pre><code>def _combine_center_file_errors(\n    syn: synapseclient.Synapse, center_errorsdf: pd.DataFrame\n) -&gt; str:\n    \"\"\"Combine all center errors into one printable string\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        center_errorsdf (pd.DataFrame): Center errors dataframe\n\n    Returns:\n        str: Center errors in a pretty formatted string\n\n    \"\"\"\n    center_errors = \"\"\n    for _, row in center_errorsdf.iterrows():\n        ent = syn.get(row[\"id\"], downloadFile=False)\n        file_errors = row[\"errors\"].replace(\"|\", \"\\n\")\n        error_text = f\"\\t{ent.name} ({ent.id}):\\n\\n{file_errors}\\n\\n\"\n        center_errors += error_text\n    return center_errors\n</code></pre>"},{"location":"reference/main_pipeline_steps/input_to_database/write_invalid_reasons/#genie.write_invalid_reasons.get_center_invalid_errors","title":"<code>get_center_invalid_errors(syn, error_tracker_synid)</code>","text":"<p>Get all invalid errors per center</p> PARAMETER DESCRIPTION <code>syn</code> <p>Synapse connection</p> <p> TYPE: <code>Synapse</code> </p> <code>error_tracker_synid</code> <p>Synapse id of invalid error database table</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>{center: file error string}</p> <p> TYPE: <code>dict</code> </p> Source code in <code>genie/write_invalid_reasons.py</code> <pre><code>def get_center_invalid_errors(\n    syn: synapseclient.Synapse, error_tracker_synid: str\n) -&gt; dict:\n    \"\"\"Get all invalid errors per center\n\n    Args:\n        syn (synapseclient.Synapse): Synapse connection\n        error_tracker_synid (str): Synapse id of invalid error database table\n\n    Returns:\n        dict: {center: file error string}\n\n    \"\"\"\n    error_tracker = syn.tableQuery(f\"SELECT * FROM {error_tracker_synid}\")\n    error_trackerdf = error_tracker.asDataFrame()\n    center_errorsdf = error_trackerdf.groupby(\"center\")\n    center_error_map = {}\n    for center, df in center_errorsdf:\n        center_error_map[center] = _combine_center_file_errors(syn, df)\n    return center_error_map\n</code></pre>"},{"location":"tutorials/local_file_validation/","title":"Local File Validation","text":"<p>One of the features of the <code>aacrgenie</code> package is that is provides a local validation tool that GENIE data contributors and install and use to validate their files locally prior to uploading to Synapse.</p> <pre><code>pip install aacrgenie\ngenie -v\n</code></pre> <p>This will install all the necessary components for you to run the validator locally on all of your files, including the Synapse client.  Please view the help to see how to run to validator.</p> <pre><code>genie validate -h\n</code></pre> <p>Validate a file</p> <pre><code>genie validate data_clinical_supp_SAGE.txt SAGE\n</code></pre>"}]}